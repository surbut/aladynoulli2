# (PART) The ALADYNOULLI Model {-}

# Everyone is a Bayesian {#bayesian}

## Thomas Bayes and the Art of Updating Beliefs

In 1763, Richard Price published a paper by the late Reverend Thomas Bayes titled "An Essay towards solving a Problem in the Doctrine of Chances." The problem Bayes addressed was deceptively simple: given observed data, what can we infer about the underlying probability that generated it?

Bayes' insight was that we should combine our **prior beliefs** with the **evidence from data** to arrive at **posterior beliefs**:

$$P(\theta | \text{Data}) \propto P(\text{Data} | \theta) \times P(\theta)$$

Or in words:
$$\text{Posterior} \propto \text{Likelihood} \times \text{Prior}$$

This formula—Bayes' theorem—is the foundation of modern Bayesian statistics and the heart of ALADYNOULLI.

## Why Bayesian? The Case for Updating

Consider a patient presenting to clinic. Before any tests, you have **prior beliefs** about their disease status based on demographics, chief complaint, and prevalence. Then you observe **data**—their symptoms, physical exam findings, test results—that update your beliefs to a **posterior** probability.

This is exactly how clinicians think:

```
Prior: "50-year-old smoker with chest pain... probably cardiac"
       → P(MI) ≈ 15%

Data:  "ECG shows ST elevations, troponin elevated"
       → Strong evidence for MI

Posterior: P(MI | data) ≈ 95%
```

Bayesian reasoning formalizes this intuitive process.

## Prior Knowledge: Signatures as Population Patterns

In ALADYNOULLI, the **prior** encodes population-level knowledge about disease patterns:

- **Signatures ($\phi$)**: Which diseases tend to co-occur?
- **Prevalence ($\mu_d$)**: How common is each disease over time?
- **Genetic effects ($\gamma$)**: How do genetic factors influence signatures?

These population parameters are learned from large biobank data and represent what we know *before* seeing any individual patient.

## Individual Data: The Likelihood

The **likelihood** captures how well the model explains an individual's observed diagnoses:

- Which diseases have they been diagnosed with?
- At what ages?
- What hasn't been diagnosed (censoring)?

For individual $i$ with disease history $Y_i$, the likelihood is:

$$P(Y_i | \theta_i, \phi) = \prod_{d=1}^{D} \prod_{t=1}^{T} P(Y_{idt} | \pi_{idt})$$

where $\pi_{idt}$ is the modeled disease probability.

## Continuously Updated Posteriors

The **posterior** combines prior and likelihood to give individualized disease trajectories:

$$P(\lambda_i | Y_i, \phi, \gamma) \propto P(Y_i | \lambda_i, \phi) \times P(\lambda_i | \gamma, g_i)$$

As new diagnoses accumulate, the posterior updates:

1. **At enrollment**: Prior dominated by genetics and demographics
2. **First diagnosis**: Posterior shifts toward associated signatures
3. **Multiple diagnoses**: Pattern becomes clearer, predictions sharpen

This continuous updating is what enables **dynamic risk prediction**—the model learns from each patient's evolving history.

## Joint Consideration: Discovery and Prediction

A key philosophical point: in ALADYNOULLI, **discovery** (learning population parameters) and **prediction** (inferring individual risk) happen jointly.

Traditional workflows separate these:
1. Run GWAS → discover genetic associations
2. Build PRS → create risk score
3. Develop model → predict outcomes

ALADYNOULLI integrates them:
- Population parameters ($\phi$, $\gamma$, $\psi$) are learned from all individuals
- Individual parameters ($\lambda_i$) are inferred given the population
- Both improve together as more data accumulates

This joint optimization is why the model achieves both strong prediction *and* novel biological discovery.


# The Mathematical Framework {#math-framework}

## Model Overview

ALADYNOULLI models the probability that individual $i$ is diagnosed with disease $d$ at time $t$ through a hierarchical Bayesian structure:

$$\pi_{idt} = \sum_{k=1}^{K} \theta_{ikt} \cdot \phi_{kdt}$$

where:
- $\pi_{idt}$ ∈ [0,1] is the disease probability
- $\theta_{ikt}$ is individual $i$'s proportion in signature $k$ at time $t$
- $\phi_{kdt}$ ∈ [0,1] is signature $k$'s association with disease $d$ at time $t$
- $K$ is the number of signatures (typically 20-21)

This decomposition separates **individual trajectories** ($\theta$) from **population-level disease patterns** ($\phi$).

## The λ Parameter: Individual Signature Affinities

Each individual has a latent affinity $\lambda_{ikt}$ for each signature at each time point. These affinities are transformed to proportions via softmax:

$$\theta_{ik}(t) = \frac{\exp(\lambda_{ik}(t))}{\sum_{k'=1}^{K} \exp(\lambda_{ik'}(t))}$$

The softmax ensures that $\sum_k \theta_{ikt} = 1$—individuals are distributed across signatures.

The λ parameters follow Gaussian processes in time:

$$\lambda_{ik}(t) \sim \mathcal{GP}(m_{ik}(t), \Omega_\lambda)$$

where:
- $m_{ik}(t) = r_k + g_i^T \gamma_k$ is the mean function
- $r_k$ is the population reference for signature $k$
- $g_i^T \gamma_k$ is the genetic effect
- $\Omega_\lambda$ is the temporal covariance kernel

## The φ Parameter: Signature-Disease Associations

The φ parameters capture how strongly each signature is associated with each disease:

$$\phi_{kd}(t) = \sigma(\psi_{kd} + \mu_d(t))$$

where:
- $\sigma(\cdot)$ is the sigmoid function
- $\psi_{kd}$ is the signature-disease loading (time-invariant)
- $\mu_d(t)$ is the population prevalence of disease $d$ at time $t$

The φ parameters also follow Gaussian processes to ensure smooth temporal dynamics:

$$\phi_{kd}(t) \sim \mathcal{GP}(\mu_d(t) + \psi_{kd}, \Omega_\phi)$$

## The Likelihood Function

The observed data are binary disease indicators $Y_{idt} \in \{0,1\}$. The likelihood incorporates censoring through event times $E_{id}$:

$$\mathcal{L} = \prod_{i=1}^{N} \prod_{d=1}^{D} \prod_{t=1}^{T} \begin{cases}
(1-\pi_{idt}) & \text{if } t < E_{id} \text{ (at risk, no event)} \\
\pi_{idt}^{Y_{idt}} (1-\pi_{idt})^{1-Y_{idt}} & \text{if } t = E_{id} \text{ (event or censor)}
\end{cases}$$

This accounts for:
- **Left truncation**: Individuals enter observation at different ages
- **Right censoring**: Follow-up ends before disease occurs
- **Interval censoring**: Disease occurs between observation times

## The Complete Model

Putting it together:

**Data:**
- $Y_{idt}$: Disease status (0/1)
- $E_{id}$: Event/censoring time
- $g_i$: Genetic profile

**Parameters:**
- $\lambda_{ikt}$: Individual signature affinities (N × K × T)
- $\phi_{kdt}$: Signature-disease associations (K × D × T)
- $\psi_{kd}$: Signature-disease loadings (K × D)
- $\gamma_k$: Genetic effects on signature $k$ (P × K)
- $r_k$: Population reference for signature $k$ (K)

**Priors:**
- $\lambda \sim \mathcal{GP}(r + G\gamma, \Omega_\lambda)$
- $\phi \sim \mathcal{GP}(\mu + \psi, \Omega_\phi)$
- $\gamma \sim \mathcal{N}(0, \sigma_\gamma^2 I)$

**Likelihood:**
$$P(Y | \lambda, \phi) = \prod_{i,d,t} \text{Bernoulli}(Y_{idt} | \pi_{idt})$$

## Identifiability Considerations

A natural question arises: with so many parameters, is the model identifiable?

**What is identifiable:**
- Signature-disease associations ($\phi$, $\psi$): Well-identified from co-occurrence patterns
- Genetic effects on level ($\gamma$): Identified through cross-sectional associations
- **Relative** genetic effects on progression: Identified as differences between signatures

**What requires constraints:**
- Absolute scale of $\lambda$: Fixed by signature references $r_k$
- Absolute genetic slopes: Only relative differences identifiable (due to softmax)

The softmax transformation means that if all signatures increase at the same rate, this is observationally equivalent to no signature changing. We can identify which signatures are *differentially* accelerated, but not absolute acceleration rates.

This is a feature, not a bug: the clinically meaningful quantity is "does high PRS shift disease profile toward cardiovascular signatures?" not "what is the absolute slope of signature 0?"


# Gaussian Processes for Temporal Dynamics {#gaussian-processes}

## Why Gaussian Processes?

Disease risk doesn't jump erratically from year to year. A 50-year-old's risk is similar to their risk at 49 and 51. This **temporal smoothness** is biologically plausible and statistically useful.

Gaussian processes (GPs) provide a principled way to encode this smoothness. A GP defines a distribution over functions such that any finite collection of function values follows a multivariate normal distribution:

$$f(t_1), f(t_2), \ldots, f(t_n) \sim \mathcal{N}(\mu, K)$$

where $K_{ij} = k(t_i, t_j)$ is determined by a covariance (kernel) function.

## The Squared Exponential Kernel

We use the squared exponential (RBF) kernel:

$$k(t, t') = \sigma^2 \exp\left(-\frac{(t-t')^2}{2\ell^2}\right)$$

where:
- $\sigma^2$ is the amplitude (variance of the function)
- $\ell$ is the length scale (how quickly correlations decay)

For disease trajectories:
- **λ length scale**: T/4 ≈ 12.5 years (individual trajectories can change over decades)
- **φ length scale**: T/3 ≈ 17 years (population patterns change slowly)

## GP Priors as Soft Constraints

In ALADYNOULLI, GPs serve as **soft constraints** on temporal dynamics. The prior pulls $\lambda$ toward the genetic mean:

$$\lambda_{ik}(t) \sim \mathcal{GP}(r_k + g_i^T \gamma_k, \Omega_\lambda)$$

But $\lambda$ is also a **free parameter** that can deviate from this mean to fit the observed data. The GP prior acts like a spring:
- Weak prior (low precision): $\lambda$ can deviate freely → better fit, less interpretability
- Strong prior (high precision): $\lambda$ stays near genetic mean → more interpretability, potential underfit

The balance is controlled by the **GP weight** parameter $W$ in the loss function:

$$\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{NLL}} + W \cdot \mathcal{L}_{\text{GP}}$$

## Computational Considerations

GPs are computationally expensive: naive evaluation scales as $O(T^3)$ due to matrix inversion. For $T=51$ time points, this is manageable. For larger $T$, approximations would be needed.

Our implementation:
1. Pre-compute Cholesky decomposition of kernel matrices
2. Vectorize across individuals and signatures
3. Use PyTorch for automatic differentiation

This enables fitting on 400,000 individuals in hours rather than days.


# Genetic Effects on Disease Trajectories {#genetic-effects}

## The γ_level Parameter

Genetic effects enter ALADYNOULLI through the parameter $\gamma$, which we call $\gamma_{\text{level}}$ to distinguish from potential slope effects:

$$\mathbb{E}[\lambda_{ik}(t)] = r_k + g_i^T \gamma_{\text{level},k}$$

This captures the **baseline genetic effect**: individuals with certain genetic profiles tend to have higher (or lower) affinity for specific signatures, on average across all ages.

## Initialization from Linear Regression

A key practical insight is that $\gamma_{\text{level}}$ can be initialized from simple linear regression:

1. Compute cluster-specific disease burden for each individual
2. Regress burden on genetic profile $G$
3. Use regression coefficients as initial $\gamma_{\text{level}}$

This "warm start" is crucial for practical identifiability. Without it, the free $\lambda$ parameters can absorb genetic signal, leaving $\gamma$ near zero.

## Relative vs Absolute Effects

Due to the softmax transformation, genetic effects are identified as **relative** differences:

**Scenario:** TRUE genetic slopes on λ are [0.05, 0.03, 0.02] per year (all positive)

**What the model identifies:**
```
Signature 0: +0.017 (fastest—above average)
Signature 1: -0.003 (middle—near average)  
Signature 2: -0.013 (slowest—below average)
```

The signs appear different because softmax makes everything relative to the mean. But the **clinical interpretation is the same**:

> "High PRS individuals shift their disease profile toward Signature 0 (cardiovascular) over time, relative to other signatures."

This is actually the meaningful quantity—we care about relative shifts, not absolute rates.

## Validation: Known Risk Populations

We validate genetic effects through enrichment in known risk populations:

**Familial Hypercholesterolemia (FH) carriers:**
- FH carriers have higher cardiovascular signature (SIG5) values than non-carriers with similar LDL levels
- This confirms the model captures genetic biology beyond observable phenotypes

**CHIP (Clonal Hematopoiesis):**
- CHIP carriers show elevated cardiovascular and inflammatory signatures
- Consistent with known CHIP biology

These validations demonstrate that $\gamma$ captures real biological signal, not just statistical associations.


# Computational Implementation {#implementation}

## The Vectorization Challenge

The original ALADYNOULLI implementation used nested loops:

```python
for i in range(N):      # 400,000 individuals
    for d in range(D):  # 348 diseases
        for t in range(T):  # 51 time points
            # compute likelihood contribution
```

With $N \times D \times T \approx 7$ billion operations per epoch, training took days.

## Vectorized Likelihood

The breakthrough was recognizing that the likelihood can be vectorized using tensor operations:

```python
# Vectorized computation
theta = torch.softmax(lambda_, dim=1)  # (N, K, T)
phi_prob = torch.sigmoid(phi)           # (K, D, T)
pi = torch.einsum('nkt,kdt->ndt', theta, phi_prob) * kappa  # (N, D, T)

# Vectorized likelihood
log_lik = Y * torch.log(pi) + (1-Y) * torch.log(1-pi)
loss = -log_lik.sum()
```

This reduces training from days to hours—a >100x speedup.

## PyTorch and Automatic Differentiation

ALADYNOULLI is implemented in PyTorch, leveraging:

1. **Automatic differentiation**: Gradients computed automatically
2. **GPU acceleration**: Large tensor operations parallelized
3. **Optimizer ecosystem**: Adam with parameter groups

```python
param_groups = [
    {'params': [lambda_], 'lr': 0.01},
    {'params': [phi], 'lr': 0.001},
    {'params': [gamma], 'lr': 0.01, 'weight_decay': 0.001}
]
optimizer = torch.optim.Adam(param_groups)
```

## Real-Time Patient Refitting

The vectorized implementation enables **real-time refitting**:

1. Load pre-trained population parameters ($\phi$, $\gamma$, $\psi$)
2. Initialize individual $\lambda$ from genetics
3. Fit $\lambda$ to patient's diagnosis history (seconds, not hours)
4. Generate updated risk predictions

This is what powers the web application at https://aladynoulli.hms.harvard.edu

## Scalability

Current implementation handles:
- **N**: 400,000 individuals
- **D**: 348 diseases  
- **T**: 51 time points
- **K**: 21 signatures
- **P**: 5+ genetic features

Training time: ~2-4 hours on a modern GPU

Individual refitting: <10 seconds per patient
