# Enrollment Retrospective Full Models

## Overview

This directory contains Aladynoulli models trained on the full 400k sample dataset using a **retrospective characterization** approach. The key difference from other approaches is that **phi (signature loadings) is estimated jointly** using the full data, rather than being fixed or estimated separately.

## Generation

These models were generated by running:

```bash
./run_full_retrospective.sh
```

Which executes `run_aladyn_batch.py` sequentially on all 40 batches of 10,000 samples each.

## Configuration

- **Total samples**: 400,000
- **Batch size**: 10,000 samples per batch
- **Number of batches**: 40
- **Training epochs**: 200 per batch
- **W parameter**: 0.0001
- **K (clusters)**: 20
- **Include PCs**: Yes (10 principal components)
- **Learning rate**: 0.1
- **Lambda regularization**: 0.01

## File Naming Convention

Files are named: `enrollment_model_W0.0001_batch_{start_index}_{end_index}.pt`

Example: `enrollment_model_W0.0001_batch_390000_400000.pt` contains the model trained on samples 390,000 to 400,000.

## Model Contents

Each `.pt` file contains:

- `model_state_dict`: Full model state (including lambda, beta, etc.)
- `phi`: Signature loadings (estimated jointly)
- `Y`: Disease tensor for the batch
- `prevalence_t`: Disease prevalence over time
- `logit_prevalence_t`: Logit-transformed prevalence
- `G`: Genetic/covariate matrix (including sex and PCs)
- `args`: Training arguments
- `indices`: Patient indices in the batch
- `clusters`: Disease cluster assignments (20 clusters)

## Key Difference: Joint Phi Estimation

Unlike other approaches where phi may be:
- Fixed from a reference
- Estimated separately
- Pre-initialized and frozen

In this retrospective full approach, **phi is estimated jointly** during training on each batch using the full data available. This allows the signature loadings to adapt to the specific characteristics of each batch while maintaining consistency through shared initialization (`initial_psi_400k.pt` and `initial_clusters_400k.pt`).

## Usage

To load a model:

```python
import torch

model_path = 'enrollment_model_W0.0001_batch_0_10000.pt'
checkpoint = torch.load(model_path, weights_only=False)

# Access model components
phi = checkpoint['phi']  # Signature loadings
model_state = checkpoint['model_state_dict']  # Full model state
clusters = checkpoint['clusters']  # Disease clusters
```

## Related Scripts

- **Generation script**: `claudefile/run_aladyn_batch.py`
- **Batch runner**: `claudefile/run_full_retrospective.sh`
- **Logs**: Check `claudefile/logs/retrospective_batch_*.log` for training logs

## Reproducibility Verification

âœ… **Verified**: Model training is fully reproducible. A test run of batch 0-10000 was compared with the original checkpoint:

```python
import torch
import numpy as np

# Load original and re-run checkpoints
old = torch.load('/path/to/enrollment_retrospective_full/enrollment_model_W0.0001_batch_0_10000.pt', weights_only=False)
new = torch.load('/tmp/test_reproducibility/enrollment_model_W0.0001_batch_0_10000.pt', weights_only=False)

# Compare lambda_ parameters (patient-specific signature loadings)
np.allclose(old['model_state_dict']['lambda_'], new['model_state_dict']['lambda_'])
# Returns: True
```

This confirms that:
- Fixed random seed (42) ensures identical initialization
- Same hyperparameters produce identical results
- Patient-specific `lambda_` parameters are reproducible across runs

## Notes

- Models are trained sequentially (not in parallel) to ensure reproducibility
- Each batch uses the same random seed (42) for consistency
- Initial psi and clusters are loaded from `initial_psi_400k.pt` and `initial_clusters_400k.pt`
- All batches use the same hyperparameters for consistency

