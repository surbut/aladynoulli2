
============================================================
BATCH 2/40: 10000-20000
============================================================

============================================================
REPARAM v2 NO KAPPA: samples 10000 to 20000
Epochs: 500, LR: 0.1, grad_clip: 5.0
kappa = 1.0 (FIXED, not learned)
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 25 diseases
Cluster 1: 25 diseases
Cluster 2: 7 diseases
Cluster 3: 23 diseases
Cluster 4: 18 diseases
Cluster 5: 98 diseases
Cluster 6: 8 diseases
Cluster 7: 9 diseases
Cluster 8: 9 diseases
Cluster 9: 5 diseases
Cluster 10: 11 diseases
Cluster 11: 16 diseases
Cluster 12: 9 diseases
Cluster 13: 11 diseases
Cluster 14: 5 diseases
Cluster 15: 12 diseases
Cluster 16: 25 diseases
Cluster 17: 7 diseases
Cluster 18: 16 diseases
Cluster 19: 9 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping (NO KAPPA)...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=63.3284, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.1007, time=0.0min
Epoch   10: Loss=219.3287, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0688, time=0.3min
Epoch   20: Loss=94.7066, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0689, time=0.6min
Epoch   30: Loss=64.3439, LR=9.9e-02, kappa=1.0 (fixed), |gamma|=0.0749, time=0.9min
Epoch   40: Loss=63.5285, LR=9.8e-02, kappa=1.0 (fixed), |gamma|=0.0796, time=1.2min
Epoch   50: Loss=61.2891, LR=9.7e-02, kappa=1.0 (fixed), |gamma|=0.0864, time=1.5min
Epoch   60: Loss=59.5882, LR=9.6e-02, kappa=1.0 (fixed), |gamma|=0.0921, time=1.8min
Epoch   70: Loss=58.6021, LR=9.5e-02, kappa=1.0 (fixed), |gamma|=0.0988, time=2.1min
Epoch   80: Loss=57.9695, LR=9.4e-02, kappa=1.0 (fixed), |gamma|=0.1064, time=2.4min
Epoch   90: Loss=57.4930, LR=9.2e-02, kappa=1.0 (fixed), |gamma|=0.1139, time=2.7min
Epoch  100: Loss=57.0755, LR=9.0e-02, kappa=1.0 (fixed), |gamma|=0.1204, time=3.0min
Epoch  110: Loss=56.6796, LR=8.8e-02, kappa=1.0 (fixed), |gamma|=0.1263, time=3.3min
Epoch  120: Loss=56.3020, LR=8.6e-02, kappa=1.0 (fixed), |gamma|=0.1319, time=3.6min
Epoch  130: Loss=55.9415, LR=8.4e-02, kappa=1.0 (fixed), |gamma|=0.1377, time=3.9min
Epoch  140: Loss=55.5922, LR=8.2e-02, kappa=1.0 (fixed), |gamma|=0.1437, time=4.2min
Epoch  150: Loss=55.2516, LR=7.9e-02, kappa=1.0 (fixed), |gamma|=0.1500, time=4.5min
Epoch  160: Loss=54.9243, LR=7.7e-02, kappa=1.0 (fixed), |gamma|=0.1563, time=4.8min
Epoch  170: Loss=54.5867, LR=7.4e-02, kappa=1.0 (fixed), |gamma|=0.1622, time=5.1min
Epoch  180: Loss=54.2548, LR=7.1e-02, kappa=1.0 (fixed), |gamma|=0.1670, time=5.4min
Epoch  190: Loss=53.9232, LR=6.8e-02, kappa=1.0 (fixed), |gamma|=0.1716, time=5.7min
Epoch  200: Loss=53.5909, LR=6.6e-02, kappa=1.0 (fixed), |gamma|=0.1766, time=6.1min
Epoch  210: Loss=53.2559, LR=6.3e-02, kappa=1.0 (fixed), |gamma|=0.1803, time=6.4min
Epoch  220: Loss=52.9151, LR=5.9e-02, kappa=1.0 (fixed), |gamma|=0.1831, time=6.7min
Epoch  230: Loss=52.5676, LR=5.6e-02, kappa=1.0 (fixed), |gamma|=0.1846, time=7.1min
Epoch  240: Loss=52.2190, LR=5.3e-02, kappa=1.0 (fixed), |gamma|=0.1849, time=7.5min
Epoch  250: Loss=51.8639, LR=5.0e-02, kappa=1.0 (fixed), |gamma|=0.1844, time=7.8min
Epoch  260: Loss=51.5115, LR=4.7e-02, kappa=1.0 (fixed), |gamma|=0.1834, time=8.1min
Epoch  270: Loss=51.1638, LR=4.4e-02, kappa=1.0 (fixed), |gamma|=0.1814, time=8.4min
Epoch  280: Loss=50.8267, LR=4.1e-02, kappa=1.0 (fixed), |gamma|=0.1794, time=8.7min
Epoch  290: Loss=50.5043, LR=3.8e-02, kappa=1.0 (fixed), |gamma|=0.1770, time=9.0min
Epoch  300: Loss=50.1977, LR=3.5e-02, kappa=1.0 (fixed), |gamma|=0.1745, time=9.3min
Epoch  310: Loss=49.9115, LR=3.2e-02, kappa=1.0 (fixed), |gamma|=0.1721, time=9.6min
Epoch  320: Loss=49.6463, LR=2.9e-02, kappa=1.0 (fixed), |gamma|=0.1699, time=9.9min
Epoch  330: Loss=49.4027, LR=2.6e-02, kappa=1.0 (fixed), |gamma|=0.1678, time=10.2min
Epoch  340: Loss=49.1807, LR=2.4e-02, kappa=1.0 (fixed), |gamma|=0.1660, time=10.6min
Epoch  350: Loss=48.9800, LR=2.1e-02, kappa=1.0 (fixed), |gamma|=0.1643, time=10.9min
Epoch  360: Loss=48.7998, LR=1.9e-02, kappa=1.0 (fixed), |gamma|=0.1629, time=11.2min
Epoch  370: Loss=48.6391, LR=1.6e-02, kappa=1.0 (fixed), |gamma|=0.1617, time=11.5min
Epoch  380: Loss=48.4968, LR=1.4e-02, kappa=1.0 (fixed), |gamma|=0.1606, time=11.8min
Epoch  390: Loss=48.3715, LR=1.2e-02, kappa=1.0 (fixed), |gamma|=0.1597, time=12.2min
Epoch  400: Loss=48.2622, LR=1.0e-02, kappa=1.0 (fixed), |gamma|=0.1589, time=12.5min
Epoch  410: Loss=48.1674, LR=8.5e-03, kappa=1.0 (fixed), |gamma|=0.1583, time=12.8min
Epoch  420: Loss=48.0858, LR=7.0e-03, kappa=1.0 (fixed), |gamma|=0.1577, time=13.2min
Epoch  430: Loss=48.0161, LR=5.6e-03, kappa=1.0 (fixed), |gamma|=0.1573, time=13.5min
Epoch  440: Loss=47.9569, LR=4.4e-03, kappa=1.0 (fixed), |gamma|=0.1570, time=13.8min
Epoch  450: Loss=47.9069, LR=3.3e-03, kappa=1.0 (fixed), |gamma|=0.1567, time=14.1min
Epoch  460: Loss=47.8646, LR=2.5e-03, kappa=1.0 (fixed), |gamma|=0.1565, time=14.4min
Epoch  470: Loss=47.8287, LR=1.8e-03, kappa=1.0 (fixed), |gamma|=0.1564, time=14.8min
Epoch  480: Loss=47.7976, LR=1.4e-03, kappa=1.0 (fixed), |gamma|=0.1563, time=15.1min
Epoch  490: Loss=47.7700, LR=1.1e-03, kappa=1.0 (fixed), |gamma|=0.1562, time=15.4min

Training complete: 500 epochs in 15.7 min
Final loss: 47.7467

Final params: kappa=1.0 (fixed), mean|gamma|=0.1562
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2_nokappa/enrollment_model_REPARAM_NOKAPPA_W0.0001_batch_10000_20000.pt
Batch 2 done in 15.8 min

============================================================
BATCH 3/40: 20000-30000
============================================================

============================================================
REPARAM v2 NO KAPPA: samples 20000 to 30000
Epochs: 500, LR: 0.1, grad_clip: 5.0
kappa = 1.0 (FIXED, not learned)
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 12 diseases
Cluster 1: 29 diseases
Cluster 2: 11 diseases
Cluster 3: 12 diseases
Cluster 4: 16 diseases
Cluster 5: 7 diseases
Cluster 6: 10 diseases
Cluster 7: 6 diseases
Cluster 8: 96 diseases
Cluster 9: 14 diseases
Cluster 10: 8 diseases
Cluster 11: 7 diseases
Cluster 12: 16 diseases
Cluster 13: 36 diseases
Cluster 14: 6 diseases
Cluster 15: 8 diseases
Cluster 16: 11 diseases
Cluster 17: 27 diseases
Cluster 18: 11 diseases
Cluster 19: 5 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping (NO KAPPA)...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=62.4698, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.1007, time=0.0min
Epoch   10: Loss=218.6046, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0716, time=0.4min
Epoch   20: Loss=93.8764, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0733, time=0.7min
Epoch   30: Loss=63.5084, LR=9.9e-02, kappa=1.0 (fixed), |gamma|=0.0774, time=1.1min
Epoch   40: Loss=62.7206, LR=9.8e-02, kappa=1.0 (fixed), |gamma|=0.0815, time=1.4min
Epoch   50: Loss=60.4941, LR=9.7e-02, kappa=1.0 (fixed), |gamma|=0.0863, time=1.7min
Epoch   60: Loss=58.8029, LR=9.6e-02, kappa=1.0 (fixed), |gamma|=0.0926, time=2.1min
Epoch   70: Loss=57.8281, LR=9.5e-02, kappa=1.0 (fixed), |gamma|=0.0992, time=2.4min
Epoch   80: Loss=57.2081, LR=9.4e-02, kappa=1.0 (fixed), |gamma|=0.1069, time=2.7min
Epoch   90: Loss=56.7439, LR=9.2e-02, kappa=1.0 (fixed), |gamma|=0.1144, time=3.1min
Epoch  100: Loss=56.3381, LR=9.0e-02, kappa=1.0 (fixed), |gamma|=0.1213, time=3.4min
Epoch  110: Loss=55.9533, LR=8.8e-02, kappa=1.0 (fixed), |gamma|=0.1278, time=3.7min
Epoch  120: Loss=55.5863, LR=8.6e-02, kappa=1.0 (fixed), |gamma|=0.1348, time=4.0min
Epoch  130: Loss=55.2331, LR=8.4e-02, kappa=1.0 (fixed), |gamma|=0.1433, time=4.3min
Epoch  140: Loss=54.8898, LR=8.2e-02, kappa=1.0 (fixed), |gamma|=0.1525, time=4.6min
Epoch  150: Loss=54.5522, LR=7.9e-02, kappa=1.0 (fixed), |gamma|=0.1615, time=5.0min
Epoch  160: Loss=54.2166, LR=7.7e-02, kappa=1.0 (fixed), |gamma|=0.1719, time=5.3min
Epoch  170: Loss=53.8826, LR=7.4e-02, kappa=1.0 (fixed), |gamma|=0.1844, time=5.6min
Epoch  180: Loss=53.5507, LR=7.1e-02, kappa=1.0 (fixed), |gamma|=0.1955, time=5.9min
Epoch  190: Loss=53.2143, LR=6.8e-02, kappa=1.0 (fixed), |gamma|=0.1985, time=6.2min
Epoch  200: Loss=52.8756, LR=6.6e-02, kappa=1.0 (fixed), |gamma|=0.2022, time=6.5min
Epoch  210: Loss=52.5301, LR=6.3e-02, kappa=1.0 (fixed), |gamma|=0.2052, time=6.8min
Epoch  220: Loss=52.1791, LR=5.9e-02, kappa=1.0 (fixed), |gamma|=0.2068, time=7.2min
Epoch  230: Loss=51.8285, LR=5.6e-02, kappa=1.0 (fixed), |gamma|=0.2098, time=7.5min
Epoch  240: Loss=51.4727, LR=5.3e-02, kappa=1.0 (fixed), |gamma|=0.2119, time=7.8min
Epoch  250: Loss=51.1214, LR=5.0e-02, kappa=1.0 (fixed), |gamma|=0.2134, time=8.1min
Epoch  260: Loss=50.7761, LR=4.7e-02, kappa=1.0 (fixed), |gamma|=0.2146, time=8.4min
Epoch  270: Loss=50.4409, LR=4.4e-02, kappa=1.0 (fixed), |gamma|=0.2153, time=8.8min
Epoch  280: Loss=50.1189, LR=4.1e-02, kappa=1.0 (fixed), |gamma|=0.2159, time=9.1min
Epoch  290: Loss=49.8125, LR=3.8e-02, kappa=1.0 (fixed), |gamma|=0.2161, time=9.4min
Epoch  300: Loss=49.5240, LR=3.5e-02, kappa=1.0 (fixed), |gamma|=0.2161, time=9.7min
Epoch  310: Loss=49.2542, LR=3.2e-02, kappa=1.0 (fixed), |gamma|=0.2159, time=10.0min
Epoch  320: Loss=49.0047, LR=2.9e-02, kappa=1.0 (fixed), |gamma|=0.2160, time=10.3min
Epoch  330: Loss=48.7756, LR=2.6e-02, kappa=1.0 (fixed), |gamma|=0.2159, time=10.6min
Epoch  340: Loss=48.5667, LR=2.4e-02, kappa=1.0 (fixed), |gamma|=0.2157, time=10.9min
Epoch  350: Loss=48.3775, LR=2.1e-02, kappa=1.0 (fixed), |gamma|=0.2156, time=11.2min
Epoch  360: Loss=48.2073, LR=1.9e-02, kappa=1.0 (fixed), |gamma|=0.2154, time=11.5min
Epoch  370: Loss=48.0552, LR=1.6e-02, kappa=1.0 (fixed), |gamma|=0.2152, time=11.8min
Epoch  380: Loss=47.9202, LR=1.4e-02, kappa=1.0 (fixed), |gamma|=0.2151, time=12.1min
Epoch  390: Loss=47.8013, LR=1.2e-02, kappa=1.0 (fixed), |gamma|=0.2150, time=12.4min
Epoch  400: Loss=47.6972, LR=1.0e-02, kappa=1.0 (fixed), |gamma|=0.2149, time=12.7min
Epoch  410: Loss=47.6069, LR=8.5e-03, kappa=1.0 (fixed), |gamma|=0.2148, time=15.2min
Epoch  420: Loss=47.5290, LR=7.0e-03, kappa=1.0 (fixed), |gamma|=0.2147, time=29.8min
Epoch  430: Loss=47.4624, LR=5.6e-03, kappa=1.0 (fixed), |gamma|=0.2146, time=30.1min
Epoch  440: Loss=47.4059, LR=4.4e-03, kappa=1.0 (fixed), |gamma|=0.2146, time=30.4min
Epoch  450: Loss=47.3580, LR=3.3e-03, kappa=1.0 (fixed), |gamma|=0.2146, time=30.7min
Epoch  460: Loss=47.3176, LR=2.5e-03, kappa=1.0 (fixed), |gamma|=0.2146, time=31.0min
Epoch  470: Loss=47.2833, LR=1.8e-03, kappa=1.0 (fixed), |gamma|=0.2146, time=31.3min
Epoch  480: Loss=47.2536, LR=1.4e-03, kappa=1.0 (fixed), |gamma|=0.2146, time=31.6min
Epoch  490: Loss=47.2272, LR=1.1e-03, kappa=1.0 (fixed), |gamma|=0.2146, time=32.0min

Training complete: 500 epochs in 32.2 min
Final loss: 47.2050

Final params: kappa=1.0 (fixed), mean|gamma|=0.2147
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2_nokappa/enrollment_model_REPARAM_NOKAPPA_W0.0001_batch_20000_30000.pt
Batch 3 done in 32.4 min

============================================================
BATCH 4/40: 30000-40000
============================================================

============================================================
REPARAM v2 NO KAPPA: samples 30000 to 40000
Epochs: 500, LR: 0.1, grad_clip: 5.0
kappa = 1.0 (FIXED, not learned)
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 28 diseases
Cluster 1: 11 diseases
Cluster 2: 13 diseases
Cluster 3: 11 diseases
Cluster 4: 18 diseases
Cluster 5: 6 diseases
Cluster 6: 9 diseases
Cluster 7: 8 diseases
Cluster 8: 16 diseases
Cluster 9: 9 diseases
Cluster 10: 6 diseases
Cluster 11: 26 diseases
Cluster 12: 14 diseases
Cluster 13: 18 diseases
Cluster 14: 22 diseases
Cluster 15: 12 diseases
Cluster 16: 31 diseases
Cluster 17: 4 diseases
Cluster 18: 17 diseases
Cluster 19: 69 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping (NO KAPPA)...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=64.1886, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.1010, time=0.0min
Epoch   10: Loss=220.1309, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0706, time=0.3min
Epoch   20: Loss=95.5233, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0707, time=0.6min
Epoch   30: Loss=65.1377, LR=9.9e-02, kappa=1.0 (fixed), |gamma|=0.0734, time=1.0min
Epoch   40: Loss=64.3107, LR=9.8e-02, kappa=1.0 (fixed), |gamma|=0.0765, time=1.3min
Epoch   50: Loss=62.0585, LR=9.7e-02, kappa=1.0 (fixed), |gamma|=0.0821, time=1.6min
Epoch   60: Loss=60.3433, LR=9.6e-02, kappa=1.0 (fixed), |gamma|=0.0877, time=1.9min
Epoch   70: Loss=59.3439, LR=9.5e-02, kappa=1.0 (fixed), |gamma|=0.0944, time=2.2min
Epoch   80: Loss=58.6999, LR=9.4e-02, kappa=1.0 (fixed), |gamma|=0.1027, time=2.6min
Epoch   90: Loss=58.2134, LR=9.2e-02, kappa=1.0 (fixed), |gamma|=0.1103, time=2.9min
Epoch  100: Loss=57.7866, LR=9.0e-02, kappa=1.0 (fixed), |gamma|=0.1171, time=3.3min
Epoch  110: Loss=57.3825, LR=8.8e-02, kappa=1.0 (fixed), |gamma|=0.1232, time=3.6min
Epoch  120: Loss=56.9977, LR=8.6e-02, kappa=1.0 (fixed), |gamma|=0.1288, time=3.9min
Epoch  130: Loss=56.6290, LR=8.4e-02, kappa=1.0 (fixed), |gamma|=0.1351, time=4.3min
Epoch  140: Loss=56.2721, LR=8.2e-02, kappa=1.0 (fixed), |gamma|=0.1412, time=4.7min
Epoch  150: Loss=55.9217, LR=7.9e-02, kappa=1.0 (fixed), |gamma|=0.1478, time=5.0min
Epoch  160: Loss=55.5750, LR=7.7e-02, kappa=1.0 (fixed), |gamma|=0.1543, time=5.4min
Epoch  170: Loss=55.2293, LR=7.4e-02, kappa=1.0 (fixed), |gamma|=0.1600, time=5.7min
Epoch  180: Loss=54.8834, LR=7.1e-02, kappa=1.0 (fixed), |gamma|=0.1652, time=6.1min
Epoch  190: Loss=54.5350, LR=6.8e-02, kappa=1.0 (fixed), |gamma|=0.1693, time=6.4min
Epoch  200: Loss=54.1790, LR=6.6e-02, kappa=1.0 (fixed), |gamma|=0.1722, time=6.8min
Epoch  210: Loss=53.8192, LR=6.3e-02, kappa=1.0 (fixed), |gamma|=0.1742, time=7.1min
Epoch  220: Loss=53.4563, LR=5.9e-02, kappa=1.0 (fixed), |gamma|=0.1752, time=7.4min
Epoch  230: Loss=53.0922, LR=5.6e-02, kappa=1.0 (fixed), |gamma|=0.1755, time=7.8min
Epoch  240: Loss=52.7254, LR=5.3e-02, kappa=1.0 (fixed), |gamma|=0.1757, time=8.1min
Epoch  250: Loss=52.3610, LR=5.0e-02, kappa=1.0 (fixed), |gamma|=0.1760, time=8.5min
Epoch  260: Loss=52.0030, LR=4.7e-02, kappa=1.0 (fixed), |gamma|=0.1765, time=8.8min
Epoch  270: Loss=51.6549, LR=4.4e-02, kappa=1.0 (fixed), |gamma|=0.1760, time=9.2min
Epoch  280: Loss=51.3170, LR=4.1e-02, kappa=1.0 (fixed), |gamma|=0.1739, time=9.5min
Epoch  290: Loss=50.9947, LR=3.8e-02, kappa=1.0 (fixed), |gamma|=0.1735, time=9.8min
Epoch  300: Loss=50.6898, LR=3.5e-02, kappa=1.0 (fixed), |gamma|=0.1732, time=10.2min
Epoch  310: Loss=50.4047, LR=3.2e-02, kappa=1.0 (fixed), |gamma|=0.1728, time=10.6min
Epoch  320: Loss=50.1405, LR=2.9e-02, kappa=1.0 (fixed), |gamma|=0.1726, time=10.9min
Epoch  330: Loss=49.8977, LR=2.6e-02, kappa=1.0 (fixed), |gamma|=0.1725, time=11.3min
Epoch  340: Loss=49.6763, LR=2.4e-02, kappa=1.0 (fixed), |gamma|=0.1724, time=11.7min
Epoch  350: Loss=49.4758, LR=2.1e-02, kappa=1.0 (fixed), |gamma|=0.1722, time=12.1min
Epoch  360: Loss=49.2957, LR=1.9e-02, kappa=1.0 (fixed), |gamma|=0.1721, time=12.5min
Epoch  370: Loss=49.1349, LR=1.6e-02, kappa=1.0 (fixed), |gamma|=0.1720, time=12.9min
Epoch  380: Loss=48.9923, LR=1.4e-02, kappa=1.0 (fixed), |gamma|=0.1719, time=13.2min
Epoch  390: Loss=48.8668, LR=1.2e-02, kappa=1.0 (fixed), |gamma|=0.1718, time=13.6min
Epoch  400: Loss=48.7571, LR=1.0e-02, kappa=1.0 (fixed), |gamma|=0.1717, time=14.0min
Epoch  410: Loss=48.6619, LR=8.5e-03, kappa=1.0 (fixed), |gamma|=0.1716, time=14.3min
Epoch  420: Loss=48.5799, LR=7.0e-03, kappa=1.0 (fixed), |gamma|=0.1716, time=14.7min
Epoch  430: Loss=48.5098, LR=5.6e-03, kappa=1.0 (fixed), |gamma|=0.1715, time=15.1min
Epoch  440: Loss=48.4501, LR=4.4e-03, kappa=1.0 (fixed), |gamma|=0.1715, time=15.4min
Epoch  450: Loss=48.3996, LR=3.3e-03, kappa=1.0 (fixed), |gamma|=0.1715, time=15.8min
Epoch  460: Loss=48.3568, LR=2.5e-03, kappa=1.0 (fixed), |gamma|=0.1715, time=16.2min
Epoch  470: Loss=48.3203, LR=1.8e-03, kappa=1.0 (fixed), |gamma|=0.1715, time=16.5min
Epoch  480: Loss=48.2887, LR=1.4e-03, kappa=1.0 (fixed), |gamma|=0.1715, time=16.9min
Epoch  490: Loss=48.2605, LR=1.1e-03, kappa=1.0 (fixed), |gamma|=0.1715, time=17.3min

Training complete: 500 epochs in 17.7 min
Final loss: 48.2368

Final params: kappa=1.0 (fixed), mean|gamma|=0.1715
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2_nokappa/enrollment_model_REPARAM_NOKAPPA_W0.0001_batch_30000_40000.pt
Batch 4 done in 17.9 min

============================================================
BATCH 5/40: 40000-50000
============================================================

============================================================
REPARAM v2 NO KAPPA: samples 40000 to 50000
Epochs: 500, LR: 0.1, grad_clip: 5.0
kappa = 1.0 (FIXED, not learned)
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 12 diseases
Cluster 1: 15 diseases
Cluster 2: 16 diseases
Cluster 3: 52 diseases
Cluster 4: 5 diseases
Cluster 5: 15 diseases
Cluster 6: 13 diseases
Cluster 7: 12 diseases
Cluster 8: 76 diseases
Cluster 9: 5 diseases
Cluster 10: 9 diseases
Cluster 11: 5 diseases
Cluster 12: 7 diseases
Cluster 13: 25 diseases
Cluster 14: 6 diseases
Cluster 15: 11 diseases
Cluster 16: 7 diseases
Cluster 17: 13 diseases
Cluster 18: 12 diseases
Cluster 19: 32 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping (NO KAPPA)...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=64.3720, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.1011, time=0.1min
Epoch   10: Loss=220.3106, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0661, time=0.6min
Epoch   20: Loss=95.6926, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0635, time=1.0min
Epoch   30: Loss=65.2900, LR=9.9e-02, kappa=1.0 (fixed), |gamma|=0.0649, time=1.4min
Epoch   40: Loss=64.4506, LR=9.8e-02, kappa=1.0 (fixed), |gamma|=0.0675, time=1.8min
Epoch   50: Loss=62.1839, LR=9.7e-02, kappa=1.0 (fixed), |gamma|=0.0749, time=2.3min
Epoch   60: Loss=60.4554, LR=9.6e-02, kappa=1.0 (fixed), |gamma|=0.0823, time=2.7min
Epoch   70: Loss=59.4450, LR=9.5e-02, kappa=1.0 (fixed), |gamma|=0.0898, time=3.1min
Epoch   80: Loss=58.7933, LR=9.4e-02, kappa=1.0 (fixed), |gamma|=0.0972, time=3.5min
Epoch   90: Loss=58.3012, LR=9.2e-02, kappa=1.0 (fixed), |gamma|=0.1040, time=4.0min
Epoch  100: Loss=57.8707, LR=9.0e-02, kappa=1.0 (fixed), |gamma|=0.1116, time=4.4min
Epoch  110: Loss=57.4653, LR=8.8e-02, kappa=1.0 (fixed), |gamma|=0.1198, time=4.8min
Epoch  120: Loss=57.0798, LR=8.6e-02, kappa=1.0 (fixed), |gamma|=0.1278, time=5.2min
Epoch  130: Loss=56.7126, LR=8.4e-02, kappa=1.0 (fixed), |gamma|=0.1353, time=5.7min
Epoch  140: Loss=56.3582, LR=8.2e-02, kappa=1.0 (fixed), |gamma|=0.1435, time=6.1min
Epoch  150: Loss=56.0160, LR=7.9e-02, kappa=1.0 (fixed), |gamma|=0.1524, time=6.5min
Epoch  160: Loss=55.6765, LR=7.7e-02, kappa=1.0 (fixed), |gamma|=0.1613, time=6.9min
Epoch  170: Loss=55.3451, LR=7.4e-02, kappa=1.0 (fixed), |gamma|=0.1705, time=7.3min
Epoch  180: Loss=55.0167, LR=7.1e-02, kappa=1.0 (fixed), |gamma|=0.1794, time=7.8min
Epoch  190: Loss=54.6917, LR=6.8e-02, kappa=1.0 (fixed), |gamma|=0.1881, time=8.2min
Epoch  200: Loss=54.3682, LR=6.6e-02, kappa=1.0 (fixed), |gamma|=0.1963, time=8.6min
Epoch  210: Loss=54.0470, LR=6.3e-02, kappa=1.0 (fixed), |gamma|=0.2040, time=9.0min
Epoch  220: Loss=53.7274, LR=5.9e-02, kappa=1.0 (fixed), |gamma|=0.2109, time=9.4min
Epoch  230: Loss=53.4094, LR=5.6e-02, kappa=1.0 (fixed), |gamma|=0.2168, time=9.8min
Epoch  240: Loss=53.0946, LR=5.3e-02, kappa=1.0 (fixed), |gamma|=0.2217, time=10.3min
Epoch  250: Loss=52.7837, LR=5.0e-02, kappa=1.0 (fixed), |gamma|=0.2256, time=10.7min
Epoch  260: Loss=52.4769, LR=4.7e-02, kappa=1.0 (fixed), |gamma|=0.2283, time=11.1min
Epoch  270: Loss=52.1764, LR=4.4e-02, kappa=1.0 (fixed), |gamma|=0.2300, time=11.5min
Epoch  280: Loss=51.8837, LR=4.1e-02, kappa=1.0 (fixed), |gamma|=0.2308, time=12.0min
Epoch  290: Loss=51.6011, LR=3.8e-02, kappa=1.0 (fixed), |gamma|=0.2311, time=12.4min
Epoch  300: Loss=51.3307, LR=3.5e-02, kappa=1.0 (fixed), |gamma|=0.2311, time=12.8min
Epoch  310: Loss=51.0743, LR=3.2e-02, kappa=1.0 (fixed), |gamma|=0.2308, time=13.2min
Epoch  320: Loss=50.8336, LR=2.9e-02, kappa=1.0 (fixed), |gamma|=0.2300, time=13.6min
Epoch  330: Loss=50.6095, LR=2.6e-02, kappa=1.0 (fixed), |gamma|=0.2292, time=14.0min
Epoch  340: Loss=50.4029, LR=2.4e-02, kappa=1.0 (fixed), |gamma|=0.2281, time=14.4min
Epoch  350: Loss=50.2139, LR=2.1e-02, kappa=1.0 (fixed), |gamma|=0.2269, time=14.8min
Epoch  360: Loss=50.0424, LR=1.9e-02, kappa=1.0 (fixed), |gamma|=0.2257, time=15.1min
Epoch  370: Loss=49.8880, LR=1.6e-02, kappa=1.0 (fixed), |gamma|=0.2247, time=15.5min
Epoch  380: Loss=49.7501, LR=1.4e-02, kappa=1.0 (fixed), |gamma|=0.2239, time=15.9min
Epoch  390: Loss=49.6279, LR=1.2e-02, kappa=1.0 (fixed), |gamma|=0.2232, time=16.3min
Epoch  400: Loss=49.5204, LR=1.0e-02, kappa=1.0 (fixed), |gamma|=0.2227, time=16.6min
Epoch  410: Loss=49.4267, LR=8.5e-03, kappa=1.0 (fixed), |gamma|=0.2222, time=17.0min
Epoch  420: Loss=49.3457, LR=7.0e-03, kappa=1.0 (fixed), |gamma|=0.2219, time=17.4min
Epoch  430: Loss=49.2762, LR=5.6e-03, kappa=1.0 (fixed), |gamma|=0.2216, time=17.8min
Epoch  440: Loss=49.2169, LR=4.4e-03, kappa=1.0 (fixed), |gamma|=0.2215, time=18.1min
Epoch  450: Loss=49.1667, LR=3.3e-03, kappa=1.0 (fixed), |gamma|=0.2213, time=18.5min
Epoch  460: Loss=49.1242, LR=2.5e-03, kappa=1.0 (fixed), |gamma|=0.2212, time=18.9min
Epoch  470: Loss=49.0880, LR=1.8e-03, kappa=1.0 (fixed), |gamma|=0.2212, time=19.2min
Epoch  480: Loss=49.0567, LR=1.4e-03, kappa=1.0 (fixed), |gamma|=0.2212, time=19.6min
Epoch  490: Loss=49.0288, LR=1.1e-03, kappa=1.0 (fixed), |gamma|=0.2212, time=19.9min

Training complete: 500 epochs in 20.3 min
Final loss: 49.0053

Final params: kappa=1.0 (fixed), mean|gamma|=0.2212
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2_nokappa/enrollment_model_REPARAM_NOKAPPA_W0.0001_batch_40000_50000.pt
Batch 5 done in 20.5 min

============================================================
BATCH 6/40: 50000-60000
============================================================

============================================================
REPARAM v2 NO KAPPA: samples 50000 to 60000
Epochs: 500, LR: 0.1, grad_clip: 5.0
kappa = 1.0 (FIXED, not learned)
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 22 diseases
Cluster 1: 24 diseases
Cluster 2: 16 diseases
Cluster 3: 5 diseases
Cluster 4: 8 diseases
Cluster 5: 9 diseases
Cluster 6: 31 diseases
Cluster 7: 16 diseases
Cluster 8: 9 diseases
Cluster 9: 8 diseases
Cluster 10: 13 diseases
Cluster 11: 17 diseases
Cluster 12: 14 diseases
Cluster 13: 8 diseases
Cluster 14: 5 diseases
Cluster 15: 12 diseases
Cluster 16: 12 diseases
Cluster 17: 88 diseases
Cluster 18: 9 diseases
Cluster 19: 22 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping (NO KAPPA)...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=63.1814, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.1009, time=0.0min
Epoch   10: Loss=219.2058, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0705, time=0.4min
Epoch   20: Loss=94.5541, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0731, time=0.8min
Epoch   30: Loss=64.1702, LR=9.9e-02, kappa=1.0 (fixed), |gamma|=0.0766, time=1.2min
Epoch   40: Loss=63.3660, LR=9.8e-02, kappa=1.0 (fixed), |gamma|=0.0800, time=1.6min
Epoch   50: Loss=61.1284, LR=9.7e-02, kappa=1.0 (fixed), |gamma|=0.0869, time=2.0min
Epoch   60: Loss=59.4280, LR=9.6e-02, kappa=1.0 (fixed), |gamma|=0.0949, time=2.4min
Epoch   70: Loss=58.4430, LR=9.5e-02, kappa=1.0 (fixed), |gamma|=0.1026, time=2.9min
Epoch   80: Loss=57.8133, LR=9.4e-02, kappa=1.0 (fixed), |gamma|=0.1107, time=3.2min
Epoch   90: Loss=57.3405, LR=9.2e-02, kappa=1.0 (fixed), |gamma|=0.1186, time=3.6min
Epoch  100: Loss=56.9275, LR=9.0e-02, kappa=1.0 (fixed), |gamma|=0.1255, time=4.0min
Epoch  110: Loss=56.5380, LR=8.8e-02, kappa=1.0 (fixed), |gamma|=0.1320, time=4.4min
Epoch  120: Loss=56.1670, LR=8.6e-02, kappa=1.0 (fixed), |gamma|=0.1385, time=4.8min
Epoch  130: Loss=55.8140, LR=8.4e-02, kappa=1.0 (fixed), |gamma|=0.1443, time=5.2min
Epoch  140: Loss=55.4713, LR=8.2e-02, kappa=1.0 (fixed), |gamma|=0.1503, time=5.6min
Epoch  150: Loss=55.1358, LR=7.9e-02, kappa=1.0 (fixed), |gamma|=0.1557, time=6.0min
Epoch  160: Loss=54.8066, LR=7.7e-02, kappa=1.0 (fixed), |gamma|=0.1609, time=6.3min
Epoch  170: Loss=54.4747, LR=7.4e-02, kappa=1.0 (fixed), |gamma|=0.1644, time=6.7min
Epoch  180: Loss=54.1440, LR=7.1e-02, kappa=1.0 (fixed), |gamma|=0.1674, time=7.1min
Epoch  190: Loss=53.8093, LR=6.8e-02, kappa=1.0 (fixed), |gamma|=0.1702, time=7.5min
Epoch  200: Loss=53.4696, LR=6.6e-02, kappa=1.0 (fixed), |gamma|=0.1726, time=7.9min
Epoch  210: Loss=53.1282, LR=6.3e-02, kappa=1.0 (fixed), |gamma|=0.1740, time=8.3min
Epoch  220: Loss=52.7813, LR=5.9e-02, kappa=1.0 (fixed), |gamma|=0.1746, time=8.6min
Epoch  230: Loss=52.4331, LR=5.6e-02, kappa=1.0 (fixed), |gamma|=0.1751, time=9.0min
Epoch  240: Loss=52.0857, LR=5.3e-02, kappa=1.0 (fixed), |gamma|=0.1752, time=9.4min
Epoch  250: Loss=51.7431, LR=5.0e-02, kappa=1.0 (fixed), |gamma|=0.1751, time=9.7min
Epoch  260: Loss=51.4052, LR=4.7e-02, kappa=1.0 (fixed), |gamma|=0.1747, time=10.1min
Epoch  270: Loss=51.0780, LR=4.4e-02, kappa=1.0 (fixed), |gamma|=0.1739, time=10.5min
Epoch  280: Loss=50.7639, LR=4.1e-02, kappa=1.0 (fixed), |gamma|=0.1729, time=10.9min
Epoch  290: Loss=50.4659, LR=3.8e-02, kappa=1.0 (fixed), |gamma|=0.1718, time=11.2min
Epoch  300: Loss=50.1857, LR=3.5e-02, kappa=1.0 (fixed), |gamma|=0.1707, time=11.6min
Epoch  310: Loss=49.9247, LR=3.2e-02, kappa=1.0 (fixed), |gamma|=0.1696, time=12.0min
Epoch  320: Loss=49.6834, LR=2.9e-02, kappa=1.0 (fixed), |gamma|=0.1686, time=12.4min
Epoch  330: Loss=49.4621, LR=2.6e-02, kappa=1.0 (fixed), |gamma|=0.1676, time=12.7min
Epoch  340: Loss=49.2606, LR=2.4e-02, kappa=1.0 (fixed), |gamma|=0.1668, time=13.1min
Epoch  350: Loss=49.0782, LR=2.1e-02, kappa=1.0 (fixed), |gamma|=0.1660, time=13.4min
Epoch  360: Loss=48.9142, LR=1.9e-02, kappa=1.0 (fixed), |gamma|=0.1653, time=13.8min
Epoch  370: Loss=48.7678, LR=1.6e-02, kappa=1.0 (fixed), |gamma|=0.1647, time=14.2min
Epoch  380: Loss=48.6380, LR=1.4e-02, kappa=1.0 (fixed), |gamma|=0.1643, time=14.6min
Epoch  390: Loss=48.5236, LR=1.2e-02, kappa=1.0 (fixed), |gamma|=0.1638, time=14.9min
Epoch  400: Loss=48.4235, LR=1.0e-02, kappa=1.0 (fixed), |gamma|=0.1635, time=15.3min
Epoch  410: Loss=48.3366, LR=8.5e-03, kappa=1.0 (fixed), |gamma|=0.1632, time=15.6min
Epoch  420: Loss=48.2618, LR=7.0e-03, kappa=1.0 (fixed), |gamma|=0.1630, time=16.0min
Epoch  430: Loss=48.1978, LR=5.6e-03, kappa=1.0 (fixed), |gamma|=0.1628, time=16.3min
Epoch  440: Loss=48.1434, LR=4.4e-03, kappa=1.0 (fixed), |gamma|=0.1627, time=16.6min
Epoch  450: Loss=48.0973, LR=3.3e-03, kappa=1.0 (fixed), |gamma|=0.1626, time=17.0min
Epoch  460: Loss=48.0585, LR=2.5e-03, kappa=1.0 (fixed), |gamma|=0.1626, time=17.3min
Epoch  470: Loss=48.0254, LR=1.8e-03, kappa=1.0 (fixed), |gamma|=0.1625, time=17.6min
Epoch  480: Loss=47.9968, LR=1.4e-03, kappa=1.0 (fixed), |gamma|=0.1625, time=17.9min
Epoch  490: Loss=47.9713, LR=1.1e-03, kappa=1.0 (fixed), |gamma|=0.1625, time=18.2min

Training complete: 500 epochs in 18.5 min
Final loss: 47.9499

Final params: kappa=1.0 (fixed), mean|gamma|=0.1625
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2_nokappa/enrollment_model_REPARAM_NOKAPPA_W0.0001_batch_50000_60000.pt
Batch 6 done in 18.7 min

============================================================
BATCH 7/40: 60000-70000
============================================================

============================================================
REPARAM v2 NO KAPPA: samples 60000 to 70000
Epochs: 500, LR: 0.1, grad_clip: 5.0
kappa = 1.0 (FIXED, not learned)
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 13 diseases
Cluster 1: 14 diseases
Cluster 2: 28 diseases
Cluster 3: 80 diseases
Cluster 4: 9 diseases
Cluster 5: 14 diseases
Cluster 6: 13 diseases
Cluster 7: 12 diseases
Cluster 8: 17 diseases
Cluster 9: 14 diseases
Cluster 10: 7 diseases
Cluster 11: 22 diseases
Cluster 12: 9 diseases
Cluster 13: 5 diseases
Cluster 14: 35 diseases
Cluster 15: 5 diseases
Cluster 16: 29 diseases
Cluster 17: 7 diseases
Cluster 18: 7 diseases
Cluster 19: 8 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping (NO KAPPA)...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=64.3034, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.1009, time=0.0min
Epoch   10: Loss=220.2977, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0671, time=0.3min
Epoch   20: Loss=95.6326, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0661, time=0.6min
Epoch   30: Loss=65.2198, LR=9.9e-02, kappa=1.0 (fixed), |gamma|=0.0661, time=0.9min
Epoch   40: Loss=64.3866, LR=9.8e-02, kappa=1.0 (fixed), |gamma|=0.0704, time=1.2min
Epoch   50: Loss=62.1240, LR=9.7e-02, kappa=1.0 (fixed), |gamma|=0.0771, time=1.6min
Epoch   60: Loss=60.3961, LR=9.6e-02, kappa=1.0 (fixed), |gamma|=0.0846, time=1.9min
Epoch   70: Loss=59.3861, LR=9.5e-02, kappa=1.0 (fixed), |gamma|=0.0919, time=2.2min
Epoch   80: Loss=58.7341, LR=9.4e-02, kappa=1.0 (fixed), |gamma|=0.0999, time=2.5min
Epoch   90: Loss=58.2416, LR=9.2e-02, kappa=1.0 (fixed), |gamma|=0.1077, time=2.8min
Epoch  100: Loss=57.8109, LR=9.0e-02, kappa=1.0 (fixed), |gamma|=0.1161, time=3.1min
Epoch  110: Loss=57.4037, LR=8.8e-02, kappa=1.0 (fixed), |gamma|=0.1243, time=3.4min
Epoch  120: Loss=57.0165, LR=8.6e-02, kappa=1.0 (fixed), |gamma|=0.1325, time=3.7min
Epoch  130: Loss=56.6477, LR=8.4e-02, kappa=1.0 (fixed), |gamma|=0.1407, time=4.0min
Epoch  140: Loss=56.2903, LR=8.2e-02, kappa=1.0 (fixed), |gamma|=0.1498, time=4.3min
Epoch  150: Loss=55.9429, LR=7.9e-02, kappa=1.0 (fixed), |gamma|=0.1590, time=4.6min
Epoch  160: Loss=55.6014, LR=7.7e-02, kappa=1.0 (fixed), |gamma|=0.1687, time=5.0min
Epoch  170: Loss=55.2651, LR=7.4e-02, kappa=1.0 (fixed), |gamma|=0.1788, time=5.3min
Epoch  180: Loss=54.9331, LR=7.1e-02, kappa=1.0 (fixed), |gamma|=0.1886, time=5.6min
Epoch  190: Loss=54.6035, LR=6.8e-02, kappa=1.0 (fixed), |gamma|=0.1981, time=5.9min
Epoch  200: Loss=54.2746, LR=6.6e-02, kappa=1.0 (fixed), |gamma|=0.2065, time=6.2min
Epoch  210: Loss=53.9450, LR=6.3e-02, kappa=1.0 (fixed), |gamma|=0.2147, time=6.5min
Epoch  220: Loss=53.6175, LR=5.9e-02, kappa=1.0 (fixed), |gamma|=0.2222, time=6.8min
Epoch  230: Loss=53.2875, LR=5.6e-02, kappa=1.0 (fixed), |gamma|=0.2281, time=7.1min
Epoch  240: Loss=52.9579, LR=5.3e-02, kappa=1.0 (fixed), |gamma|=0.2330, time=7.4min
Epoch  250: Loss=52.6290, LR=5.0e-02, kappa=1.0 (fixed), |gamma|=0.2370, time=7.7min
Epoch  260: Loss=52.3040, LR=4.7e-02, kappa=1.0 (fixed), |gamma|=0.2403, time=8.1min
Epoch  270: Loss=51.9856, LR=4.4e-02, kappa=1.0 (fixed), |gamma|=0.2430, time=8.4min
Epoch  280: Loss=51.6750, LR=4.1e-02, kappa=1.0 (fixed), |gamma|=0.2451, time=8.7min
Epoch  290: Loss=51.3750, LR=3.8e-02, kappa=1.0 (fixed), |gamma|=0.2459, time=9.0min
Epoch  300: Loss=51.0883, LR=3.5e-02, kappa=1.0 (fixed), |gamma|=0.2451, time=9.3min
Epoch  310: Loss=50.8147, LR=3.2e-02, kappa=1.0 (fixed), |gamma|=0.2433, time=9.6min
Epoch  320: Loss=50.5579, LR=2.9e-02, kappa=1.0 (fixed), |gamma|=0.2414, time=9.9min
Epoch  330: Loss=50.3186, LR=2.6e-02, kappa=1.0 (fixed), |gamma|=0.2393, time=10.2min
Epoch  340: Loss=50.0974, LR=2.4e-02, kappa=1.0 (fixed), |gamma|=0.2372, time=10.6min
Epoch  350: Loss=49.8948, LR=2.1e-02, kappa=1.0 (fixed), |gamma|=0.2352, time=10.9min
Epoch  360: Loss=49.7108, LR=1.9e-02, kappa=1.0 (fixed), |gamma|=0.2333, time=11.2min
Epoch  370: Loss=49.5452, LR=1.6e-02, kappa=1.0 (fixed), |gamma|=0.2315, time=11.5min
Epoch  380: Loss=49.3973, LR=1.4e-02, kappa=1.0 (fixed), |gamma|=0.2299, time=11.8min
Epoch  390: Loss=49.2663, LR=1.2e-02, kappa=1.0 (fixed), |gamma|=0.2285, time=12.1min
Epoch  400: Loss=49.1513, LR=1.0e-02, kappa=1.0 (fixed), |gamma|=0.2273, time=12.5min
Epoch  410: Loss=49.0512, LR=8.5e-03, kappa=1.0 (fixed), |gamma|=0.2262, time=12.8min
Epoch  420: Loss=48.9646, LR=7.0e-03, kappa=1.0 (fixed), |gamma|=0.2254, time=13.1min
Epoch  430: Loss=48.8904, LR=5.6e-03, kappa=1.0 (fixed), |gamma|=0.2247, time=13.4min
Epoch  440: Loss=48.8272, LR=4.4e-03, kappa=1.0 (fixed), |gamma|=0.2241, time=13.7min
Epoch  450: Loss=48.7736, LR=3.3e-03, kappa=1.0 (fixed), |gamma|=0.2237, time=14.1min
Epoch  460: Loss=48.7283, LR=2.5e-03, kappa=1.0 (fixed), |gamma|=0.2234, time=14.4min
Epoch  470: Loss=48.6896, LR=1.8e-03, kappa=1.0 (fixed), |gamma|=0.2231, time=14.7min
Epoch  480: Loss=48.6562, LR=1.4e-03, kappa=1.0 (fixed), |gamma|=0.2230, time=15.1min
Epoch  490: Loss=48.6263, LR=1.1e-03, kappa=1.0 (fixed), |gamma|=0.2228, time=15.4min

Training complete: 500 epochs in 15.7 min
Final loss: 48.6012

Final params: kappa=1.0 (fixed), mean|gamma|=0.2227
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2_nokappa/enrollment_model_REPARAM_NOKAPPA_W0.0001_batch_60000_70000.pt
Batch 7 done in 15.9 min

============================================================
BATCH 8/40: 70000-80000
============================================================

============================================================
REPARAM v2 NO KAPPA: samples 70000 to 80000
Epochs: 500, LR: 0.1, grad_clip: 5.0
kappa = 1.0 (FIXED, not learned)
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 11 diseases
Cluster 1: 14 diseases
Cluster 2: 17 diseases
Cluster 3: 8 diseases
Cluster 4: 16 diseases
Cluster 5: 114 diseases
Cluster 6: 13 diseases
Cluster 7: 10 diseases
Cluster 8: 16 diseases
Cluster 9: 9 diseases
Cluster 10: 6 diseases
Cluster 11: 10 diseases
Cluster 12: 9 diseases
Cluster 13: 11 diseases
Cluster 14: 5 diseases
Cluster 15: 9 diseases
Cluster 16: 16 diseases
Cluster 17: 24 diseases
Cluster 18: 12 diseases
Cluster 19: 18 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping (NO KAPPA)...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=63.4252, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.1006, time=0.0min
Epoch   10: Loss=219.4543, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0668, time=0.4min
Epoch   20: Loss=94.8117, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0662, time=0.7min
Epoch   30: Loss=64.4274, LR=9.9e-02, kappa=1.0 (fixed), |gamma|=0.0672, time=1.0min
Epoch   40: Loss=63.6120, LR=9.8e-02, kappa=1.0 (fixed), |gamma|=0.0709, time=1.3min
Epoch   50: Loss=61.3709, LR=9.7e-02, kappa=1.0 (fixed), |gamma|=0.0776, time=1.7min
Epoch   60: Loss=59.6632, LR=9.6e-02, kappa=1.0 (fixed), |gamma|=0.0840, time=2.0min
Epoch   70: Loss=58.6731, LR=9.5e-02, kappa=1.0 (fixed), |gamma|=0.0907, time=2.3min
Epoch   80: Loss=58.0392, LR=9.4e-02, kappa=1.0 (fixed), |gamma|=0.0977, time=2.7min
Epoch   90: Loss=57.5627, LR=9.2e-02, kappa=1.0 (fixed), |gamma|=0.1053, time=3.0min
Epoch  100: Loss=57.1470, LR=9.0e-02, kappa=1.0 (fixed), |gamma|=0.1126, time=3.3min
Epoch  110: Loss=56.7540, LR=8.8e-02, kappa=1.0 (fixed), |gamma|=0.1190, time=3.6min
Epoch  120: Loss=56.3795, LR=8.6e-02, kappa=1.0 (fixed), |gamma|=0.1250, time=4.0min
Epoch  130: Loss=56.0226, LR=8.4e-02, kappa=1.0 (fixed), |gamma|=0.1304, time=4.3min
Epoch  140: Loss=55.6736, LR=8.2e-02, kappa=1.0 (fixed), |gamma|=0.1360, time=4.6min
Epoch  150: Loss=55.3332, LR=7.9e-02, kappa=1.0 (fixed), |gamma|=0.1411, time=4.9min
Epoch  160: Loss=54.9957, LR=7.7e-02, kappa=1.0 (fixed), |gamma|=0.1465, time=5.3min
Epoch  170: Loss=54.6611, LR=7.4e-02, kappa=1.0 (fixed), |gamma|=0.1519, time=5.6min
Epoch  180: Loss=54.3222, LR=7.1e-02, kappa=1.0 (fixed), |gamma|=0.1563, time=5.9min
Epoch  190: Loss=53.9841, LR=6.8e-02, kappa=1.0 (fixed), |gamma|=0.1603, time=6.3min
Epoch  200: Loss=53.6356, LR=6.6e-02, kappa=1.0 (fixed), |gamma|=0.1628, time=6.6min
Epoch  210: Loss=53.2822, LR=6.3e-02, kappa=1.0 (fixed), |gamma|=0.1644, time=7.0min
Epoch  220: Loss=52.9205, LR=5.9e-02, kappa=1.0 (fixed), |gamma|=0.1645, time=7.3min
Epoch  230: Loss=52.5546, LR=5.6e-02, kappa=1.0 (fixed), |gamma|=0.1640, time=7.6min
Epoch  240: Loss=52.1818, LR=5.3e-02, kappa=1.0 (fixed), |gamma|=0.1628, time=8.0min
Epoch  250: Loss=51.8074, LR=5.0e-02, kappa=1.0 (fixed), |gamma|=0.1609, time=8.3min
Epoch  260: Loss=51.4374, LR=4.7e-02, kappa=1.0 (fixed), |gamma|=0.1585, time=8.7min
Epoch  270: Loss=51.0712, LR=4.4e-02, kappa=1.0 (fixed), |gamma|=0.1558, time=9.1min
Epoch  280: Loss=50.7173, LR=4.1e-02, kappa=1.0 (fixed), |gamma|=0.1530, time=9.4min
Epoch  290: Loss=50.3784, LR=3.8e-02, kappa=1.0 (fixed), |gamma|=0.1504, time=9.8min
Epoch  300: Loss=50.0573, LR=3.5e-02, kappa=1.0 (fixed), |gamma|=0.1480, time=10.2min
Epoch  310: Loss=49.7566, LR=3.2e-02, kappa=1.0 (fixed), |gamma|=0.1458, time=10.6min
Epoch  320: Loss=49.4776, LR=2.9e-02, kappa=1.0 (fixed), |gamma|=0.1438, time=10.9min
Epoch  330: Loss=49.2211, LR=2.6e-02, kappa=1.0 (fixed), |gamma|=0.1421, time=11.3min
Epoch  340: Loss=48.9872, LR=2.4e-02, kappa=1.0 (fixed), |gamma|=0.1405, time=11.7min
Epoch  350: Loss=48.7757, LR=2.1e-02, kappa=1.0 (fixed), |gamma|=0.1392, time=12.1min
Epoch  360: Loss=48.5857, LR=1.9e-02, kappa=1.0 (fixed), |gamma|=0.1381, time=12.5min
Epoch  370: Loss=48.4163, LR=1.6e-02, kappa=1.0 (fixed), |gamma|=0.1371, time=12.9min
Epoch  380: Loss=48.2662, LR=1.4e-02, kappa=1.0 (fixed), |gamma|=0.1363, time=13.2min
Epoch  390: Loss=48.1342, LR=1.2e-02, kappa=1.0 (fixed), |gamma|=0.1356, time=13.6min
Epoch  400: Loss=48.0189, LR=1.0e-02, kappa=1.0 (fixed), |gamma|=0.1350, time=14.0min
Epoch  410: Loss=47.9189, LR=8.5e-03, kappa=1.0 (fixed), |gamma|=0.1346, time=14.4min
Epoch  420: Loss=47.8329, LR=7.0e-03, kappa=1.0 (fixed), |gamma|=0.1342, time=14.8min
Epoch  430: Loss=47.7594, LR=5.6e-03, kappa=1.0 (fixed), |gamma|=0.1338, time=15.2min
Epoch  440: Loss=47.6969, LR=4.4e-03, kappa=1.0 (fixed), |gamma|=0.1336, time=15.5min
Epoch  450: Loss=47.6440, LR=3.3e-03, kappa=1.0 (fixed), |gamma|=0.1334, time=15.9min
Epoch  460: Loss=47.5993, LR=2.5e-03, kappa=1.0 (fixed), |gamma|=0.1332, time=16.3min
Epoch  470: Loss=47.5612, LR=1.8e-03, kappa=1.0 (fixed), |gamma|=0.1331, time=16.7min
Epoch  480: Loss=47.5283, LR=1.4e-03, kappa=1.0 (fixed), |gamma|=0.1330, time=17.1min
Epoch  490: Loss=47.4989, LR=1.1e-03, kappa=1.0 (fixed), |gamma|=0.1330, time=17.5min

Training complete: 500 epochs in 17.9 min
Final loss: 47.4741

Final params: kappa=1.0 (fixed), mean|gamma|=0.1329
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2_nokappa/enrollment_model_REPARAM_NOKAPPA_W0.0001_batch_70000_80000.pt
Batch 8 done in 18.0 min

============================================================
BATCH 9/40: 80000-90000
============================================================

============================================================
REPARAM v2 NO KAPPA: samples 80000 to 90000
Epochs: 500, LR: 0.1, grad_clip: 5.0
kappa = 1.0 (FIXED, not learned)
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 17 diseases
Cluster 1: 24 diseases
Cluster 2: 5 diseases
Cluster 3: 10 diseases
Cluster 4: 16 diseases
Cluster 5: 13 diseases
Cluster 6: 12 diseases
Cluster 7: 20 diseases
Cluster 8: 32 diseases
Cluster 9: 9 diseases
Cluster 10: 100 diseases
Cluster 11: 13 diseases
Cluster 12: 9 diseases
Cluster 13: 8 diseases
Cluster 14: 17 diseases
Cluster 15: 5 diseases
Cluster 16: 4 diseases
Cluster 17: 14 diseases
Cluster 18: 11 diseases
Cluster 19: 9 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping (NO KAPPA)...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=63.3644, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.1008, time=0.0min
Epoch   10: Loss=219.4091, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0689, time=0.4min
Epoch   20: Loss=94.7561, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0688, time=0.8min
Epoch   30: Loss=64.3699, LR=9.9e-02, kappa=1.0 (fixed), |gamma|=0.0695, time=1.2min
Epoch   40: Loss=63.5585, LR=9.8e-02, kappa=1.0 (fixed), |gamma|=0.0722, time=1.6min
Epoch   50: Loss=61.3181, LR=9.7e-02, kappa=1.0 (fixed), |gamma|=0.0777, time=2.0min
Epoch   60: Loss=59.6134, LR=9.6e-02, kappa=1.0 (fixed), |gamma|=0.0837, time=2.4min
Epoch   70: Loss=58.6276, LR=9.5e-02, kappa=1.0 (fixed), |gamma|=0.0899, time=2.8min
Epoch   80: Loss=57.9989, LR=9.4e-02, kappa=1.0 (fixed), |gamma|=0.0963, time=3.2min
Epoch   90: Loss=57.5276, LR=9.2e-02, kappa=1.0 (fixed), |gamma|=0.1025, time=3.6min
Epoch  100: Loss=57.1164, LR=9.0e-02, kappa=1.0 (fixed), |gamma|=0.1096, time=4.0min
Epoch  110: Loss=56.7268, LR=8.8e-02, kappa=1.0 (fixed), |gamma|=0.1166, time=4.4min
Epoch  120: Loss=56.3558, LR=8.6e-02, kappa=1.0 (fixed), |gamma|=0.1232, time=4.8min
Epoch  130: Loss=56.0010, LR=8.4e-02, kappa=1.0 (fixed), |gamma|=0.1299, time=5.1min
Epoch  140: Loss=55.6597, LR=8.2e-02, kappa=1.0 (fixed), |gamma|=0.1363, time=5.5min
Epoch  150: Loss=55.3216, LR=7.9e-02, kappa=1.0 (fixed), |gamma|=0.1434, time=5.9min
Epoch  160: Loss=54.9914, LR=7.7e-02, kappa=1.0 (fixed), |gamma|=0.1501, time=6.2min
Epoch  170: Loss=54.6667, LR=7.4e-02, kappa=1.0 (fixed), |gamma|=0.1563, time=6.6min
Epoch  180: Loss=54.3423, LR=7.1e-02, kappa=1.0 (fixed), |gamma|=0.1620, time=7.0min
Epoch  190: Loss=54.0199, LR=6.8e-02, kappa=1.0 (fixed), |gamma|=0.1677, time=7.3min
Epoch  200: Loss=53.6991, LR=6.6e-02, kappa=1.0 (fixed), |gamma|=0.1732, time=7.7min
Epoch  210: Loss=53.3767, LR=6.3e-02, kappa=1.0 (fixed), |gamma|=0.1783, time=8.1min
Epoch  220: Loss=53.0555, LR=5.9e-02, kappa=1.0 (fixed), |gamma|=0.1826, time=8.5min
Epoch  230: Loss=52.7367, LR=5.6e-02, kappa=1.0 (fixed), |gamma|=0.1859, time=8.8min
Epoch  240: Loss=52.4180, LR=5.3e-02, kappa=1.0 (fixed), |gamma|=0.1881, time=9.2min
Epoch  250: Loss=52.1024, LR=5.0e-02, kappa=1.0 (fixed), |gamma|=0.1889, time=9.6min
Epoch  260: Loss=51.7915, LR=4.7e-02, kappa=1.0 (fixed), |gamma|=0.1888, time=10.0min
Epoch  270: Loss=51.4841, LR=4.4e-02, kappa=1.0 (fixed), |gamma|=0.1880, time=10.4min
Epoch  280: Loss=51.1838, LR=4.1e-02, kappa=1.0 (fixed), |gamma|=0.1869, time=10.7min
Epoch  290: Loss=50.8929, LR=3.8e-02, kappa=1.0 (fixed), |gamma|=0.1858, time=11.1min
Epoch  300: Loss=50.6132, LR=3.5e-02, kappa=1.0 (fixed), |gamma|=0.1849, time=11.5min
Epoch  310: Loss=50.3473, LR=3.2e-02, kappa=1.0 (fixed), |gamma|=0.1838, time=11.9min
Epoch  320: Loss=50.0972, LR=2.9e-02, kappa=1.0 (fixed), |gamma|=0.1829, time=12.3min
Epoch  330: Loss=49.8643, LR=2.6e-02, kappa=1.0 (fixed), |gamma|=0.1820, time=12.7min
Epoch  340: Loss=49.6493, LR=2.4e-02, kappa=1.0 (fixed), |gamma|=0.1812, time=13.1min
Epoch  350: Loss=49.4528, LR=2.1e-02, kappa=1.0 (fixed), |gamma|=0.1805, time=13.4min
Epoch  360: Loss=49.2746, LR=1.9e-02, kappa=1.0 (fixed), |gamma|=0.1800, time=13.8min
Epoch  370: Loss=49.1141, LR=1.6e-02, kappa=1.0 (fixed), |gamma|=0.1796, time=14.2min
Epoch  380: Loss=48.9708, LR=1.4e-02, kappa=1.0 (fixed), |gamma|=0.1794, time=14.6min
Epoch  390: Loss=48.8438, LR=1.2e-02, kappa=1.0 (fixed), |gamma|=0.1793, time=14.9min
Epoch  400: Loss=48.7317, LR=1.0e-02, kappa=1.0 (fixed), |gamma|=0.1792, time=15.3min
Epoch  410: Loss=48.6326, LR=8.5e-03, kappa=1.0 (fixed), |gamma|=0.1791, time=15.7min
Epoch  420: Loss=48.5448, LR=7.0e-03, kappa=1.0 (fixed), |gamma|=0.1790, time=16.1min
Epoch  430: Loss=48.4719, LR=5.6e-03, kappa=1.0 (fixed), |gamma|=0.1788, time=16.4min
Epoch  440: Loss=48.4097, LR=4.4e-03, kappa=1.0 (fixed), |gamma|=0.1788, time=16.8min
Epoch  450: Loss=48.3569, LR=3.3e-03, kappa=1.0 (fixed), |gamma|=0.1787, time=17.2min
Epoch  460: Loss=48.3122, LR=2.5e-03, kappa=1.0 (fixed), |gamma|=0.1786, time=17.5min
Epoch  470: Loss=48.2742, LR=1.8e-03, kappa=1.0 (fixed), |gamma|=0.1786, time=17.9min
Epoch  480: Loss=48.2412, LR=1.4e-03, kappa=1.0 (fixed), |gamma|=0.1785, time=18.3min
Epoch  490: Loss=48.2118, LR=1.1e-03, kappa=1.0 (fixed), |gamma|=0.1785, time=18.7min

Training complete: 500 epochs in 19.1 min
Final loss: 48.1871

Final params: kappa=1.0 (fixed), mean|gamma|=0.1785
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2_nokappa/enrollment_model_REPARAM_NOKAPPA_W0.0001_batch_80000_90000.pt
Batch 9 done in 19.2 min

============================================================
BATCH 10/40: 90000-100000
============================================================

============================================================
REPARAM v2 NO KAPPA: samples 90000 to 100000
Epochs: 500, LR: 0.1, grad_clip: 5.0
kappa = 1.0 (FIXED, not learned)
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 7 diseases
Cluster 1: 20 diseases
Cluster 2: 8 diseases
Cluster 3: 19 diseases
Cluster 4: 7 diseases
Cluster 5: 22 diseases
Cluster 6: 12 diseases
Cluster 7: 15 diseases
Cluster 8: 9 diseases
Cluster 9: 5 diseases
Cluster 10: 9 diseases
Cluster 11: 12 diseases
Cluster 12: 26 diseases
Cluster 13: 15 diseases
Cluster 14: 5 diseases
Cluster 15: 13 diseases
Cluster 16: 8 diseases
Cluster 17: 22 diseases
Cluster 18: 20 diseases
Cluster 19: 94 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping (NO KAPPA)...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=63.9898, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.1011, time=0.0min
Epoch   10: Loss=219.9671, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0672, time=0.5min
Epoch   20: Loss=95.3495, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0633, time=0.9min
Epoch   30: Loss=64.9661, LR=9.9e-02, kappa=1.0 (fixed), |gamma|=0.0655, time=1.3min
Epoch   40: Loss=64.1321, LR=9.8e-02, kappa=1.0 (fixed), |gamma|=0.0709, time=1.7min
Epoch   50: Loss=61.8783, LR=9.7e-02, kappa=1.0 (fixed), |gamma|=0.0778, time=2.1min
Epoch   60: Loss=60.1618, LR=9.6e-02, kappa=1.0 (fixed), |gamma|=0.0842, time=2.5min
Epoch   70: Loss=59.1638, LR=9.5e-02, kappa=1.0 (fixed), |gamma|=0.0914, time=3.0min
Epoch   80: Loss=58.5238, LR=9.4e-02, kappa=1.0 (fixed), |gamma|=0.0982, time=3.4min
Epoch   90: Loss=58.0424, LR=9.2e-02, kappa=1.0 (fixed), |gamma|=0.1049, time=3.8min
Epoch  100: Loss=57.6208, LR=9.0e-02, kappa=1.0 (fixed), |gamma|=0.1123, time=4.2min
Epoch  110: Loss=57.2213, LR=8.8e-02, kappa=1.0 (fixed), |gamma|=0.1188, time=4.5min
Epoch  120: Loss=56.8398, LR=8.6e-02, kappa=1.0 (fixed), |gamma|=0.1244, time=4.9min
Epoch  130: Loss=56.4750, LR=8.4e-02, kappa=1.0 (fixed), |gamma|=0.1294, time=5.3min
Epoch  140: Loss=56.1174, LR=8.2e-02, kappa=1.0 (fixed), |gamma|=0.1344, time=5.7min
Epoch  150: Loss=55.7658, LR=7.9e-02, kappa=1.0 (fixed), |gamma|=0.1391, time=6.1min
Epoch  160: Loss=55.4203, LR=7.7e-02, kappa=1.0 (fixed), |gamma|=0.1433, time=6.4min
Epoch  170: Loss=55.0707, LR=7.4e-02, kappa=1.0 (fixed), |gamma|=0.1471, time=6.8min
Epoch  180: Loss=54.7138, LR=7.1e-02, kappa=1.0 (fixed), |gamma|=0.1506, time=7.2min
Epoch  190: Loss=54.3586, LR=6.8e-02, kappa=1.0 (fixed), |gamma|=0.1542, time=7.6min
Epoch  200: Loss=53.9986, LR=6.6e-02, kappa=1.0 (fixed), |gamma|=0.1576, time=8.0min
Epoch  210: Loss=53.6369, LR=6.3e-02, kappa=1.0 (fixed), |gamma|=0.1605, time=8.3min
Epoch  220: Loss=53.2704, LR=5.9e-02, kappa=1.0 (fixed), |gamma|=0.1631, time=8.7min
Epoch  230: Loss=52.9020, LR=5.6e-02, kappa=1.0 (fixed), |gamma|=0.1642, time=9.1min
Epoch  240: Loss=52.5329, LR=5.3e-02, kappa=1.0 (fixed), |gamma|=0.1640, time=9.5min
Epoch  250: Loss=52.1680, LR=5.0e-02, kappa=1.0 (fixed), |gamma|=0.1638, time=9.8min
Epoch  260: Loss=51.8042, LR=4.7e-02, kappa=1.0 (fixed), |gamma|=0.1639, time=10.2min
Epoch  270: Loss=51.4508, LR=4.4e-02, kappa=1.0 (fixed), |gamma|=0.1637, time=10.6min
Epoch  280: Loss=51.1087, LR=4.1e-02, kappa=1.0 (fixed), |gamma|=0.1633, time=10.9min
Epoch  290: Loss=50.7815, LR=3.8e-02, kappa=1.0 (fixed), |gamma|=0.1629, time=11.3min
Epoch  300: Loss=50.4712, LR=3.5e-02, kappa=1.0 (fixed), |gamma|=0.1625, time=11.7min
Epoch  310: Loss=50.1802, LR=3.2e-02, kappa=1.0 (fixed), |gamma|=0.1623, time=12.1min
Epoch  320: Loss=49.9099, LR=2.9e-02, kappa=1.0 (fixed), |gamma|=0.1620, time=12.4min
Epoch  330: Loss=49.6611, LR=2.6e-02, kappa=1.0 (fixed), |gamma|=0.1618, time=12.8min
Epoch  340: Loss=49.4339, LR=2.4e-02, kappa=1.0 (fixed), |gamma|=0.1616, time=13.1min
Epoch  350: Loss=49.2281, LR=2.1e-02, kappa=1.0 (fixed), |gamma|=0.1613, time=13.5min
Epoch  360: Loss=49.0431, LR=1.9e-02, kappa=1.0 (fixed), |gamma|=0.1610, time=13.8min
Epoch  370: Loss=48.8778, LR=1.6e-02, kappa=1.0 (fixed), |gamma|=0.1608, time=14.2min
Epoch  380: Loss=48.7314, LR=1.4e-02, kappa=1.0 (fixed), |gamma|=0.1606, time=14.5min
Epoch  390: Loss=48.6025, LR=1.2e-02, kappa=1.0 (fixed), |gamma|=0.1605, time=14.9min
Epoch  400: Loss=48.4898, LR=1.0e-02, kappa=1.0 (fixed), |gamma|=0.1603, time=15.2min
Epoch  410: Loss=48.3920, LR=8.5e-03, kappa=1.0 (fixed), |gamma|=0.1601, time=15.6min
Epoch  420: Loss=48.3079, LR=7.0e-03, kappa=1.0 (fixed), |gamma|=0.1600, time=15.9min
Epoch  430: Loss=48.2359, LR=5.6e-03, kappa=1.0 (fixed), |gamma|=0.1600, time=16.2min
Epoch  440: Loss=48.1747, LR=4.4e-03, kappa=1.0 (fixed), |gamma|=0.1599, time=16.6min
Epoch  450: Loss=48.1230, LR=3.3e-03, kappa=1.0 (fixed), |gamma|=0.1598, time=16.9min
Epoch  460: Loss=48.0793, LR=2.5e-03, kappa=1.0 (fixed), |gamma|=0.1598, time=17.3min
Epoch  470: Loss=48.0420, LR=1.8e-03, kappa=1.0 (fixed), |gamma|=0.1597, time=17.6min
Epoch  480: Loss=48.0098, LR=1.4e-03, kappa=1.0 (fixed), |gamma|=0.1597, time=17.9min
Epoch  490: Loss=47.9811, LR=1.1e-03, kappa=1.0 (fixed), |gamma|=0.1597, time=18.3min

Training complete: 500 epochs in 18.6 min
Final loss: 47.9568

Final params: kappa=1.0 (fixed), mean|gamma|=0.1597
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2_nokappa/enrollment_model_REPARAM_NOKAPPA_W0.0001_batch_90000_100000.pt
Batch 10 done in 18.8 min

============================================================
BATCH 11/40: 100000-110000
============================================================

============================================================
REPARAM v2 NO KAPPA: samples 100000 to 110000
Epochs: 500, LR: 0.1, grad_clip: 5.0
kappa = 1.0 (FIXED, not learned)
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 42 diseases
Cluster 1: 24 diseases
Cluster 2: 8 diseases
Cluster 3: 5 diseases
Cluster 4: 8 diseases
Cluster 5: 13 diseases
Cluster 6: 60 diseases
Cluster 7: 30 diseases
Cluster 8: 7 diseases
Cluster 9: 19 diseases
Cluster 10: 10 diseases
Cluster 11: 15 diseases
Cluster 12: 16 diseases
Cluster 13: 11 diseases
Cluster 14: 7 diseases
Cluster 15: 7 diseases
Cluster 16: 12 diseases
Cluster 17: 16 diseases
Cluster 18: 22 diseases
Cluster 19: 16 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping (NO KAPPA)...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=63.1875, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.1010, time=0.0min
Epoch   10: Loss=219.2429, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0682, time=0.4min
Epoch   20: Loss=94.5873, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0694, time=0.8min
Epoch   30: Loss=64.2014, LR=9.9e-02, kappa=1.0 (fixed), |gamma|=0.0729, time=1.2min
Epoch   40: Loss=63.3966, LR=9.8e-02, kappa=1.0 (fixed), |gamma|=0.0754, time=1.6min
Epoch   50: Loss=61.1648, LR=9.7e-02, kappa=1.0 (fixed), |gamma|=0.0811, time=2.0min
Epoch   60: Loss=59.4664, LR=9.6e-02, kappa=1.0 (fixed), |gamma|=0.0872, time=2.4min
Epoch   70: Loss=58.4832, LR=9.5e-02, kappa=1.0 (fixed), |gamma|=0.0945, time=2.7min
Epoch   80: Loss=57.8546, LR=9.4e-02, kappa=1.0 (fixed), |gamma|=0.1026, time=3.1min
Epoch   90: Loss=57.3806, LR=9.2e-02, kappa=1.0 (fixed), |gamma|=0.1094, time=3.4min
Epoch  100: Loss=56.9658, LR=9.0e-02, kappa=1.0 (fixed), |gamma|=0.1174, time=3.8min
Epoch  110: Loss=56.5726, LR=8.8e-02, kappa=1.0 (fixed), |gamma|=0.1257, time=4.2min
Epoch  120: Loss=56.1976, LR=8.6e-02, kappa=1.0 (fixed), |gamma|=0.1335, time=4.6min
Epoch  130: Loss=55.8388, LR=8.4e-02, kappa=1.0 (fixed), |gamma|=0.1415, time=4.9min
Epoch  140: Loss=55.4910, LR=8.2e-02, kappa=1.0 (fixed), |gamma|=0.1496, time=5.3min
Epoch  150: Loss=55.1498, LR=7.9e-02, kappa=1.0 (fixed), |gamma|=0.1576, time=5.6min
Epoch  160: Loss=54.8150, LR=7.7e-02, kappa=1.0 (fixed), |gamma|=0.1669, time=6.0min
Epoch  170: Loss=54.4809, LR=7.4e-02, kappa=1.0 (fixed), |gamma|=0.1768, time=6.3min
Epoch  180: Loss=54.1496, LR=7.1e-02, kappa=1.0 (fixed), |gamma|=0.1835, time=6.7min
Epoch  190: Loss=53.8168, LR=6.8e-02, kappa=1.0 (fixed), |gamma|=0.1911, time=7.0min
Epoch  200: Loss=53.4843, LR=6.6e-02, kappa=1.0 (fixed), |gamma|=0.1965, time=7.4min
Epoch  210: Loss=53.1495, LR=6.3e-02, kappa=1.0 (fixed), |gamma|=0.2014, time=7.7min
Epoch  220: Loss=52.8128, LR=5.9e-02, kappa=1.0 (fixed), |gamma|=0.2053, time=8.1min
Epoch  230: Loss=52.4773, LR=5.6e-02, kappa=1.0 (fixed), |gamma|=0.2079, time=8.4min
Epoch  240: Loss=52.1429, LR=5.3e-02, kappa=1.0 (fixed), |gamma|=0.2095, time=8.8min
Epoch  250: Loss=51.8116, LR=5.0e-02, kappa=1.0 (fixed), |gamma|=0.2100, time=9.1min
Epoch  260: Loss=51.4863, LR=4.7e-02, kappa=1.0 (fixed), |gamma|=0.2097, time=9.5min
Epoch  270: Loss=51.1695, LR=4.4e-02, kappa=1.0 (fixed), |gamma|=0.2088, time=9.8min
Epoch  280: Loss=50.8653, LR=4.1e-02, kappa=1.0 (fixed), |gamma|=0.2076, time=10.2min
Epoch  290: Loss=50.5745, LR=3.8e-02, kappa=1.0 (fixed), |gamma|=0.2063, time=10.5min
Epoch  300: Loss=50.3025, LR=3.5e-02, kappa=1.0 (fixed), |gamma|=0.2056, time=10.9min
Epoch  310: Loss=50.0486, LR=3.2e-02, kappa=1.0 (fixed), |gamma|=0.2045, time=11.2min
Epoch  320: Loss=49.8135, LR=2.9e-02, kappa=1.0 (fixed), |gamma|=0.2039, time=11.6min
Epoch  330: Loss=49.5972, LR=2.6e-02, kappa=1.0 (fixed), |gamma|=0.2033, time=11.9min
Epoch  340: Loss=49.3996, LR=2.4e-02, kappa=1.0 (fixed), |gamma|=0.2029, time=12.3min
Epoch  350: Loss=49.2201, LR=2.1e-02, kappa=1.0 (fixed), |gamma|=0.2024, time=12.6min
Epoch  360: Loss=49.0583, LR=1.9e-02, kappa=1.0 (fixed), |gamma|=0.2021, time=13.0min
Epoch  370: Loss=48.9132, LR=1.6e-02, kappa=1.0 (fixed), |gamma|=0.2018, time=13.3min
Epoch  380: Loss=48.7842, LR=1.4e-02, kappa=1.0 (fixed), |gamma|=0.2015, time=13.7min
Epoch  390: Loss=48.6702, LR=1.2e-02, kappa=1.0 (fixed), |gamma|=0.2013, time=14.0min
Epoch  400: Loss=48.5703, LR=1.0e-02, kappa=1.0 (fixed), |gamma|=0.2011, time=14.3min
Epoch  410: Loss=48.4834, LR=8.5e-03, kappa=1.0 (fixed), |gamma|=0.2009, time=14.7min
Epoch  420: Loss=48.4085, LR=7.0e-03, kappa=1.0 (fixed), |gamma|=0.2008, time=15.0min
Epoch  430: Loss=48.3444, LR=5.6e-03, kappa=1.0 (fixed), |gamma|=0.2007, time=15.4min
Epoch  440: Loss=48.2899, LR=4.4e-03, kappa=1.0 (fixed), |gamma|=0.2007, time=15.8min
Epoch  450: Loss=48.2438, LR=3.3e-03, kappa=1.0 (fixed), |gamma|=0.2006, time=16.1min
Epoch  460: Loss=48.2050, LR=2.5e-03, kappa=1.0 (fixed), |gamma|=0.2006, time=16.5min
Epoch  470: Loss=48.1721, LR=1.8e-03, kappa=1.0 (fixed), |gamma|=0.2006, time=16.9min
Epoch  480: Loss=48.1437, LR=1.4e-03, kappa=1.0 (fixed), |gamma|=0.2007, time=17.2min
Epoch  490: Loss=48.1185, LR=1.1e-03, kappa=1.0 (fixed), |gamma|=0.2007, time=17.6min

Training complete: 500 epochs in 17.9 min
Final loss: 48.0973

Final params: kappa=1.0 (fixed), mean|gamma|=0.2007
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2_nokappa/enrollment_model_REPARAM_NOKAPPA_W0.0001_batch_100000_110000.pt
Batch 11 done in 18.1 min

============================================================
BATCH 12/40: 110000-120000
============================================================

============================================================
REPARAM v2 NO KAPPA: samples 110000 to 120000
Epochs: 500, LR: 0.1, grad_clip: 5.0
kappa = 1.0 (FIXED, not learned)
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 11 diseases
Cluster 1: 27 diseases
Cluster 2: 9 diseases
Cluster 3: 22 diseases
Cluster 4: 13 diseases
Cluster 5: 4 diseases
Cluster 6: 13 diseases
Cluster 7: 8 diseases
Cluster 8: 15 diseases
Cluster 9: 13 diseases
Cluster 10: 27 diseases
Cluster 11: 6 diseases
Cluster 12: 12 diseases
Cluster 13: 16 diseases
Cluster 14: 9 diseases
Cluster 15: 28 diseases
Cluster 16: 8 diseases
Cluster 17: 15 diseases
Cluster 18: 87 diseases
Cluster 19: 5 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping (NO KAPPA)...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=62.7846, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.1011, time=0.0min
Epoch   10: Loss=218.8401, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0716, time=0.4min
Epoch   20: Loss=94.1967, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0723, time=0.8min
Epoch   30: Loss=63.8133, LR=9.9e-02, kappa=1.0 (fixed), |gamma|=0.0761, time=1.1min
Epoch   40: Loss=63.0155, LR=9.8e-02, kappa=1.0 (fixed), |gamma|=0.0837, time=1.5min
Epoch   50: Loss=60.7905, LR=9.7e-02, kappa=1.0 (fixed), |gamma|=0.0905, time=1.9min
Epoch   60: Loss=59.1003, LR=9.6e-02, kappa=1.0 (fixed), |gamma|=0.0959, time=2.2min
Epoch   70: Loss=58.1260, LR=9.5e-02, kappa=1.0 (fixed), |gamma|=0.1015, time=2.6min
Epoch   80: Loss=57.5058, LR=9.4e-02, kappa=1.0 (fixed), |gamma|=0.1078, time=3.0min
Epoch   90: Loss=57.0414, LR=9.2e-02, kappa=1.0 (fixed), |gamma|=0.1133, time=3.3min
Epoch  100: Loss=56.6353, LR=9.0e-02, kappa=1.0 (fixed), |gamma|=0.1185, time=3.7min
Epoch  110: Loss=56.2507, LR=8.8e-02, kappa=1.0 (fixed), |gamma|=0.1236, time=4.1min
Epoch  120: Loss=55.8837, LR=8.6e-02, kappa=1.0 (fixed), |gamma|=0.1285, time=4.4min
Epoch  130: Loss=55.5327, LR=8.4e-02, kappa=1.0 (fixed), |gamma|=0.1332, time=4.7min
Epoch  140: Loss=55.1915, LR=8.2e-02, kappa=1.0 (fixed), |gamma|=0.1379, time=5.1min
Epoch  150: Loss=54.8559, LR=7.9e-02, kappa=1.0 (fixed), |gamma|=0.1423, time=5.4min
Epoch  160: Loss=54.5200, LR=7.7e-02, kappa=1.0 (fixed), |gamma|=0.1473, time=5.7min
Epoch  170: Loss=54.1891, LR=7.4e-02, kappa=1.0 (fixed), |gamma|=0.1534, time=6.0min
Epoch  180: Loss=53.8582, LR=7.1e-02, kappa=1.0 (fixed), |gamma|=0.1593, time=6.4min
Epoch  190: Loss=53.5260, LR=6.8e-02, kappa=1.0 (fixed), |gamma|=0.1648, time=6.7min
Epoch  200: Loss=53.1859, LR=6.6e-02, kappa=1.0 (fixed), |gamma|=0.1700, time=7.0min
Epoch  210: Loss=52.8415, LR=6.3e-02, kappa=1.0 (fixed), |gamma|=0.1740, time=7.3min
Epoch  220: Loss=52.4815, LR=5.9e-02, kappa=1.0 (fixed), |gamma|=0.1768, time=7.7min
Epoch  230: Loss=52.1173, LR=5.6e-02, kappa=1.0 (fixed), |gamma|=0.1782, time=8.0min
Epoch  240: Loss=51.7466, LR=5.3e-02, kappa=1.0 (fixed), |gamma|=0.1784, time=8.3min
Epoch  250: Loss=51.3740, LR=5.0e-02, kappa=1.0 (fixed), |gamma|=0.1776, time=8.7min
Epoch  260: Loss=51.0054, LR=4.7e-02, kappa=1.0 (fixed), |gamma|=0.1761, time=9.0min
Epoch  270: Loss=50.6450, LR=4.4e-02, kappa=1.0 (fixed), |gamma|=0.1740, time=9.3min
Epoch  280: Loss=50.2963, LR=4.1e-02, kappa=1.0 (fixed), |gamma|=0.1716, time=9.6min
Epoch  290: Loss=49.9646, LR=3.8e-02, kappa=1.0 (fixed), |gamma|=0.1694, time=10.0min
Epoch  300: Loss=49.6530, LR=3.5e-02, kappa=1.0 (fixed), |gamma|=0.1673, time=10.3min
Epoch  310: Loss=49.3632, LR=3.2e-02, kappa=1.0 (fixed), |gamma|=0.1651, time=10.6min
Epoch  320: Loss=49.0960, LR=2.9e-02, kappa=1.0 (fixed), |gamma|=0.1630, time=11.0min
Epoch  330: Loss=48.8517, LR=2.6e-02, kappa=1.0 (fixed), |gamma|=0.1610, time=11.3min
Epoch  340: Loss=48.6299, LR=2.4e-02, kappa=1.0 (fixed), |gamma|=0.1593, time=11.6min
Epoch  350: Loss=48.4299, LR=2.1e-02, kappa=1.0 (fixed), |gamma|=0.1578, time=11.9min
Epoch  360: Loss=48.2508, LR=1.9e-02, kappa=1.0 (fixed), |gamma|=0.1564, time=12.3min
Epoch  370: Loss=48.0913, LR=1.6e-02, kappa=1.0 (fixed), |gamma|=0.1552, time=12.6min
Epoch  380: Loss=47.9503, LR=1.4e-02, kappa=1.0 (fixed), |gamma|=0.1542, time=12.9min
Epoch  390: Loss=47.8264, LR=1.2e-02, kappa=1.0 (fixed), |gamma|=0.1533, time=13.3min
Epoch  400: Loss=47.7182, LR=1.0e-02, kappa=1.0 (fixed), |gamma|=0.1525, time=13.6min
Epoch  410: Loss=47.6244, LR=8.5e-03, kappa=1.0 (fixed), |gamma|=0.1519, time=13.9min
Epoch  420: Loss=47.5438, LR=7.0e-03, kappa=1.0 (fixed), |gamma|=0.1513, time=14.2min
Epoch  430: Loss=47.4748, LR=5.6e-03, kappa=1.0 (fixed), |gamma|=0.1509, time=14.6min
Epoch  440: Loss=47.4162, LR=4.4e-03, kappa=1.0 (fixed), |gamma|=0.1506, time=14.9min
Epoch  450: Loss=47.3667, LR=3.3e-03, kappa=1.0 (fixed), |gamma|=0.1503, time=15.2min
Epoch  460: Loss=47.3247, LR=2.5e-03, kappa=1.0 (fixed), |gamma|=0.1501, time=15.6min
Epoch  470: Loss=47.2890, LR=1.8e-03, kappa=1.0 (fixed), |gamma|=0.1499, time=15.9min
Epoch  480: Loss=47.2581, LR=1.4e-03, kappa=1.0 (fixed), |gamma|=0.1498, time=16.2min
Epoch  490: Loss=47.2306, LR=1.1e-03, kappa=1.0 (fixed), |gamma|=0.1497, time=16.6min

Training complete: 500 epochs in 16.9 min
Final loss: 47.2073

Final params: kappa=1.0 (fixed), mean|gamma|=0.1496
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2_nokappa/enrollment_model_REPARAM_NOKAPPA_W0.0001_batch_110000_120000.pt
Batch 12 done in 17.0 min

============================================================
BATCH 13/40: 120000-130000
============================================================

============================================================
REPARAM v2 NO KAPPA: samples 120000 to 130000
Epochs: 500, LR: 0.1, grad_clip: 5.0
kappa = 1.0 (FIXED, not learned)
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 11 diseases
Cluster 1: 6 diseases
Cluster 2: 5 diseases
Cluster 3: 18 diseases
Cluster 4: 8 diseases
Cluster 5: 6 diseases
Cluster 6: 26 diseases
Cluster 7: 97 diseases
Cluster 8: 27 diseases
Cluster 9: 12 diseases
Cluster 10: 14 diseases
Cluster 11: 15 diseases
Cluster 12: 14 diseases
Cluster 13: 6 diseases
Cluster 14: 5 diseases
Cluster 15: 18 diseases
Cluster 16: 7 diseases
Cluster 17: 9 diseases
Cluster 18: 15 diseases
Cluster 19: 29 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping (NO KAPPA)...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=63.3996, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.1009, time=0.0min
Epoch   10: Loss=219.4454, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0654, time=0.4min
Epoch   20: Loss=94.7826, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0633, time=0.8min
Epoch   30: Loss=64.3925, LR=9.9e-02, kappa=1.0 (fixed), |gamma|=0.0662, time=1.1min
Epoch   40: Loss=63.5829, LR=9.8e-02, kappa=1.0 (fixed), |gamma|=0.0706, time=1.5min
Epoch   50: Loss=61.3402, LR=9.7e-02, kappa=1.0 (fixed), |gamma|=0.0772, time=1.8min
Epoch   60: Loss=59.6324, LR=9.6e-02, kappa=1.0 (fixed), |gamma|=0.0834, time=2.2min
Epoch   70: Loss=58.6420, LR=9.5e-02, kappa=1.0 (fixed), |gamma|=0.0897, time=2.5min
Epoch   80: Loss=58.0081, LR=9.4e-02, kappa=1.0 (fixed), |gamma|=0.0974, time=2.9min
Epoch   90: Loss=57.5303, LR=9.2e-02, kappa=1.0 (fixed), |gamma|=0.1065, time=3.2min
Epoch  100: Loss=57.1113, LR=9.0e-02, kappa=1.0 (fixed), |gamma|=0.1162, time=3.6min
Epoch  110: Loss=56.7144, LR=8.8e-02, kappa=1.0 (fixed), |gamma|=0.1247, time=3.9min
Epoch  120: Loss=56.3354, LR=8.6e-02, kappa=1.0 (fixed), |gamma|=0.1320, time=4.2min
Epoch  130: Loss=55.9707, LR=8.4e-02, kappa=1.0 (fixed), |gamma|=0.1381, time=4.5min
Epoch  140: Loss=55.6175, LR=8.2e-02, kappa=1.0 (fixed), |gamma|=0.1449, time=4.9min
Epoch  150: Loss=55.2714, LR=7.9e-02, kappa=1.0 (fixed), |gamma|=0.1519, time=5.2min
Epoch  160: Loss=54.9315, LR=7.7e-02, kappa=1.0 (fixed), |gamma|=0.1598, time=5.5min
Epoch  170: Loss=54.5948, LR=7.4e-02, kappa=1.0 (fixed), |gamma|=0.1689, time=5.8min
Epoch  180: Loss=54.2638, LR=7.1e-02, kappa=1.0 (fixed), |gamma|=0.1780, time=6.2min
Epoch  190: Loss=53.9283, LR=6.8e-02, kappa=1.0 (fixed), |gamma|=0.1857, time=6.5min
Epoch  200: Loss=53.5875, LR=6.6e-02, kappa=1.0 (fixed), |gamma|=0.1919, time=6.8min
Epoch  210: Loss=53.2445, LR=6.3e-02, kappa=1.0 (fixed), |gamma|=0.1962, time=7.2min
Epoch  220: Loss=52.8954, LR=5.9e-02, kappa=1.0 (fixed), |gamma|=0.1992, time=7.5min
Epoch  230: Loss=52.5417, LR=5.6e-02, kappa=1.0 (fixed), |gamma|=0.2007, time=7.8min
Epoch  240: Loss=52.1842, LR=5.3e-02, kappa=1.0 (fixed), |gamma|=0.2007, time=8.2min
Epoch  250: Loss=51.8245, LR=5.0e-02, kappa=1.0 (fixed), |gamma|=0.1996, time=8.5min
Epoch  260: Loss=51.4689, LR=4.7e-02, kappa=1.0 (fixed), |gamma|=0.1979, time=8.8min
Epoch  270: Loss=51.1204, LR=4.4e-02, kappa=1.0 (fixed), |gamma|=0.1954, time=9.2min
Epoch  280: Loss=50.7841, LR=4.1e-02, kappa=1.0 (fixed), |gamma|=0.1932, time=9.5min
Epoch  290: Loss=50.4636, LR=3.8e-02, kappa=1.0 (fixed), |gamma|=0.1910, time=9.8min
Epoch  300: Loss=50.1589, LR=3.5e-02, kappa=1.0 (fixed), |gamma|=0.1888, time=10.2min
Epoch  310: Loss=49.8748, LR=3.2e-02, kappa=1.0 (fixed), |gamma|=0.1868, time=10.5min
Epoch  320: Loss=49.6120, LR=2.9e-02, kappa=1.0 (fixed), |gamma|=0.1849, time=10.8min
Epoch  330: Loss=49.3710, LR=2.6e-02, kappa=1.0 (fixed), |gamma|=0.1832, time=11.2min
Epoch  340: Loss=49.1516, LR=2.4e-02, kappa=1.0 (fixed), |gamma|=0.1818, time=11.5min
Epoch  350: Loss=48.9533, LR=2.1e-02, kappa=1.0 (fixed), |gamma|=0.1805, time=11.8min
Epoch  360: Loss=48.7753, LR=1.9e-02, kappa=1.0 (fixed), |gamma|=0.1794, time=12.2min
Epoch  370: Loss=48.6166, LR=1.6e-02, kappa=1.0 (fixed), |gamma|=0.1785, time=12.5min
Epoch  380: Loss=48.4761, LR=1.4e-02, kappa=1.0 (fixed), |gamma|=0.1777, time=12.8min
Epoch  390: Loss=48.3525, LR=1.2e-02, kappa=1.0 (fixed), |gamma|=0.1769, time=13.2min
Epoch  400: Loss=48.2445, LR=1.0e-02, kappa=1.0 (fixed), |gamma|=0.1763, time=13.5min
Epoch  410: Loss=48.1509, LR=8.5e-03, kappa=1.0 (fixed), |gamma|=0.1758, time=13.8min
Epoch  420: Loss=48.0703, LR=7.0e-03, kappa=1.0 (fixed), |gamma|=0.1753, time=14.2min
Epoch  430: Loss=48.0014, LR=5.6e-03, kappa=1.0 (fixed), |gamma|=0.1749, time=14.5min
Epoch  440: Loss=47.9429, LR=4.4e-03, kappa=1.0 (fixed), |gamma|=0.1746, time=14.8min
Epoch  450: Loss=47.8934, LR=3.3e-03, kappa=1.0 (fixed), |gamma|=0.1744, time=15.2min
Epoch  460: Loss=47.8515, LR=2.5e-03, kappa=1.0 (fixed), |gamma|=0.1742, time=15.5min
Epoch  470: Loss=47.8158, LR=1.8e-03, kappa=1.0 (fixed), |gamma|=0.1741, time=15.8min
Epoch  480: Loss=47.7850, LR=1.4e-03, kappa=1.0 (fixed), |gamma|=0.1740, time=16.2min
Epoch  490: Loss=47.7575, LR=1.1e-03, kappa=1.0 (fixed), |gamma|=0.1740, time=16.5min

Training complete: 500 epochs in 16.8 min
Final loss: 47.7343

Final params: kappa=1.0 (fixed), mean|gamma|=0.1739
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2_nokappa/enrollment_model_REPARAM_NOKAPPA_W0.0001_batch_120000_130000.pt
Batch 13 done in 17.0 min

============================================================
BATCH 14/40: 130000-140000
============================================================

============================================================
REPARAM v2 NO KAPPA: samples 130000 to 140000
Epochs: 500, LR: 0.1, grad_clip: 5.0
kappa = 1.0 (FIXED, not learned)
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 13 diseases
Cluster 1: 21 diseases
Cluster 2: 16 diseases
Cluster 3: 13 diseases
Cluster 4: 13 diseases
Cluster 5: 6 diseases
Cluster 6: 9 diseases
Cluster 7: 87 diseases
Cluster 8: 8 diseases
Cluster 9: 11 diseases
Cluster 10: 12 diseases
Cluster 11: 25 diseases
Cluster 12: 11 diseases
Cluster 13: 5 diseases
Cluster 14: 8 diseases
Cluster 15: 31 diseases
Cluster 16: 18 diseases
Cluster 17: 27 diseases
Cluster 18: 7 diseases
Cluster 19: 7 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping (NO KAPPA)...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=63.6466, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.1009, time=0.0min
Epoch   10: Loss=219.6688, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0669, time=0.4min
Epoch   20: Loss=95.0198, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0659, time=0.8min
Epoch   30: Loss=64.6305, LR=9.9e-02, kappa=1.0 (fixed), |gamma|=0.0693, time=1.2min
Epoch   40: Loss=63.8227, LR=9.8e-02, kappa=1.0 (fixed), |gamma|=0.0746, time=1.5min
Epoch   50: Loss=61.5791, LR=9.7e-02, kappa=1.0 (fixed), |gamma|=0.0833, time=1.9min
Epoch   60: Loss=59.8696, LR=9.6e-02, kappa=1.0 (fixed), |gamma|=0.0913, time=2.2min
Epoch   70: Loss=58.8763, LR=9.5e-02, kappa=1.0 (fixed), |gamma|=0.1002, time=2.6min
Epoch   80: Loss=58.2384, LR=9.4e-02, kappa=1.0 (fixed), |gamma|=0.1084, time=3.0min
Epoch   90: Loss=57.7574, LR=9.2e-02, kappa=1.0 (fixed), |gamma|=0.1158, time=3.3min
Epoch  100: Loss=57.3360, LR=9.0e-02, kappa=1.0 (fixed), |gamma|=0.1226, time=3.7min
Epoch  110: Loss=56.9371, LR=8.8e-02, kappa=1.0 (fixed), |gamma|=0.1288, time=4.1min
Epoch  120: Loss=56.5573, LR=8.6e-02, kappa=1.0 (fixed), |gamma|=0.1347, time=4.5min
Epoch  130: Loss=56.1939, LR=8.4e-02, kappa=1.0 (fixed), |gamma|=0.1395, time=4.8min
Epoch  140: Loss=55.8412, LR=8.2e-02, kappa=1.0 (fixed), |gamma|=0.1454, time=5.2min
Epoch  150: Loss=55.4972, LR=7.9e-02, kappa=1.0 (fixed), |gamma|=0.1519, time=5.6min
Epoch  160: Loss=55.1582, LR=7.7e-02, kappa=1.0 (fixed), |gamma|=0.1585, time=6.0min
Epoch  170: Loss=54.8218, LR=7.4e-02, kappa=1.0 (fixed), |gamma|=0.1655, time=6.4min
Epoch  180: Loss=54.4846, LR=7.1e-02, kappa=1.0 (fixed), |gamma|=0.1719, time=6.7min
Epoch  190: Loss=54.1498, LR=6.8e-02, kappa=1.0 (fixed), |gamma|=0.1775, time=7.1min
Epoch  200: Loss=53.7971, LR=6.6e-02, kappa=1.0 (fixed), |gamma|=0.1820, time=7.5min
Epoch  210: Loss=53.4416, LR=6.3e-02, kappa=1.0 (fixed), |gamma|=0.1854, time=7.9min
Epoch  220: Loss=53.0811, LR=5.9e-02, kappa=1.0 (fixed), |gamma|=0.1874, time=8.2min
Epoch  230: Loss=52.7046, LR=5.6e-02, kappa=1.0 (fixed), |gamma|=0.1880, time=8.6min
Epoch  240: Loss=52.3247, LR=5.3e-02, kappa=1.0 (fixed), |gamma|=0.1871, time=9.0min
Epoch  250: Loss=51.9407, LR=5.0e-02, kappa=1.0 (fixed), |gamma|=0.1850, time=9.4min
Epoch  260: Loss=51.5611, LR=4.7e-02, kappa=1.0 (fixed), |gamma|=0.1823, time=9.8min
Epoch  270: Loss=51.1865, LR=4.4e-02, kappa=1.0 (fixed), |gamma|=0.1794, time=10.2min
Epoch  280: Loss=50.8257, LR=4.1e-02, kappa=1.0 (fixed), |gamma|=0.1763, time=10.6min
Epoch  290: Loss=50.4832, LR=3.8e-02, kappa=1.0 (fixed), |gamma|=0.1733, time=10.9min
Epoch  300: Loss=50.1615, LR=3.5e-02, kappa=1.0 (fixed), |gamma|=0.1704, time=11.3min
Epoch  310: Loss=49.8627, LR=3.2e-02, kappa=1.0 (fixed), |gamma|=0.1677, time=11.7min
Epoch  320: Loss=49.5878, LR=2.9e-02, kappa=1.0 (fixed), |gamma|=0.1653, time=12.0min
Epoch  330: Loss=49.3370, LR=2.6e-02, kappa=1.0 (fixed), |gamma|=0.1631, time=12.4min
Epoch  340: Loss=49.1099, LR=2.4e-02, kappa=1.0 (fixed), |gamma|=0.1613, time=12.7min
Epoch  350: Loss=48.9056, LR=2.1e-02, kappa=1.0 (fixed), |gamma|=0.1597, time=13.1min
Epoch  360: Loss=48.7230, LR=1.9e-02, kappa=1.0 (fixed), |gamma|=0.1583, time=13.5min
Epoch  370: Loss=48.5608, LR=1.6e-02, kappa=1.0 (fixed), |gamma|=0.1572, time=13.8min
Epoch  380: Loss=48.4175, LR=1.4e-02, kappa=1.0 (fixed), |gamma|=0.1561, time=14.2min
Epoch  390: Loss=48.2918, LR=1.2e-02, kappa=1.0 (fixed), |gamma|=0.1552, time=14.6min
Epoch  400: Loss=48.1823, LR=1.0e-02, kappa=1.0 (fixed), |gamma|=0.1545, time=15.0min
Epoch  410: Loss=48.0875, LR=8.5e-03, kappa=1.0 (fixed), |gamma|=0.1538, time=15.3min
Epoch  420: Loss=48.0059, LR=7.0e-03, kappa=1.0 (fixed), |gamma|=0.1533, time=15.7min
Epoch  430: Loss=47.9363, LR=5.6e-03, kappa=1.0 (fixed), |gamma|=0.1529, time=16.1min
Epoch  440: Loss=47.8772, LR=4.4e-03, kappa=1.0 (fixed), |gamma|=0.1525, time=16.5min
Epoch  450: Loss=47.8273, LR=3.3e-03, kappa=1.0 (fixed), |gamma|=0.1523, time=16.9min
Epoch  460: Loss=47.7850, LR=2.5e-03, kappa=1.0 (fixed), |gamma|=0.1521, time=17.2min
Epoch  470: Loss=47.7491, LR=1.8e-03, kappa=1.0 (fixed), |gamma|=0.1519, time=17.6min
Epoch  480: Loss=47.7179, LR=1.4e-03, kappa=1.0 (fixed), |gamma|=0.1518, time=18.0min
Epoch  490: Loss=47.6902, LR=1.1e-03, kappa=1.0 (fixed), |gamma|=0.1517, time=18.4min

Training complete: 500 epochs in 18.7 min
Final loss: 47.6668

Final params: kappa=1.0 (fixed), mean|gamma|=0.1516
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2_nokappa/enrollment_model_REPARAM_NOKAPPA_W0.0001_batch_130000_140000.pt
Batch 14 done in 18.9 min

============================================================
BATCH 15/40: 140000-150000
============================================================

============================================================
REPARAM v2 NO KAPPA: samples 140000 to 150000
Epochs: 500, LR: 0.1, grad_clip: 5.0
kappa = 1.0 (FIXED, not learned)
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 18 diseases
Cluster 1: 11 diseases
Cluster 2: 20 diseases
Cluster 3: 8 diseases
Cluster 4: 8 diseases
Cluster 5: 5 diseases
Cluster 6: 26 diseases
Cluster 7: 19 diseases
Cluster 8: 5 diseases
Cluster 9: 28 diseases
Cluster 10: 11 diseases
Cluster 11: 9 diseases
Cluster 12: 16 diseases
Cluster 13: 14 diseases
Cluster 14: 14 diseases
Cluster 15: 15 diseases
Cluster 16: 8 diseases
Cluster 17: 95 diseases
Cluster 18: 11 diseases
Cluster 19: 7 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping (NO KAPPA)...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=64.2038, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.1009, time=0.0min
Epoch   10: Loss=220.1351, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0669, time=0.4min
Epoch   20: Loss=95.5379, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0670, time=0.8min
Epoch   30: Loss=65.1570, LR=9.9e-02, kappa=1.0 (fixed), |gamma|=0.0675, time=1.2min
Epoch   40: Loss=64.3197, LR=9.8e-02, kappa=1.0 (fixed), |gamma|=0.0719, time=1.5min
Epoch   50: Loss=62.0639, LR=9.7e-02, kappa=1.0 (fixed), |gamma|=0.0780, time=1.9min
Epoch   60: Loss=60.3444, LR=9.6e-02, kappa=1.0 (fixed), |gamma|=0.0840, time=2.3min
Epoch   70: Loss=59.3425, LR=9.5e-02, kappa=1.0 (fixed), |gamma|=0.0918, time=2.7min
Epoch   80: Loss=58.6964, LR=9.4e-02, kappa=1.0 (fixed), |gamma|=0.1003, time=3.0min
Epoch   90: Loss=58.2078, LR=9.2e-02, kappa=1.0 (fixed), |gamma|=0.1085, time=3.4min
Epoch  100: Loss=57.7795, LR=9.0e-02, kappa=1.0 (fixed), |gamma|=0.1162, time=3.8min
Epoch  110: Loss=57.3743, LR=8.8e-02, kappa=1.0 (fixed), |gamma|=0.1238, time=4.2min
Epoch  120: Loss=56.9878, LR=8.6e-02, kappa=1.0 (fixed), |gamma|=0.1320, time=4.6min
Epoch  130: Loss=56.6188, LR=8.4e-02, kappa=1.0 (fixed), |gamma|=0.1410, time=5.0min
Epoch  140: Loss=56.2635, LR=8.2e-02, kappa=1.0 (fixed), |gamma|=0.1506, time=5.3min
Epoch  150: Loss=55.9174, LR=7.9e-02, kappa=1.0 (fixed), |gamma|=0.1602, time=5.7min
Epoch  160: Loss=55.5783, LR=7.7e-02, kappa=1.0 (fixed), |gamma|=0.1695, time=6.1min
Epoch  170: Loss=55.2473, LR=7.4e-02, kappa=1.0 (fixed), |gamma|=0.1788, time=6.5min
Epoch  180: Loss=54.9169, LR=7.1e-02, kappa=1.0 (fixed), |gamma|=0.1881, time=6.9min
Epoch  190: Loss=54.5909, LR=6.8e-02, kappa=1.0 (fixed), |gamma|=0.1969, time=7.2min
Epoch  200: Loss=54.2717, LR=6.6e-02, kappa=1.0 (fixed), |gamma|=0.2054, time=7.6min
Epoch  210: Loss=53.9478, LR=6.3e-02, kappa=1.0 (fixed), |gamma|=0.2131, time=8.0min
Epoch  220: Loss=53.6275, LR=5.9e-02, kappa=1.0 (fixed), |gamma|=0.2209, time=8.4min
Epoch  230: Loss=53.3059, LR=5.6e-02, kappa=1.0 (fixed), |gamma|=0.2292, time=8.8min
Epoch  240: Loss=52.9852, LR=5.3e-02, kappa=1.0 (fixed), |gamma|=0.2366, time=9.2min
Epoch  250: Loss=52.6652, LR=5.0e-02, kappa=1.0 (fixed), |gamma|=0.2410, time=9.6min
Epoch  260: Loss=52.3455, LR=4.7e-02, kappa=1.0 (fixed), |gamma|=0.2408, time=9.9min
Epoch  270: Loss=52.0255, LR=4.4e-02, kappa=1.0 (fixed), |gamma|=0.2379, time=10.3min
Epoch  280: Loss=51.7103, LR=4.1e-02, kappa=1.0 (fixed), |gamma|=0.2350, time=10.7min
Epoch  290: Loss=51.4042, LR=3.8e-02, kappa=1.0 (fixed), |gamma|=0.2326, time=11.1min
Epoch  300: Loss=51.1111, LR=3.5e-02, kappa=1.0 (fixed), |gamma|=0.2303, time=11.5min
Epoch  310: Loss=50.8334, LR=3.2e-02, kappa=1.0 (fixed), |gamma|=0.2281, time=11.9min
Epoch  320: Loss=50.5735, LR=2.9e-02, kappa=1.0 (fixed), |gamma|=0.2260, time=12.3min
Epoch  330: Loss=50.3297, LR=2.6e-02, kappa=1.0 (fixed), |gamma|=0.2247, time=12.7min
Epoch  340: Loss=50.1054, LR=2.4e-02, kappa=1.0 (fixed), |gamma|=0.2226, time=13.1min
Epoch  350: Loss=49.9002, LR=2.1e-02, kappa=1.0 (fixed), |gamma|=0.2210, time=13.5min
Epoch  360: Loss=49.7148, LR=1.9e-02, kappa=1.0 (fixed), |gamma|=0.2196, time=13.9min
Epoch  370: Loss=49.5485, LR=1.6e-02, kappa=1.0 (fixed), |gamma|=0.2184, time=14.4min
Epoch  380: Loss=49.4004, LR=1.4e-02, kappa=1.0 (fixed), |gamma|=0.2173, time=14.8min
Epoch  390: Loss=49.2697, LR=1.2e-02, kappa=1.0 (fixed), |gamma|=0.2165, time=15.2min
Epoch  400: Loss=49.1552, LR=1.0e-02, kappa=1.0 (fixed), |gamma|=0.2157, time=15.6min
Epoch  410: Loss=49.0557, LR=8.5e-03, kappa=1.0 (fixed), |gamma|=0.2152, time=16.0min
Epoch  420: Loss=48.9698, LR=7.0e-03, kappa=1.0 (fixed), |gamma|=0.2147, time=16.4min
Epoch  430: Loss=48.8963, LR=5.6e-03, kappa=1.0 (fixed), |gamma|=0.2143, time=16.8min
Epoch  440: Loss=48.8338, LR=4.4e-03, kappa=1.0 (fixed), |gamma|=0.2141, time=17.2min
Epoch  450: Loss=48.7808, LR=3.3e-03, kappa=1.0 (fixed), |gamma|=0.2139, time=17.6min
Epoch  460: Loss=48.7360, LR=2.5e-03, kappa=1.0 (fixed), |gamma|=0.2137, time=18.0min
Epoch  470: Loss=48.6979, LR=1.8e-03, kappa=1.0 (fixed), |gamma|=0.2137, time=18.4min
Epoch  480: Loss=48.6649, LR=1.4e-03, kappa=1.0 (fixed), |gamma|=0.2136, time=18.8min
Epoch  490: Loss=48.6355, LR=1.1e-03, kappa=1.0 (fixed), |gamma|=0.2136, time=19.2min

Training complete: 500 epochs in 19.6 min
Final loss: 48.6107

Final params: kappa=1.0 (fixed), mean|gamma|=0.2136
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2_nokappa/enrollment_model_REPARAM_NOKAPPA_W0.0001_batch_140000_150000.pt
Batch 15 done in 19.7 min

============================================================
BATCH 16/40: 150000-160000
============================================================

============================================================
REPARAM v2 NO KAPPA: samples 150000 to 160000
Epochs: 500, LR: 0.1, grad_clip: 5.0
kappa = 1.0 (FIXED, not learned)
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 15 diseases
Cluster 1: 18 diseases
Cluster 2: 82 diseases
Cluster 3: 29 diseases
Cluster 4: 9 diseases
Cluster 5: 7 diseases
Cluster 6: 24 diseases
Cluster 7: 11 diseases
Cluster 8: 12 diseases
Cluster 9: 8 diseases
Cluster 10: 31 diseases
Cluster 11: 5 diseases
Cluster 12: 15 diseases
Cluster 13: 21 diseases
Cluster 14: 8 diseases
Cluster 15: 5 diseases
Cluster 16: 10 diseases
Cluster 17: 15 diseases
Cluster 18: 17 diseases
Cluster 19: 6 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping (NO KAPPA)...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=62.6422, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.1008, time=0.0min
Epoch   10: Loss=218.7133, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0678, time=0.5min
Epoch   20: Loss=94.0586, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0679, time=0.9min
Epoch   30: Loss=63.6855, LR=9.9e-02, kappa=1.0 (fixed), |gamma|=0.0744, time=1.3min
Epoch   40: Loss=62.8909, LR=9.8e-02, kappa=1.0 (fixed), |gamma|=0.0788, time=1.7min
Epoch   50: Loss=60.6686, LR=9.7e-02, kappa=1.0 (fixed), |gamma|=0.0852, time=2.1min
Epoch   60: Loss=58.9802, LR=9.6e-02, kappa=1.0 (fixed), |gamma|=0.0919, time=2.5min
Epoch   70: Loss=58.0070, LR=9.5e-02, kappa=1.0 (fixed), |gamma|=0.0985, time=3.0min
Epoch   80: Loss=57.3866, LR=9.4e-02, kappa=1.0 (fixed), |gamma|=0.1058, time=3.4min
Epoch   90: Loss=56.9205, LR=9.2e-02, kappa=1.0 (fixed), |gamma|=0.1136, time=3.8min
Epoch  100: Loss=56.5112, LR=9.0e-02, kappa=1.0 (fixed), |gamma|=0.1208, time=4.2min
Epoch  110: Loss=56.1228, LR=8.8e-02, kappa=1.0 (fixed), |gamma|=0.1277, time=4.6min
Epoch  120: Loss=55.7508, LR=8.6e-02, kappa=1.0 (fixed), |gamma|=0.1351, time=5.0min
Epoch  130: Loss=55.3935, LR=8.4e-02, kappa=1.0 (fixed), |gamma|=0.1432, time=5.4min
Epoch  140: Loss=55.0479, LR=8.2e-02, kappa=1.0 (fixed), |gamma|=0.1515, time=5.8min
Epoch  150: Loss=54.7072, LR=7.9e-02, kappa=1.0 (fixed), |gamma|=0.1607, time=6.3min
Epoch  160: Loss=54.3737, LR=7.7e-02, kappa=1.0 (fixed), |gamma|=0.1708, time=6.7min
Epoch  170: Loss=54.0476, LR=7.4e-02, kappa=1.0 (fixed), |gamma|=0.1815, time=7.1min
Epoch  180: Loss=53.7206, LR=7.1e-02, kappa=1.0 (fixed), |gamma|=0.1926, time=7.5min
Epoch  190: Loss=53.3993, LR=6.8e-02, kappa=1.0 (fixed), |gamma|=0.2042, time=7.9min
Epoch  200: Loss=53.0809, LR=6.6e-02, kappa=1.0 (fixed), |gamma|=0.2156, time=8.3min
Epoch  210: Loss=52.7662, LR=6.3e-02, kappa=1.0 (fixed), |gamma|=0.2262, time=8.7min
Epoch  220: Loss=52.4508, LR=5.9e-02, kappa=1.0 (fixed), |gamma|=0.2361, time=9.2min
Epoch  230: Loss=52.1404, LR=5.6e-02, kappa=1.0 (fixed), |gamma|=0.2452, time=9.6min
Epoch  240: Loss=51.8350, LR=5.3e-02, kappa=1.0 (fixed), |gamma|=0.2535, time=10.1min
Epoch  250: Loss=51.5379, LR=5.0e-02, kappa=1.0 (fixed), |gamma|=0.2608, time=10.5min
Epoch  260: Loss=51.2425, LR=4.7e-02, kappa=1.0 (fixed), |gamma|=0.2661, time=10.9min
Epoch  270: Loss=50.9565, LR=4.4e-02, kappa=1.0 (fixed), |gamma|=0.2693, time=11.3min
Epoch  280: Loss=50.6784, LR=4.1e-02, kappa=1.0 (fixed), |gamma|=0.2709, time=11.7min
Epoch  290: Loss=50.4099, LR=3.8e-02, kappa=1.0 (fixed), |gamma|=0.2710, time=12.1min
Epoch  300: Loss=50.1528, LR=3.5e-02, kappa=1.0 (fixed), |gamma|=0.2703, time=12.5min
Epoch  310: Loss=49.9083, LR=3.2e-02, kappa=1.0 (fixed), |gamma|=0.2688, time=12.9min
Epoch  320: Loss=49.6781, LR=2.9e-02, kappa=1.0 (fixed), |gamma|=0.2671, time=13.4min
Epoch  330: Loss=49.4631, LR=2.6e-02, kappa=1.0 (fixed), |gamma|=0.2653, time=13.8min
Epoch  340: Loss=49.2640, LR=2.4e-02, kappa=1.0 (fixed), |gamma|=0.2635, time=14.2min
Epoch  350: Loss=49.0811, LR=2.1e-02, kappa=1.0 (fixed), |gamma|=0.2616, time=14.6min
Epoch  360: Loss=48.9146, LR=1.9e-02, kappa=1.0 (fixed), |gamma|=0.2599, time=15.0min
Epoch  370: Loss=48.7643, LR=1.6e-02, kappa=1.0 (fixed), |gamma|=0.2582, time=15.4min
Epoch  380: Loss=48.6297, LR=1.4e-02, kappa=1.0 (fixed), |gamma|=0.2568, time=15.9min
Epoch  390: Loss=48.5102, LR=1.2e-02, kappa=1.0 (fixed), |gamma|=0.2555, time=16.4min
Epoch  400: Loss=48.4051, LR=1.0e-02, kappa=1.0 (fixed), |gamma|=0.2543, time=16.8min
Epoch  410: Loss=48.3134, LR=8.5e-03, kappa=1.0 (fixed), |gamma|=0.2533, time=17.2min
Epoch  420: Loss=48.2341, LR=7.0e-03, kappa=1.0 (fixed), |gamma|=0.2525, time=17.7min
Epoch  430: Loss=48.1662, LR=5.6e-03, kappa=1.0 (fixed), |gamma|=0.2519, time=18.1min
Epoch  440: Loss=48.1083, LR=4.4e-03, kappa=1.0 (fixed), |gamma|=0.2514, time=18.6min
Epoch  450: Loss=48.0594, LR=3.3e-03, kappa=1.0 (fixed), |gamma|=0.2510, time=19.1min
Epoch  460: Loss=48.0181, LR=2.5e-03, kappa=1.0 (fixed), |gamma|=0.2507, time=19.5min
Epoch  470: Loss=47.9830, LR=1.8e-03, kappa=1.0 (fixed), |gamma|=0.2505, time=20.0min
Epoch  480: Loss=47.9528, LR=1.4e-03, kappa=1.0 (fixed), |gamma|=0.2504, time=20.4min
Epoch  490: Loss=47.9259, LR=1.1e-03, kappa=1.0 (fixed), |gamma|=0.2503, time=20.8min

Training complete: 500 epochs in 21.2 min
Final loss: 47.9033

Final params: kappa=1.0 (fixed), mean|gamma|=0.2502
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2_nokappa/enrollment_model_REPARAM_NOKAPPA_W0.0001_batch_150000_160000.pt
Batch 16 done in 21.4 min

============================================================
BATCH 17/40: 160000-170000
============================================================

============================================================
REPARAM v2 NO KAPPA: samples 160000 to 170000
Epochs: 500, LR: 0.1, grad_clip: 5.0
kappa = 1.0 (FIXED, not learned)
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 16 diseases
Cluster 1: 13 diseases
Cluster 2: 6 diseases
Cluster 3: 17 diseases
Cluster 4: 9 diseases
Cluster 5: 24 diseases
Cluster 6: 11 diseases
Cluster 7: 84 diseases
Cluster 8: 6 diseases
Cluster 9: 12 diseases
Cluster 10: 7 diseases
Cluster 11: 14 diseases
Cluster 12: 11 diseases
Cluster 13: 10 diseases
Cluster 14: 39 diseases
Cluster 15: 30 diseases
Cluster 16: 8 diseases
Cluster 17: 4 diseases
Cluster 18: 19 diseases
Cluster 19: 8 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping (NO KAPPA)...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=63.1468, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.1012, time=0.0min
Epoch   10: Loss=219.1618, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0703, time=0.4min
Epoch   20: Loss=94.5349, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0692, time=0.8min
Epoch   30: Loss=64.1397, LR=9.9e-02, kappa=1.0 (fixed), |gamma|=0.0712, time=1.2min
Epoch   40: Loss=63.3282, LR=9.8e-02, kappa=1.0 (fixed), |gamma|=0.0777, time=1.6min
Epoch   50: Loss=61.0873, LR=9.7e-02, kappa=1.0 (fixed), |gamma|=0.0863, time=2.0min
Epoch   60: Loss=59.3832, LR=9.6e-02, kappa=1.0 (fixed), |gamma|=0.0941, time=8.3min
Epoch   70: Loss=58.3965, LR=9.5e-02, kappa=1.0 (fixed), |gamma|=0.1005, time=31.9min
Epoch   80: Loss=57.7663, LR=9.4e-02, kappa=1.0 (fixed), |gamma|=0.1065, time=35.0min
Epoch   90: Loss=57.2928, LR=9.2e-02, kappa=1.0 (fixed), |gamma|=0.1124, time=37.4min
Epoch  100: Loss=56.8792, LR=9.0e-02, kappa=1.0 (fixed), |gamma|=0.1192, time=39.7min
Epoch  110: Loss=56.4889, LR=8.8e-02, kappa=1.0 (fixed), |gamma|=0.1259, time=40.0min
Epoch  120: Loss=56.1174, LR=8.6e-02, kappa=1.0 (fixed), |gamma|=0.1325, time=40.3min
Epoch  130: Loss=55.7630, LR=8.4e-02, kappa=1.0 (fixed), |gamma|=0.1396, time=40.5min
Epoch  140: Loss=55.4192, LR=8.2e-02, kappa=1.0 (fixed), |gamma|=0.1471, time=40.8min
Epoch  150: Loss=55.0843, LR=7.9e-02, kappa=1.0 (fixed), |gamma|=0.1553, time=41.0min
Epoch  160: Loss=54.7561, LR=7.7e-02, kappa=1.0 (fixed), |gamma|=0.1635, time=41.3min
Epoch  170: Loss=54.4348, LR=7.4e-02, kappa=1.0 (fixed), |gamma|=0.1723, time=41.5min
Epoch  180: Loss=54.1137, LR=7.1e-02, kappa=1.0 (fixed), |gamma|=0.1809, time=41.8min
Epoch  190: Loss=53.7950, LR=6.8e-02, kappa=1.0 (fixed), |gamma|=0.1882, time=42.1min
Epoch  200: Loss=53.4716, LR=6.6e-02, kappa=1.0 (fixed), |gamma|=0.1942, time=42.4min
Epoch  210: Loss=53.1455, LR=6.3e-02, kappa=1.0 (fixed), |gamma|=0.1984, time=42.7min
Epoch  220: Loss=52.8103, LR=5.9e-02, kappa=1.0 (fixed), |gamma|=0.2006, time=43.0min
Epoch  230: Loss=52.4679, LR=5.6e-02, kappa=1.0 (fixed), |gamma|=0.2017, time=43.3min
Epoch  240: Loss=52.1130, LR=5.3e-02, kappa=1.0 (fixed), |gamma|=0.2011, time=43.6min
Epoch  250: Loss=51.7549, LR=5.0e-02, kappa=1.0 (fixed), |gamma|=0.1996, time=43.9min
Epoch  260: Loss=51.3962, LR=4.7e-02, kappa=1.0 (fixed), |gamma|=0.1973, time=44.2min
Epoch  270: Loss=51.0420, LR=4.4e-02, kappa=1.0 (fixed), |gamma|=0.1945, time=44.5min
Epoch  280: Loss=50.6936, LR=4.1e-02, kappa=1.0 (fixed), |gamma|=0.1913, time=44.9min
Epoch  290: Loss=50.3582, LR=3.8e-02, kappa=1.0 (fixed), |gamma|=0.1882, time=45.2min
Epoch  300: Loss=50.0398, LR=3.5e-02, kappa=1.0 (fixed), |gamma|=0.1853, time=45.5min
Epoch  310: Loss=49.7412, LR=3.2e-02, kappa=1.0 (fixed), |gamma|=0.1825, time=45.9min
Epoch  320: Loss=49.4642, LR=2.9e-02, kappa=1.0 (fixed), |gamma|=0.1799, time=46.2min
Epoch  330: Loss=49.2098, LR=2.6e-02, kappa=1.0 (fixed), |gamma|=0.1776, time=46.6min
Epoch  340: Loss=48.9782, LR=2.4e-02, kappa=1.0 (fixed), |gamma|=0.1756, time=46.9min
Epoch  350: Loss=48.7689, LR=2.1e-02, kappa=1.0 (fixed), |gamma|=0.1739, time=47.3min
Epoch  360: Loss=48.5813, LR=1.9e-02, kappa=1.0 (fixed), |gamma|=0.1723, time=47.7min
Epoch  370: Loss=48.4141, LR=1.6e-02, kappa=1.0 (fixed), |gamma|=0.1710, time=48.1min
Epoch  380: Loss=48.2663, LR=1.4e-02, kappa=1.0 (fixed), |gamma|=0.1699, time=48.4min
Epoch  390: Loss=48.1364, LR=1.2e-02, kappa=1.0 (fixed), |gamma|=0.1689, time=48.8min
Epoch  400: Loss=48.0230, LR=1.0e-02, kappa=1.0 (fixed), |gamma|=0.1682, time=49.2min
Epoch  410: Loss=47.9248, LR=8.5e-03, kappa=1.0 (fixed), |gamma|=0.1675, time=49.5min
Epoch  420: Loss=47.8403, LR=7.0e-03, kappa=1.0 (fixed), |gamma|=0.1670, time=49.9min
Epoch  430: Loss=47.7681, LR=5.6e-03, kappa=1.0 (fixed), |gamma|=0.1665, time=50.2min
Epoch  440: Loss=47.7068, LR=4.4e-03, kappa=1.0 (fixed), |gamma|=0.1662, time=50.6min
Epoch  450: Loss=47.6549, LR=3.3e-03, kappa=1.0 (fixed), |gamma|=0.1659, time=50.9min
Epoch  460: Loss=47.6110, LR=2.5e-03, kappa=1.0 (fixed), |gamma|=0.1657, time=51.3min
Epoch  470: Loss=47.5736, LR=1.8e-03, kappa=1.0 (fixed), |gamma|=0.1656, time=51.6min
Epoch  480: Loss=47.5412, LR=1.4e-03, kappa=1.0 (fixed), |gamma|=0.1654, time=51.9min
Epoch  490: Loss=47.5124, LR=1.1e-03, kappa=1.0 (fixed), |gamma|=0.1654, time=52.3min

Training complete: 500 epochs in 52.6 min
Final loss: 47.4880

Final params: kappa=1.0 (fixed), mean|gamma|=0.1653
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2_nokappa/enrollment_model_REPARAM_NOKAPPA_W0.0001_batch_160000_170000.pt
Batch 17 done in 52.7 min

============================================================
BATCH 18/40: 170000-180000
============================================================

============================================================
REPARAM v2 NO KAPPA: samples 170000 to 180000
Epochs: 500, LR: 0.1, grad_clip: 5.0
kappa = 1.0 (FIXED, not learned)
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 11 diseases
Cluster 1: 27 diseases
Cluster 2: 18 diseases
Cluster 3: 9 diseases
Cluster 4: 29 diseases
Cluster 5: 6 diseases
Cluster 6: 31 diseases
Cluster 7: 5 diseases
Cluster 8: 30 diseases
Cluster 9: 8 diseases
Cluster 10: 12 diseases
Cluster 11: 15 diseases
Cluster 12: 5 diseases
Cluster 13: 11 diseases
Cluster 14: 9 diseases
Cluster 15: 8 diseases
Cluster 16: 6 diseases
Cluster 17: 15 diseases
Cluster 18: 11 diseases
Cluster 19: 82 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping (NO KAPPA)...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=63.4167, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.1005, time=0.0min
Epoch   10: Loss=219.4636, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0723, time=0.3min
Epoch   20: Loss=94.8042, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0704, time=0.6min
Epoch   30: Loss=64.4191, LR=9.9e-02, kappa=1.0 (fixed), |gamma|=0.0688, time=1.0min
Epoch   40: Loss=63.6135, LR=9.8e-02, kappa=1.0 (fixed), |gamma|=0.0706, time=1.3min
Epoch   50: Loss=61.3770, LR=9.7e-02, kappa=1.0 (fixed), |gamma|=0.0761, time=1.6min
Epoch   60: Loss=59.6729, LR=9.6e-02, kappa=1.0 (fixed), |gamma|=0.0828, time=1.9min
Epoch   70: Loss=58.6852, LR=9.5e-02, kappa=1.0 (fixed), |gamma|=0.0888, time=2.3min
Epoch   80: Loss=58.0512, LR=9.4e-02, kappa=1.0 (fixed), |gamma|=0.0946, time=2.6min
Epoch   90: Loss=57.5755, LR=9.2e-02, kappa=1.0 (fixed), |gamma|=0.1014, time=2.9min
Epoch  100: Loss=57.1592, LR=9.0e-02, kappa=1.0 (fixed), |gamma|=0.1085, time=3.2min
Epoch  110: Loss=56.7654, LR=8.8e-02, kappa=1.0 (fixed), |gamma|=0.1164, time=3.6min
Epoch  120: Loss=56.3893, LR=8.6e-02, kappa=1.0 (fixed), |gamma|=0.1236, time=3.9min
Epoch  130: Loss=56.0309, LR=8.4e-02, kappa=1.0 (fixed), |gamma|=0.1320, time=4.3min
Epoch  140: Loss=55.6813, LR=8.2e-02, kappa=1.0 (fixed), |gamma|=0.1389, time=4.6min
Epoch  150: Loss=55.3370, LR=7.9e-02, kappa=1.0 (fixed), |gamma|=0.1463, time=5.0min
Epoch  160: Loss=54.9997, LR=7.7e-02, kappa=1.0 (fixed), |gamma|=0.1541, time=5.4min
Epoch  170: Loss=54.6643, LR=7.4e-02, kappa=1.0 (fixed), |gamma|=0.1620, time=5.8min
Epoch  180: Loss=54.3287, LR=7.1e-02, kappa=1.0 (fixed), |gamma|=0.1696, time=6.2min
Epoch  190: Loss=53.9913, LR=6.8e-02, kappa=1.0 (fixed), |gamma|=0.1764, time=6.6min
Epoch  200: Loss=53.6489, LR=6.6e-02, kappa=1.0 (fixed), |gamma|=0.1819, time=7.0min
Epoch  210: Loss=53.3017, LR=6.3e-02, kappa=1.0 (fixed), |gamma|=0.1863, time=7.4min
Epoch  220: Loss=52.9483, LR=5.9e-02, kappa=1.0 (fixed), |gamma|=0.1895, time=7.8min
Epoch  230: Loss=52.5894, LR=5.6e-02, kappa=1.0 (fixed), |gamma|=0.1917, time=8.2min
Epoch  240: Loss=52.2273, LR=5.3e-02, kappa=1.0 (fixed), |gamma|=0.1919, time=8.6min
Epoch  250: Loss=51.8656, LR=5.0e-02, kappa=1.0 (fixed), |gamma|=0.1902, time=8.9min
Epoch  260: Loss=51.5069, LR=4.7e-02, kappa=1.0 (fixed), |gamma|=0.1884, time=9.3min
Epoch  270: Loss=51.1568, LR=4.4e-02, kappa=1.0 (fixed), |gamma|=0.1864, time=9.7min
Epoch  280: Loss=50.8192, LR=4.1e-02, kappa=1.0 (fixed), |gamma|=0.1844, time=10.1min
Epoch  290: Loss=50.4972, LR=3.8e-02, kappa=1.0 (fixed), |gamma|=0.1824, time=10.5min
Epoch  300: Loss=50.1938, LR=3.5e-02, kappa=1.0 (fixed), |gamma|=0.1806, time=10.8min
Epoch  310: Loss=49.9105, LR=3.2e-02, kappa=1.0 (fixed), |gamma|=0.1789, time=11.2min
Epoch  320: Loss=49.6483, LR=2.9e-02, kappa=1.0 (fixed), |gamma|=0.1776, time=11.6min
Epoch  330: Loss=49.4076, LR=2.6e-02, kappa=1.0 (fixed), |gamma|=0.1764, time=11.9min
Epoch  340: Loss=49.1881, LR=2.4e-02, kappa=1.0 (fixed), |gamma|=0.1754, time=12.3min
Epoch  350: Loss=48.9895, LR=2.1e-02, kappa=1.0 (fixed), |gamma|=0.1745, time=12.6min
Epoch  360: Loss=48.8108, LR=1.9e-02, kappa=1.0 (fixed), |gamma|=0.1738, time=13.0min
Epoch  370: Loss=48.6512, LR=1.6e-02, kappa=1.0 (fixed), |gamma|=0.1732, time=13.4min
Epoch  380: Loss=48.5096, LR=1.4e-02, kappa=1.0 (fixed), |gamma|=0.1728, time=13.7min
Epoch  390: Loss=48.3848, LR=1.2e-02, kappa=1.0 (fixed), |gamma|=0.1724, time=14.1min
Epoch  400: Loss=48.2755, LR=1.0e-02, kappa=1.0 (fixed), |gamma|=0.1720, time=14.5min
Epoch  410: Loss=48.1807, LR=8.5e-03, kappa=1.0 (fixed), |gamma|=0.1718, time=14.8min
Epoch  420: Loss=48.0990, LR=7.0e-03, kappa=1.0 (fixed), |gamma|=0.1716, time=15.2min
Epoch  430: Loss=48.0290, LR=5.6e-03, kappa=1.0 (fixed), |gamma|=0.1714, time=15.5min
Epoch  440: Loss=47.9696, LR=4.4e-03, kappa=1.0 (fixed), |gamma|=0.1713, time=15.9min
Epoch  450: Loss=47.9194, LR=3.3e-03, kappa=1.0 (fixed), |gamma|=0.1711, time=16.2min
Epoch  460: Loss=47.8769, LR=2.5e-03, kappa=1.0 (fixed), |gamma|=0.1711, time=16.6min
Epoch  470: Loss=47.8408, LR=1.8e-03, kappa=1.0 (fixed), |gamma|=0.1710, time=17.0min
Epoch  480: Loss=47.8096, LR=1.4e-03, kappa=1.0 (fixed), |gamma|=0.1710, time=17.3min
Epoch  490: Loss=47.7819, LR=1.1e-03, kappa=1.0 (fixed), |gamma|=0.1709, time=17.7min

Training complete: 500 epochs in 18.0 min
Final loss: 47.7585

Final params: kappa=1.0 (fixed), mean|gamma|=0.1709
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2_nokappa/enrollment_model_REPARAM_NOKAPPA_W0.0001_batch_170000_180000.pt
Batch 18 done in 18.1 min

============================================================
BATCH 19/40: 180000-190000
============================================================

============================================================
REPARAM v2 NO KAPPA: samples 180000 to 190000
Epochs: 500, LR: 0.1, grad_clip: 5.0
kappa = 1.0 (FIXED, not learned)
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 89 diseases
Cluster 1: 6 diseases
Cluster 2: 10 diseases
Cluster 3: 15 diseases
Cluster 4: 8 diseases
Cluster 5: 28 diseases
Cluster 6: 8 diseases
Cluster 7: 10 diseases
Cluster 8: 21 diseases
Cluster 9: 5 diseases
Cluster 10: 37 diseases
Cluster 11: 9 diseases
Cluster 12: 13 diseases
Cluster 13: 15 diseases
Cluster 14: 13 diseases
Cluster 15: 17 diseases
Cluster 16: 8 diseases
Cluster 17: 13 diseases
Cluster 18: 10 diseases
Cluster 19: 13 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping (NO KAPPA)...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=63.8359, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.1005, time=0.0min
Epoch   10: Loss=219.8318, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0674, time=0.3min
Epoch   20: Loss=95.1945, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0639, time=0.7min
Epoch   30: Loss=64.8054, LR=9.9e-02, kappa=1.0 (fixed), |gamma|=0.0648, time=1.0min
Epoch   40: Loss=63.9846, LR=9.8e-02, kappa=1.0 (fixed), |gamma|=0.0680, time=1.3min
Epoch   50: Loss=61.7343, LR=9.7e-02, kappa=1.0 (fixed), |gamma|=0.0760, time=1.7min
Epoch   60: Loss=60.0208, LR=9.6e-02, kappa=1.0 (fixed), |gamma|=0.0827, time=2.0min
Epoch   70: Loss=59.0261, LR=9.5e-02, kappa=1.0 (fixed), |gamma|=0.0901, time=2.4min
Epoch   80: Loss=58.3886, LR=9.4e-02, kappa=1.0 (fixed), |gamma|=0.0976, time=2.7min
Epoch   90: Loss=57.9091, LR=9.2e-02, kappa=1.0 (fixed), |gamma|=0.1047, time=3.0min
Epoch  100: Loss=57.4891, LR=9.0e-02, kappa=1.0 (fixed), |gamma|=0.1119, time=3.4min
Epoch  110: Loss=57.0909, LR=8.8e-02, kappa=1.0 (fixed), |gamma|=0.1191, time=3.7min
Epoch  120: Loss=56.7094, LR=8.6e-02, kappa=1.0 (fixed), |gamma|=0.1262, time=4.1min
Epoch  130: Loss=56.3435, LR=8.4e-02, kappa=1.0 (fixed), |gamma|=0.1341, time=4.4min
Epoch  140: Loss=55.9900, LR=8.2e-02, kappa=1.0 (fixed), |gamma|=0.1417, time=4.8min
Epoch  150: Loss=55.6452, LR=7.9e-02, kappa=1.0 (fixed), |gamma|=0.1490, time=5.1min
Epoch  160: Loss=55.3065, LR=7.7e-02, kappa=1.0 (fixed), |gamma|=0.1556, time=5.4min
Epoch  170: Loss=54.9729, LR=7.4e-02, kappa=1.0 (fixed), |gamma|=0.1627, time=5.8min
Epoch  180: Loss=54.6509, LR=7.1e-02, kappa=1.0 (fixed), |gamma|=0.1691, time=6.1min
Epoch  190: Loss=54.3171, LR=6.8e-02, kappa=1.0 (fixed), |gamma|=0.1745, time=6.5min
Epoch  200: Loss=53.9901, LR=6.6e-02, kappa=1.0 (fixed), |gamma|=0.1788, time=6.8min
Epoch  210: Loss=53.6636, LR=6.3e-02, kappa=1.0 (fixed), |gamma|=0.1820, time=7.2min
Epoch  220: Loss=53.3358, LR=5.9e-02, kappa=1.0 (fixed), |gamma|=0.1834, time=7.6min
Epoch  230: Loss=53.0022, LR=5.6e-02, kappa=1.0 (fixed), |gamma|=0.1836, time=7.9min
Epoch  240: Loss=52.6689, LR=5.3e-02, kappa=1.0 (fixed), |gamma|=0.1829, time=8.3min
Epoch  250: Loss=52.3330, LR=5.0e-02, kappa=1.0 (fixed), |gamma|=0.1811, time=8.6min
Epoch  260: Loss=52.0019, LR=4.7e-02, kappa=1.0 (fixed), |gamma|=0.1788, time=9.0min
Epoch  270: Loss=51.6779, LR=4.4e-02, kappa=1.0 (fixed), |gamma|=0.1765, time=9.3min
Epoch  280: Loss=51.3629, LR=4.1e-02, kappa=1.0 (fixed), |gamma|=0.1739, time=9.7min
Epoch  290: Loss=51.0618, LR=3.8e-02, kappa=1.0 (fixed), |gamma|=0.1715, time=10.0min
Epoch  300: Loss=50.7749, LR=3.5e-02, kappa=1.0 (fixed), |gamma|=0.1694, time=10.3min
Epoch  310: Loss=50.5058, LR=3.2e-02, kappa=1.0 (fixed), |gamma|=0.1674, time=10.7min
Epoch  320: Loss=50.2553, LR=2.9e-02, kappa=1.0 (fixed), |gamma|=0.1658, time=11.1min
Epoch  330: Loss=50.0239, LR=2.6e-02, kappa=1.0 (fixed), |gamma|=0.1642, time=11.4min
Epoch  340: Loss=49.8116, LR=2.4e-02, kappa=1.0 (fixed), |gamma|=0.1627, time=11.8min
Epoch  350: Loss=49.6186, LR=2.1e-02, kappa=1.0 (fixed), |gamma|=0.1614, time=12.1min
Epoch  360: Loss=49.4441, LR=1.9e-02, kappa=1.0 (fixed), |gamma|=0.1603, time=12.5min
Epoch  370: Loss=49.2875, LR=1.6e-02, kappa=1.0 (fixed), |gamma|=0.1594, time=12.8min
Epoch  380: Loss=49.1482, LR=1.4e-02, kappa=1.0 (fixed), |gamma|=0.1586, time=13.2min
Epoch  390: Loss=49.0251, LR=1.2e-02, kappa=1.0 (fixed), |gamma|=0.1579, time=13.5min
Epoch  400: Loss=48.9172, LR=1.0e-02, kappa=1.0 (fixed), |gamma|=0.1573, time=13.9min
Epoch  410: Loss=48.8234, LR=8.5e-03, kappa=1.0 (fixed), |gamma|=0.1569, time=14.2min
Epoch  420: Loss=48.7424, LR=7.0e-03, kappa=1.0 (fixed), |gamma|=0.1565, time=14.6min
Epoch  430: Loss=48.6731, LR=5.6e-03, kappa=1.0 (fixed), |gamma|=0.1562, time=14.9min
Epoch  440: Loss=48.6140, LR=4.4e-03, kappa=1.0 (fixed), |gamma|=0.1559, time=15.3min
Epoch  450: Loss=48.5640, LR=3.3e-03, kappa=1.0 (fixed), |gamma|=0.1557, time=15.6min
Epoch  460: Loss=48.5217, LR=2.5e-03, kappa=1.0 (fixed), |gamma|=0.1555, time=16.0min
Epoch  470: Loss=48.4857, LR=1.8e-03, kappa=1.0 (fixed), |gamma|=0.1554, time=16.3min
Epoch  480: Loss=48.4546, LR=1.4e-03, kappa=1.0 (fixed), |gamma|=0.1553, time=16.7min
Epoch  490: Loss=48.4268, LR=1.1e-03, kappa=1.0 (fixed), |gamma|=0.1553, time=17.1min

Training complete: 500 epochs in 17.4 min
Final loss: 48.4033

Final params: kappa=1.0 (fixed), mean|gamma|=0.1552
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2_nokappa/enrollment_model_REPARAM_NOKAPPA_W0.0001_batch_180000_190000.pt
Batch 19 done in 17.6 min

============================================================
BATCH 20/40: 190000-200000
============================================================

============================================================
REPARAM v2 NO KAPPA: samples 190000 to 200000
Epochs: 500, LR: 0.1, grad_clip: 5.0
kappa = 1.0 (FIXED, not learned)
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 10 diseases
Cluster 1: 9 diseases
Cluster 2: 26 diseases
Cluster 3: 19 diseases
Cluster 4: 27 diseases
Cluster 5: 96 diseases
Cluster 6: 14 diseases
Cluster 7: 5 diseases
Cluster 8: 15 diseases
Cluster 9: 9 diseases
Cluster 10: 16 diseases
Cluster 11: 13 diseases
Cluster 12: 17 diseases
Cluster 13: 10 diseases
Cluster 14: 7 diseases
Cluster 15: 13 diseases
Cluster 16: 9 diseases
Cluster 17: 19 diseases
Cluster 18: 9 diseases
Cluster 19: 5 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping (NO KAPPA)...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=63.6597, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.1007, time=0.0min
Epoch   10: Loss=219.6986, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0680, time=0.4min
Epoch   20: Loss=95.0348, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0654, time=0.8min
Epoch   30: Loss=64.6361, LR=9.9e-02, kappa=1.0 (fixed), |gamma|=0.0681, time=1.2min
Epoch   40: Loss=63.8329, LR=9.8e-02, kappa=1.0 (fixed), |gamma|=0.0721, time=1.5min
Epoch   50: Loss=61.5924, LR=9.7e-02, kappa=1.0 (fixed), |gamma|=0.0787, time=1.9min
Epoch   60: Loss=59.8850, LR=9.6e-02, kappa=1.0 (fixed), |gamma|=0.0853, time=2.3min
Epoch   70: Loss=58.8943, LR=9.5e-02, kappa=1.0 (fixed), |gamma|=0.0931, time=2.7min
Epoch   80: Loss=58.2593, LR=9.4e-02, kappa=1.0 (fixed), |gamma|=0.1003, time=3.1min
Epoch   90: Loss=57.7817, LR=9.2e-02, kappa=1.0 (fixed), |gamma|=0.1057, time=3.5min
Epoch  100: Loss=57.3636, LR=9.0e-02, kappa=1.0 (fixed), |gamma|=0.1109, time=3.9min
Epoch  110: Loss=56.9667, LR=8.8e-02, kappa=1.0 (fixed), |gamma|=0.1163, time=4.3min
Epoch  120: Loss=56.5876, LR=8.6e-02, kappa=1.0 (fixed), |gamma|=0.1228, time=4.7min
Epoch  130: Loss=56.2266, LR=8.4e-02, kappa=1.0 (fixed), |gamma|=0.1300, time=5.1min
Epoch  140: Loss=55.8764, LR=8.2e-02, kappa=1.0 (fixed), |gamma|=0.1375, time=5.5min
Epoch  150: Loss=55.5355, LR=7.9e-02, kappa=1.0 (fixed), |gamma|=0.1447, time=5.8min
Epoch  160: Loss=55.2002, LR=7.7e-02, kappa=1.0 (fixed), |gamma|=0.1523, time=6.2min
Epoch  170: Loss=54.8686, LR=7.4e-02, kappa=1.0 (fixed), |gamma|=0.1595, time=6.6min
Epoch  180: Loss=54.5402, LR=7.1e-02, kappa=1.0 (fixed), |gamma|=0.1666, time=7.0min
Epoch  190: Loss=54.2139, LR=6.8e-02, kappa=1.0 (fixed), |gamma|=0.1733, time=7.4min
Epoch  200: Loss=53.8894, LR=6.6e-02, kappa=1.0 (fixed), |gamma|=0.1795, time=7.8min
Epoch  210: Loss=53.5658, LR=6.3e-02, kappa=1.0 (fixed), |gamma|=0.1847, time=8.2min
Epoch  220: Loss=53.2442, LR=5.9e-02, kappa=1.0 (fixed), |gamma|=0.1899, time=8.6min
Epoch  230: Loss=52.9254, LR=5.6e-02, kappa=1.0 (fixed), |gamma|=0.1944, time=9.0min
Epoch  240: Loss=52.6104, LR=5.3e-02, kappa=1.0 (fixed), |gamma|=0.1975, time=9.3min
Epoch  250: Loss=52.2978, LR=5.0e-02, kappa=1.0 (fixed), |gamma|=0.1992, time=9.7min
Epoch  260: Loss=51.9929, LR=4.7e-02, kappa=1.0 (fixed), |gamma|=0.2001, time=10.1min
Epoch  270: Loss=51.6896, LR=4.4e-02, kappa=1.0 (fixed), |gamma|=0.2004, time=10.5min
Epoch  280: Loss=51.3966, LR=4.1e-02, kappa=1.0 (fixed), |gamma|=0.2001, time=10.9min
Epoch  290: Loss=51.1149, LR=3.8e-02, kappa=1.0 (fixed), |gamma|=0.1996, time=11.3min
Epoch  300: Loss=50.8453, LR=3.5e-02, kappa=1.0 (fixed), |gamma|=0.1991, time=11.7min
Epoch  310: Loss=50.5873, LR=3.2e-02, kappa=1.0 (fixed), |gamma|=0.1988, time=12.1min
Epoch  320: Loss=50.3477, LR=2.9e-02, kappa=1.0 (fixed), |gamma|=0.1979, time=12.5min
Epoch  330: Loss=50.1214, LR=2.6e-02, kappa=1.0 (fixed), |gamma|=0.1971, time=12.8min
Epoch  340: Loss=49.9143, LR=2.4e-02, kappa=1.0 (fixed), |gamma|=0.1959, time=13.3min
Epoch  350: Loss=49.7249, LR=2.1e-02, kappa=1.0 (fixed), |gamma|=0.1950, time=13.6min
Epoch  360: Loss=49.5531, LR=1.9e-02, kappa=1.0 (fixed), |gamma|=0.1941, time=14.0min
Epoch  370: Loss=49.3985, LR=1.6e-02, kappa=1.0 (fixed), |gamma|=0.1930, time=14.4min
Epoch  380: Loss=49.2603, LR=1.4e-02, kappa=1.0 (fixed), |gamma|=0.1922, time=14.8min
Epoch  390: Loss=49.1378, LR=1.2e-02, kappa=1.0 (fixed), |gamma|=0.1914, time=15.2min
Epoch  400: Loss=49.0302, LR=1.0e-02, kappa=1.0 (fixed), |gamma|=0.1907, time=15.6min
Epoch  410: Loss=48.9364, LR=8.5e-03, kappa=1.0 (fixed), |gamma|=0.1901, time=16.0min
Epoch  420: Loss=48.8553, LR=7.0e-03, kappa=1.0 (fixed), |gamma|=0.1896, time=16.4min
Epoch  430: Loss=48.7858, LR=5.6e-03, kappa=1.0 (fixed), |gamma|=0.1892, time=16.8min
Epoch  440: Loss=48.7265, LR=4.4e-03, kappa=1.0 (fixed), |gamma|=0.1889, time=17.2min
Epoch  450: Loss=48.6763, LR=3.3e-03, kappa=1.0 (fixed), |gamma|=0.1887, time=17.6min
Epoch  460: Loss=48.6338, LR=2.5e-03, kappa=1.0 (fixed), |gamma|=0.1885, time=18.0min
Epoch  470: Loss=48.5976, LR=1.8e-03, kappa=1.0 (fixed), |gamma|=0.1884, time=18.4min
Epoch  480: Loss=48.5662, LR=1.4e-03, kappa=1.0 (fixed), |gamma|=0.1883, time=18.8min
Epoch  490: Loss=48.5383, LR=1.1e-03, kappa=1.0 (fixed), |gamma|=0.1882, time=19.2min

Training complete: 500 epochs in 19.6 min
Final loss: 48.5148

Final params: kappa=1.0 (fixed), mean|gamma|=0.1882
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2_nokappa/enrollment_model_REPARAM_NOKAPPA_W0.0001_batch_190000_200000.pt
Batch 20 done in 19.7 min

============================================================
BATCH 21/40: 200000-210000
============================================================

============================================================
REPARAM v2 NO KAPPA: samples 200000 to 210000
Epochs: 500, LR: 0.1, grad_clip: 5.0
kappa = 1.0 (FIXED, not learned)
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 11 diseases
Cluster 1: 28 diseases
Cluster 2: 9 diseases
Cluster 3: 24 diseases
Cluster 4: 16 diseases
Cluster 5: 9 diseases
Cluster 6: 19 diseases
Cluster 7: 16 diseases
Cluster 8: 78 diseases
Cluster 9: 12 diseases
Cluster 10: 22 diseases
Cluster 11: 18 diseases
Cluster 12: 5 diseases
Cluster 13: 5 diseases
Cluster 14: 13 diseases
Cluster 15: 22 diseases
Cluster 16: 11 diseases
Cluster 17: 8 diseases
Cluster 18: 15 diseases
Cluster 19: 7 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping (NO KAPPA)...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=63.3513, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.1009, time=0.0min
Epoch   10: Loss=219.3244, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0677, time=0.4min
Epoch   20: Loss=94.7299, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0697, time=0.8min
Epoch   30: Loss=64.3614, LR=9.9e-02, kappa=1.0 (fixed), |gamma|=0.0694, time=1.2min
Epoch   40: Loss=63.5455, LR=9.8e-02, kappa=1.0 (fixed), |gamma|=0.0723, time=1.6min
Epoch   50: Loss=61.3036, LR=9.7e-02, kappa=1.0 (fixed), |gamma|=0.0801, time=2.0min
Epoch   60: Loss=59.5972, LR=9.6e-02, kappa=1.0 (fixed), |gamma|=0.0874, time=2.3min
Epoch   70: Loss=58.6068, LR=9.5e-02, kappa=1.0 (fixed), |gamma|=0.0953, time=2.7min
Epoch   80: Loss=57.9700, LR=9.4e-02, kappa=1.0 (fixed), |gamma|=0.1030, time=3.1min
Epoch   90: Loss=57.4884, LR=9.2e-02, kappa=1.0 (fixed), |gamma|=0.1101, time=3.5min
Epoch  100: Loss=57.0668, LR=9.0e-02, kappa=1.0 (fixed), |gamma|=0.1178, time=3.9min
Epoch  110: Loss=56.6683, LR=8.8e-02, kappa=1.0 (fixed), |gamma|=0.1254, time=4.3min
Epoch  120: Loss=56.2887, LR=8.6e-02, kappa=1.0 (fixed), |gamma|=0.1332, time=4.7min
Epoch  130: Loss=55.9274, LR=8.4e-02, kappa=1.0 (fixed), |gamma|=0.1406, time=5.1min
Epoch  140: Loss=55.5764, LR=8.2e-02, kappa=1.0 (fixed), |gamma|=0.1491, time=5.4min
Epoch  150: Loss=55.2347, LR=7.9e-02, kappa=1.0 (fixed), |gamma|=0.1574, time=5.8min
Epoch  160: Loss=54.8995, LR=7.7e-02, kappa=1.0 (fixed), |gamma|=0.1669, time=6.2min
Epoch  170: Loss=54.5710, LR=7.4e-02, kappa=1.0 (fixed), |gamma|=0.1769, time=6.6min
Epoch  180: Loss=54.2445, LR=7.1e-02, kappa=1.0 (fixed), |gamma|=0.1877, time=7.0min
Epoch  190: Loss=53.9212, LR=6.8e-02, kappa=1.0 (fixed), |gamma|=0.1984, time=7.4min
Epoch  200: Loss=53.6008, LR=6.6e-02, kappa=1.0 (fixed), |gamma|=0.2094, time=7.8min
Epoch  210: Loss=53.2818, LR=6.3e-02, kappa=1.0 (fixed), |gamma|=0.2212, time=8.1min
Epoch  220: Loss=52.9669, LR=5.9e-02, kappa=1.0 (fixed), |gamma|=0.2330, time=8.5min
Epoch  230: Loss=52.6557, LR=5.6e-02, kappa=1.0 (fixed), |gamma|=0.2452, time=8.9min
Epoch  240: Loss=52.3491, LR=5.3e-02, kappa=1.0 (fixed), |gamma|=0.2576, time=9.3min
Epoch  250: Loss=52.0478, LR=5.0e-02, kappa=1.0 (fixed), |gamma|=0.2696, time=9.7min
Epoch  260: Loss=51.7531, LR=4.7e-02, kappa=1.0 (fixed), |gamma|=0.2800, time=10.1min
Epoch  270: Loss=51.4666, LR=4.4e-02, kappa=1.0 (fixed), |gamma|=0.2875, time=10.5min
Epoch  280: Loss=51.1877, LR=4.1e-02, kappa=1.0 (fixed), |gamma|=0.2922, time=10.8min
Epoch  290: Loss=50.9190, LR=3.8e-02, kappa=1.0 (fixed), |gamma|=0.2949, time=11.2min
Epoch  300: Loss=50.6607, LR=3.5e-02, kappa=1.0 (fixed), |gamma|=0.2964, time=11.6min
Epoch  310: Loss=50.4136, LR=3.2e-02, kappa=1.0 (fixed), |gamma|=0.2968, time=12.0min
Epoch  320: Loss=50.1786, LR=2.9e-02, kappa=1.0 (fixed), |gamma|=0.2963, time=12.3min
Epoch  330: Loss=49.9570, LR=2.6e-02, kappa=1.0 (fixed), |gamma|=0.2953, time=12.7min
Epoch  340: Loss=49.7501, LR=2.4e-02, kappa=1.0 (fixed), |gamma|=0.2939, time=13.1min
Epoch  350: Loss=49.5590, LR=2.1e-02, kappa=1.0 (fixed), |gamma|=0.2924, time=13.5min
Epoch  360: Loss=49.3844, LR=1.9e-02, kappa=1.0 (fixed), |gamma|=0.2910, time=13.8min
Epoch  370: Loss=49.2263, LR=1.6e-02, kappa=1.0 (fixed), |gamma|=0.2897, time=14.2min
Epoch  380: Loss=49.0844, LR=1.4e-02, kappa=1.0 (fixed), |gamma|=0.2885, time=14.6min
Epoch  390: Loss=48.9583, LR=1.2e-02, kappa=1.0 (fixed), |gamma|=0.2875, time=14.9min
Epoch  400: Loss=48.8473, LR=1.0e-02, kappa=1.0 (fixed), |gamma|=0.2866, time=15.3min
Epoch  410: Loss=48.7504, LR=8.5e-03, kappa=1.0 (fixed), |gamma|=0.2860, time=15.7min
Epoch  420: Loss=48.6666, LR=7.0e-03, kappa=1.0 (fixed), |gamma|=0.2854, time=16.0min
Epoch  430: Loss=48.5946, LR=5.6e-03, kappa=1.0 (fixed), |gamma|=0.2851, time=16.4min
Epoch  440: Loss=48.5333, LR=4.4e-03, kappa=1.0 (fixed), |gamma|=0.2848, time=16.8min
Epoch  450: Loss=48.4814, LR=3.3e-03, kappa=1.0 (fixed), |gamma|=0.2846, time=17.2min
Epoch  460: Loss=48.4375, LR=2.5e-03, kappa=1.0 (fixed), |gamma|=0.2845, time=17.6min
Epoch  470: Loss=48.4001, LR=1.8e-03, kappa=1.0 (fixed), |gamma|=0.2844, time=17.9min
Epoch  480: Loss=48.3678, LR=1.4e-03, kappa=1.0 (fixed), |gamma|=0.2844, time=18.3min
Epoch  490: Loss=48.3390, LR=1.1e-03, kappa=1.0 (fixed), |gamma|=0.2843, time=18.7min

Training complete: 500 epochs in 19.0 min
Final loss: 48.3148

Final params: kappa=1.0 (fixed), mean|gamma|=0.2843
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2_nokappa/enrollment_model_REPARAM_NOKAPPA_W0.0001_batch_200000_210000.pt
Batch 21 done in 19.2 min

============================================================
BATCH 22/40: 210000-220000
============================================================

============================================================
REPARAM v2 NO KAPPA: samples 210000 to 220000
Epochs: 500, LR: 0.1, grad_clip: 5.0
kappa = 1.0 (FIXED, not learned)
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 16 diseases
Cluster 1: 7 diseases
Cluster 2: 14 diseases
Cluster 3: 102 diseases
Cluster 4: 9 diseases
Cluster 5: 18 diseases
Cluster 6: 5 diseases
Cluster 7: 26 diseases
Cluster 8: 9 diseases
Cluster 9: 8 diseases
Cluster 10: 12 diseases
Cluster 11: 14 diseases
Cluster 12: 14 diseases
Cluster 13: 10 diseases
Cluster 14: 5 diseases
Cluster 15: 12 diseases
Cluster 16: 25 diseases
Cluster 17: 5 diseases
Cluster 18: 24 diseases
Cluster 19: 13 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping (NO KAPPA)...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=63.6113, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.1013, time=0.0min
Epoch   10: Loss=219.5961, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0714, time=0.4min
Epoch   20: Loss=94.9660, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0717, time=0.8min
Epoch   30: Loss=64.5848, LR=9.9e-02, kappa=1.0 (fixed), |gamma|=0.0765, time=1.1min
Epoch   40: Loss=63.7659, LR=9.8e-02, kappa=1.0 (fixed), |gamma|=0.0807, time=1.5min
Epoch   50: Loss=61.5190, LR=9.7e-02, kappa=1.0 (fixed), |gamma|=0.0867, time=1.9min
Epoch   60: Loss=59.8105, LR=9.6e-02, kappa=1.0 (fixed), |gamma|=0.0936, time=2.2min
Epoch   70: Loss=58.8202, LR=9.5e-02, kappa=1.0 (fixed), |gamma|=0.1007, time=2.6min
Epoch   80: Loss=58.1861, LR=9.4e-02, kappa=1.0 (fixed), |gamma|=0.1081, time=3.0min
Epoch   90: Loss=57.7098, LR=9.2e-02, kappa=1.0 (fixed), |gamma|=0.1162, time=3.3min
Epoch  100: Loss=57.2940, LR=9.0e-02, kappa=1.0 (fixed), |gamma|=0.1248, time=3.7min
Epoch  110: Loss=56.9017, LR=8.8e-02, kappa=1.0 (fixed), |gamma|=0.1330, time=4.0min
Epoch  120: Loss=56.5272, LR=8.6e-02, kappa=1.0 (fixed), |gamma|=0.1407, time=4.4min
Epoch  130: Loss=56.1718, LR=8.4e-02, kappa=1.0 (fixed), |gamma|=0.1487, time=4.8min
Epoch  140: Loss=55.8277, LR=8.2e-02, kappa=1.0 (fixed), |gamma|=0.1560, time=5.1min
Epoch  150: Loss=55.4925, LR=7.9e-02, kappa=1.0 (fixed), |gamma|=0.1625, time=5.5min
Epoch  160: Loss=55.1642, LR=7.7e-02, kappa=1.0 (fixed), |gamma|=0.1686, time=5.9min
Epoch  170: Loss=54.8344, LR=7.4e-02, kappa=1.0 (fixed), |gamma|=0.1736, time=6.2min
Epoch  180: Loss=54.5026, LR=7.1e-02, kappa=1.0 (fixed), |gamma|=0.1783, time=6.6min
Epoch  190: Loss=54.1655, LR=6.8e-02, kappa=1.0 (fixed), |gamma|=0.1822, time=7.0min
Epoch  200: Loss=53.8205, LR=6.6e-02, kappa=1.0 (fixed), |gamma|=0.1859, time=7.3min
Epoch  210: Loss=53.4730, LR=6.3e-02, kappa=1.0 (fixed), |gamma|=0.1886, time=7.7min
Epoch  220: Loss=53.1100, LR=5.9e-02, kappa=1.0 (fixed), |gamma|=0.1900, time=8.1min
Epoch  230: Loss=52.7428, LR=5.6e-02, kappa=1.0 (fixed), |gamma|=0.1900, time=8.4min
Epoch  240: Loss=52.3679, LR=5.3e-02, kappa=1.0 (fixed), |gamma|=0.1880, time=8.8min
Epoch  250: Loss=51.9899, LR=5.0e-02, kappa=1.0 (fixed), |gamma|=0.1858, time=9.1min
Epoch  260: Loss=51.6173, LR=4.7e-02, kappa=1.0 (fixed), |gamma|=0.1833, time=9.5min
Epoch  270: Loss=51.2540, LR=4.4e-02, kappa=1.0 (fixed), |gamma|=0.1805, time=9.8min
Epoch  280: Loss=50.9047, LR=4.1e-02, kappa=1.0 (fixed), |gamma|=0.1776, time=10.2min
Epoch  290: Loss=50.5712, LR=3.8e-02, kappa=1.0 (fixed), |gamma|=0.1749, time=10.5min
Epoch  300: Loss=50.2569, LR=3.5e-02, kappa=1.0 (fixed), |gamma|=0.1724, time=10.9min
Epoch  310: Loss=49.9633, LR=3.2e-02, kappa=1.0 (fixed), |gamma|=0.1700, time=11.2min
Epoch  320: Loss=49.6912, LR=2.9e-02, kappa=1.0 (fixed), |gamma|=0.1679, time=11.6min
Epoch  330: Loss=49.4411, LR=2.6e-02, kappa=1.0 (fixed), |gamma|=0.1660, time=11.9min
Epoch  340: Loss=49.2128, LR=2.4e-02, kappa=1.0 (fixed), |gamma|=0.1642, time=12.3min
Epoch  350: Loss=49.0059, LR=2.1e-02, kappa=1.0 (fixed), |gamma|=0.1627, time=12.7min
Epoch  360: Loss=48.8197, LR=1.9e-02, kappa=1.0 (fixed), |gamma|=0.1613, time=13.0min
Epoch  370: Loss=48.6532, LR=1.6e-02, kappa=1.0 (fixed), |gamma|=0.1600, time=13.4min
Epoch  380: Loss=48.5055, LR=1.4e-02, kappa=1.0 (fixed), |gamma|=0.1589, time=13.7min
Epoch  390: Loss=48.3752, LR=1.2e-02, kappa=1.0 (fixed), |gamma|=0.1580, time=14.1min
Epoch  400: Loss=48.2613, LR=1.0e-02, kappa=1.0 (fixed), |gamma|=0.1571, time=14.5min
Epoch  410: Loss=48.1624, LR=8.5e-03, kappa=1.0 (fixed), |gamma|=0.1564, time=14.8min
Epoch  420: Loss=48.0773, LR=7.0e-03, kappa=1.0 (fixed), |gamma|=0.1558, time=15.2min
Epoch  430: Loss=48.0044, LR=5.6e-03, kappa=1.0 (fixed), |gamma|=0.1553, time=15.5min
Epoch  440: Loss=47.9425, LR=4.4e-03, kappa=1.0 (fixed), |gamma|=0.1549, time=15.9min
Epoch  450: Loss=47.8902, LR=3.3e-03, kappa=1.0 (fixed), |gamma|=0.1546, time=16.3min
Epoch  460: Loss=47.8460, LR=2.5e-03, kappa=1.0 (fixed), |gamma|=0.1543, time=16.6min
Epoch  470: Loss=47.8084, LR=1.8e-03, kappa=1.0 (fixed), |gamma|=0.1542, time=17.0min
Epoch  480: Loss=47.7759, LR=1.4e-03, kappa=1.0 (fixed), |gamma|=0.1540, time=17.3min
Epoch  490: Loss=47.7470, LR=1.1e-03, kappa=1.0 (fixed), |gamma|=0.1539, time=17.7min

Training complete: 500 epochs in 18.0 min
Final loss: 47.7227

Final params: kappa=1.0 (fixed), mean|gamma|=0.1538
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2_nokappa/enrollment_model_REPARAM_NOKAPPA_W0.0001_batch_210000_220000.pt
Batch 22 done in 18.2 min

============================================================
BATCH 23/40: 220000-230000
============================================================

============================================================
REPARAM v2 NO KAPPA: samples 220000 to 230000
Epochs: 500, LR: 0.1, grad_clip: 5.0
kappa = 1.0 (FIXED, not learned)
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 16 diseases
Cluster 1: 10 diseases
Cluster 2: 16 diseases
Cluster 3: 27 diseases
Cluster 4: 35 diseases
Cluster 5: 9 diseases
Cluster 6: 9 diseases
Cluster 7: 8 diseases
Cluster 8: 5 diseases
Cluster 9: 81 diseases
Cluster 10: 11 diseases
Cluster 11: 12 diseases
Cluster 12: 16 diseases
Cluster 13: 15 diseases
Cluster 14: 8 diseases
Cluster 15: 5 diseases
Cluster 16: 24 diseases
Cluster 17: 10 diseases
Cluster 18: 15 diseases
Cluster 19: 16 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping (NO KAPPA)...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=63.1564, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.1008, time=0.0min
Epoch   10: Loss=219.2051, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0699, time=0.4min
Epoch   20: Loss=94.5266, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0693, time=0.8min
Epoch   30: Loss=64.1462, LR=9.9e-02, kappa=1.0 (fixed), |gamma|=0.0706, time=1.1min
Epoch   40: Loss=63.3415, LR=9.8e-02, kappa=1.0 (fixed), |gamma|=0.0720, time=1.5min
Epoch   50: Loss=61.1045, LR=9.7e-02, kappa=1.0 (fixed), |gamma|=0.0782, time=1.9min
Epoch   60: Loss=59.4034, LR=9.6e-02, kappa=1.0 (fixed), |gamma|=0.0846, time=2.2min
Epoch   70: Loss=58.4200, LR=9.5e-02, kappa=1.0 (fixed), |gamma|=0.0900, time=2.6min
Epoch   80: Loss=57.7920, LR=9.4e-02, kappa=1.0 (fixed), |gamma|=0.0958, time=3.0min
Epoch   90: Loss=57.3203, LR=9.2e-02, kappa=1.0 (fixed), |gamma|=0.1029, time=3.3min
Epoch  100: Loss=56.9075, LR=9.0e-02, kappa=1.0 (fixed), |gamma|=0.1110, time=3.7min
Epoch  110: Loss=56.5169, LR=8.8e-02, kappa=1.0 (fixed), |gamma|=0.1189, time=4.1min
Epoch  120: Loss=56.1440, LR=8.6e-02, kappa=1.0 (fixed), |gamma|=0.1261, time=4.4min
Epoch  130: Loss=55.7875, LR=8.4e-02, kappa=1.0 (fixed), |gamma|=0.1331, time=4.8min
Epoch  140: Loss=55.4410, LR=8.2e-02, kappa=1.0 (fixed), |gamma|=0.1411, time=5.1min
Epoch  150: Loss=55.1024, LR=7.9e-02, kappa=1.0 (fixed), |gamma|=0.1485, time=5.5min
Epoch  160: Loss=54.7702, LR=7.7e-02, kappa=1.0 (fixed), |gamma|=0.1564, time=5.9min
Epoch  170: Loss=54.4414, LR=7.4e-02, kappa=1.0 (fixed), |gamma|=0.1646, time=6.2min
Epoch  180: Loss=54.1172, LR=7.1e-02, kappa=1.0 (fixed), |gamma|=0.1728, time=6.6min
Epoch  190: Loss=53.7955, LR=6.8e-02, kappa=1.0 (fixed), |gamma|=0.1817, time=7.0min
Epoch  200: Loss=53.4716, LR=6.6e-02, kappa=1.0 (fixed), |gamma|=0.1902, time=7.3min
Epoch  210: Loss=53.1511, LR=6.3e-02, kappa=1.0 (fixed), |gamma|=0.1990, time=7.7min
Epoch  220: Loss=52.8327, LR=5.9e-02, kappa=1.0 (fixed), |gamma|=0.2076, time=8.1min
Epoch  230: Loss=52.5157, LR=5.6e-02, kappa=1.0 (fixed), |gamma|=0.2158, time=8.4min
Epoch  240: Loss=52.2020, LR=5.3e-02, kappa=1.0 (fixed), |gamma|=0.2238, time=8.8min
Epoch  250: Loss=51.8911, LR=5.0e-02, kappa=1.0 (fixed), |gamma|=0.2315, time=9.1min
Epoch  260: Loss=51.5858, LR=4.7e-02, kappa=1.0 (fixed), |gamma|=0.2385, time=9.5min
Epoch  270: Loss=51.2883, LR=4.4e-02, kappa=1.0 (fixed), |gamma|=0.2428, time=9.8min
Epoch  280: Loss=51.0009, LR=4.1e-02, kappa=1.0 (fixed), |gamma|=0.2449, time=10.1min
Epoch  290: Loss=50.7257, LR=3.8e-02, kappa=1.0 (fixed), |gamma|=0.2471, time=10.4min
Epoch  300: Loss=50.4643, LR=3.5e-02, kappa=1.0 (fixed), |gamma|=0.2490, time=10.8min
Epoch  310: Loss=50.2181, LR=3.2e-02, kappa=1.0 (fixed), |gamma|=0.2502, time=11.1min
Epoch  320: Loss=49.9879, LR=2.9e-02, kappa=1.0 (fixed), |gamma|=0.2509, time=11.4min
Epoch  330: Loss=49.7744, LR=2.6e-02, kappa=1.0 (fixed), |gamma|=0.2514, time=11.8min
Epoch  340: Loss=49.5777, LR=2.4e-02, kappa=1.0 (fixed), |gamma|=0.2516, time=12.1min
Epoch  350: Loss=49.3978, LR=2.1e-02, kappa=1.0 (fixed), |gamma|=0.2517, time=12.4min
Epoch  360: Loss=49.2345, LR=1.9e-02, kappa=1.0 (fixed), |gamma|=0.2515, time=12.8min
Epoch  370: Loss=49.0874, LR=1.6e-02, kappa=1.0 (fixed), |gamma|=0.2513, time=13.1min
Epoch  380: Loss=48.9558, LR=1.4e-02, kappa=1.0 (fixed), |gamma|=0.2510, time=13.4min
Epoch  390: Loss=48.8390, LR=1.2e-02, kappa=1.0 (fixed), |gamma|=0.2506, time=13.8min
Epoch  400: Loss=48.7361, LR=1.0e-02, kappa=1.0 (fixed), |gamma|=0.2502, time=14.1min
Epoch  410: Loss=48.6464, LR=8.5e-03, kappa=1.0 (fixed), |gamma|=0.2498, time=14.4min
Epoch  420: Loss=48.5686, LR=7.0e-03, kappa=1.0 (fixed), |gamma|=0.2495, time=14.7min
Epoch  430: Loss=48.5018, LR=5.6e-03, kappa=1.0 (fixed), |gamma|=0.2492, time=15.0min
Epoch  440: Loss=48.4449, LR=4.4e-03, kappa=1.0 (fixed), |gamma|=0.2490, time=15.4min
Epoch  450: Loss=48.3965, LR=3.3e-03, kappa=1.0 (fixed), |gamma|=0.2488, time=15.7min
Epoch  460: Loss=48.3556, LR=2.5e-03, kappa=1.0 (fixed), |gamma|=0.2487, time=16.0min
Epoch  470: Loss=48.3207, LR=1.8e-03, kappa=1.0 (fixed), |gamma|=0.2486, time=16.3min
Epoch  480: Loss=48.2905, LR=1.4e-03, kappa=1.0 (fixed), |gamma|=0.2485, time=16.6min
Epoch  490: Loss=48.2636, LR=1.1e-03, kappa=1.0 (fixed), |gamma|=0.2485, time=16.9min

Training complete: 500 epochs in 17.2 min
Final loss: 48.2409

Final params: kappa=1.0 (fixed), mean|gamma|=0.2484
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2_nokappa/enrollment_model_REPARAM_NOKAPPA_W0.0001_batch_220000_230000.pt
Batch 23 done in 17.4 min

============================================================
BATCH 24/40: 230000-240000
============================================================

============================================================
REPARAM v2 NO KAPPA: samples 230000 to 240000
Epochs: 500, LR: 0.1, grad_clip: 5.0
kappa = 1.0 (FIXED, not learned)
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 17 diseases
Cluster 1: 14 diseases
Cluster 2: 5 diseases
Cluster 3: 37 diseases
Cluster 4: 20 diseases
Cluster 5: 10 diseases
Cluster 6: 86 diseases
Cluster 7: 24 diseases
Cluster 8: 6 diseases
Cluster 9: 13 diseases
Cluster 10: 8 diseases
Cluster 11: 17 diseases
Cluster 12: 5 diseases
Cluster 13: 11 diseases
Cluster 14: 8 diseases
Cluster 15: 10 diseases
Cluster 16: 4 diseases
Cluster 17: 8 diseases
Cluster 18: 10 diseases
Cluster 19: 35 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping (NO KAPPA)...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=62.9686, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.1012, time=0.0min
Epoch   10: Loss=219.0101, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0728, time=0.4min
Epoch   20: Loss=94.3507, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0696, time=0.7min
Epoch   30: Loss=63.9768, LR=9.9e-02, kappa=1.0 (fixed), |gamma|=0.0723, time=1.1min
Epoch   40: Loss=63.1736, LR=9.8e-02, kappa=1.0 (fixed), |gamma|=0.0763, time=1.4min
Epoch   50: Loss=60.9413, LR=9.7e-02, kappa=1.0 (fixed), |gamma|=0.0829, time=1.8min
Epoch   60: Loss=59.2434, LR=9.6e-02, kappa=1.0 (fixed), |gamma|=0.0892, time=2.1min
Epoch   70: Loss=58.2609, LR=9.5e-02, kappa=1.0 (fixed), |gamma|=0.0966, time=2.5min
Epoch   80: Loss=57.6334, LR=9.4e-02, kappa=1.0 (fixed), |gamma|=0.1047, time=2.8min
Epoch   90: Loss=57.1619, LR=9.2e-02, kappa=1.0 (fixed), |gamma|=0.1122, time=3.1min
Epoch  100: Loss=56.7489, LR=9.0e-02, kappa=1.0 (fixed), |gamma|=0.1200, time=3.5min
Epoch  110: Loss=56.3568, LR=8.8e-02, kappa=1.0 (fixed), |gamma|=0.1278, time=3.8min
Epoch  120: Loss=55.9817, LR=8.6e-02, kappa=1.0 (fixed), |gamma|=0.1354, time=4.1min
Epoch  130: Loss=55.6234, LR=8.4e-02, kappa=1.0 (fixed), |gamma|=0.1430, time=4.5min
Epoch  140: Loss=55.2755, LR=8.2e-02, kappa=1.0 (fixed), |gamma|=0.1519, time=4.8min
Epoch  150: Loss=54.9357, LR=7.9e-02, kappa=1.0 (fixed), |gamma|=0.1614, time=5.1min
Epoch  160: Loss=54.6033, LR=7.7e-02, kappa=1.0 (fixed), |gamma|=0.1718, time=5.4min
Epoch  170: Loss=54.2769, LR=7.4e-02, kappa=1.0 (fixed), |gamma|=0.1820, time=5.8min
Epoch  180: Loss=53.9523, LR=7.1e-02, kappa=1.0 (fixed), |gamma|=0.1925, time=6.1min
Epoch  190: Loss=53.6308, LR=6.8e-02, kappa=1.0 (fixed), |gamma|=0.2027, time=6.4min
Epoch  200: Loss=53.3117, LR=6.6e-02, kappa=1.0 (fixed), |gamma|=0.2125, time=6.7min
Epoch  210: Loss=52.9927, LR=6.3e-02, kappa=1.0 (fixed), |gamma|=0.2215, time=7.1min
Epoch  220: Loss=52.6779, LR=5.9e-02, kappa=1.0 (fixed), |gamma|=0.2291, time=7.4min
Epoch  230: Loss=52.3564, LR=5.6e-02, kappa=1.0 (fixed), |gamma|=0.2348, time=7.7min
Epoch  240: Loss=52.0347, LR=5.3e-02, kappa=1.0 (fixed), |gamma|=0.2385, time=8.0min
Epoch  250: Loss=51.7106, LR=5.0e-02, kappa=1.0 (fixed), |gamma|=0.2404, time=8.3min
Epoch  260: Loss=51.3826, LR=4.7e-02, kappa=1.0 (fixed), |gamma|=0.2402, time=8.6min
Epoch  270: Loss=51.0550, LR=4.4e-02, kappa=1.0 (fixed), |gamma|=0.2390, time=8.9min
Epoch  280: Loss=50.7342, LR=4.1e-02, kappa=1.0 (fixed), |gamma|=0.2363, time=9.2min
Epoch  290: Loss=50.4262, LR=3.8e-02, kappa=1.0 (fixed), |gamma|=0.2328, time=9.6min
Epoch  300: Loss=50.1314, LR=3.5e-02, kappa=1.0 (fixed), |gamma|=0.2293, time=9.9min
Epoch  310: Loss=49.8557, LR=3.2e-02, kappa=1.0 (fixed), |gamma|=0.2260, time=10.2min
Epoch  320: Loss=49.5995, LR=2.9e-02, kappa=1.0 (fixed), |gamma|=0.2232, time=10.5min
Epoch  330: Loss=49.3631, LR=2.6e-02, kappa=1.0 (fixed), |gamma|=0.2210, time=10.8min
Epoch  340: Loss=49.1470, LR=2.4e-02, kappa=1.0 (fixed), |gamma|=0.2187, time=11.1min
Epoch  350: Loss=48.9508, LR=2.1e-02, kappa=1.0 (fixed), |gamma|=0.2166, time=11.4min
Epoch  360: Loss=48.7740, LR=1.9e-02, kappa=1.0 (fixed), |gamma|=0.2150, time=11.7min
Epoch  370: Loss=48.6160, LR=1.6e-02, kappa=1.0 (fixed), |gamma|=0.2143, time=12.0min
Epoch  380: Loss=48.4757, LR=1.4e-02, kappa=1.0 (fixed), |gamma|=0.2141, time=12.4min
Epoch  390: Loss=48.3519, LR=1.2e-02, kappa=1.0 (fixed), |gamma|=0.2144, time=12.7min
Epoch  400: Loss=48.2434, LR=1.0e-02, kappa=1.0 (fixed), |gamma|=0.2149, time=13.0min
Epoch  410: Loss=48.1487, LR=8.5e-03, kappa=1.0 (fixed), |gamma|=0.2160, time=13.3min
Epoch  420: Loss=48.0665, LR=7.0e-03, kappa=1.0 (fixed), |gamma|=0.2173, time=13.6min
Epoch  430: Loss=47.9959, LR=5.6e-03, kappa=1.0 (fixed), |gamma|=0.2182, time=13.9min
Epoch  440: Loss=47.9362, LR=4.4e-03, kappa=1.0 (fixed), |gamma|=0.2190, time=14.2min
Epoch  450: Loss=47.8855, LR=3.3e-03, kappa=1.0 (fixed), |gamma|=0.2197, time=14.5min
Epoch  460: Loss=47.8427, LR=2.5e-03, kappa=1.0 (fixed), |gamma|=0.2201, time=14.8min
Epoch  470: Loss=47.8062, LR=1.8e-03, kappa=1.0 (fixed), |gamma|=0.2204, time=15.2min
Epoch  480: Loss=47.7747, LR=1.4e-03, kappa=1.0 (fixed), |gamma|=0.2207, time=15.6min
Epoch  490: Loss=47.7467, LR=1.1e-03, kappa=1.0 (fixed), |gamma|=0.2209, time=15.9min

Training complete: 500 epochs in 16.2 min
Final loss: 47.7232

Final params: kappa=1.0 (fixed), mean|gamma|=0.2210
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2_nokappa/enrollment_model_REPARAM_NOKAPPA_W0.0001_batch_230000_240000.pt
Batch 24 done in 16.4 min

============================================================
BATCH 25/40: 240000-250000
============================================================

============================================================
REPARAM v2 NO KAPPA: samples 240000 to 250000
Epochs: 500, LR: 0.1, grad_clip: 5.0
kappa = 1.0 (FIXED, not learned)
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 28 diseases
Cluster 1: 29 diseases
Cluster 2: 14 diseases
Cluster 3: 8 diseases
Cluster 4: 102 diseases
Cluster 5: 13 diseases
Cluster 6: 9 diseases
Cluster 7: 16 diseases
Cluster 8: 19 diseases
Cluster 9: 7 diseases
Cluster 10: 16 diseases
Cluster 11: 13 diseases
Cluster 12: 6 diseases
Cluster 13: 6 diseases
Cluster 14: 5 diseases
Cluster 15: 13 diseases
Cluster 16: 15 diseases
Cluster 17: 6 diseases
Cluster 18: 12 diseases
Cluster 19: 11 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping (NO KAPPA)...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=64.0704, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.1006, time=0.0min
Epoch   10: Loss=220.0387, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0677, time=0.4min
Epoch   20: Loss=95.4133, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0668, time=0.7min
Epoch   30: Loss=65.0284, LR=9.9e-02, kappa=1.0 (fixed), |gamma|=0.0720, time=1.1min
Epoch   40: Loss=64.1990, LR=9.8e-02, kappa=1.0 (fixed), |gamma|=0.0775, time=1.4min
Epoch   50: Loss=61.9472, LR=9.7e-02, kappa=1.0 (fixed), |gamma|=0.0859, time=1.8min
Epoch   60: Loss=60.2312, LR=9.6e-02, kappa=1.0 (fixed), |gamma|=0.0936, time=2.1min
Epoch   70: Loss=59.2326, LR=9.5e-02, kappa=1.0 (fixed), |gamma|=0.1010, time=2.4min
Epoch   80: Loss=58.5895, LR=9.4e-02, kappa=1.0 (fixed), |gamma|=0.1090, time=2.8min
Epoch   90: Loss=58.1036, LR=9.2e-02, kappa=1.0 (fixed), |gamma|=0.1163, time=3.1min
Epoch  100: Loss=57.6779, LR=9.0e-02, kappa=1.0 (fixed), |gamma|=0.1223, time=3.4min
Epoch  110: Loss=57.2756, LR=8.8e-02, kappa=1.0 (fixed), |gamma|=0.1286, time=3.7min
Epoch  120: Loss=56.8935, LR=8.6e-02, kappa=1.0 (fixed), |gamma|=0.1347, time=4.0min
Epoch  130: Loss=56.5290, LR=8.4e-02, kappa=1.0 (fixed), |gamma|=0.1412, time=4.4min
Epoch  140: Loss=56.1766, LR=8.2e-02, kappa=1.0 (fixed), |gamma|=0.1484, time=4.7min
Epoch  150: Loss=55.8324, LR=7.9e-02, kappa=1.0 (fixed), |gamma|=0.1546, time=5.1min
Epoch  160: Loss=55.4924, LR=7.7e-02, kappa=1.0 (fixed), |gamma|=0.1608, time=5.4min
Epoch  170: Loss=55.1554, LR=7.4e-02, kappa=1.0 (fixed), |gamma|=0.1669, time=5.7min
Epoch  180: Loss=54.8182, LR=7.1e-02, kappa=1.0 (fixed), |gamma|=0.1725, time=6.1min
Epoch  190: Loss=54.4810, LR=6.8e-02, kappa=1.0 (fixed), |gamma|=0.1773, time=6.4min
Epoch  200: Loss=54.1392, LR=6.6e-02, kappa=1.0 (fixed), |gamma|=0.1814, time=6.8min
Epoch  210: Loss=53.7941, LR=6.3e-02, kappa=1.0 (fixed), |gamma|=0.1849, time=7.1min
Epoch  220: Loss=53.4473, LR=5.9e-02, kappa=1.0 (fixed), |gamma|=0.1871, time=7.5min
Epoch  230: Loss=53.0989, LR=5.6e-02, kappa=1.0 (fixed), |gamma|=0.1881, time=7.8min
Epoch  240: Loss=52.7491, LR=5.3e-02, kappa=1.0 (fixed), |gamma|=0.1881, time=8.2min
Epoch  250: Loss=52.3981, LR=5.0e-02, kappa=1.0 (fixed), |gamma|=0.1872, time=8.5min
Epoch  260: Loss=52.0512, LR=4.7e-02, kappa=1.0 (fixed), |gamma|=0.1859, time=8.9min
Epoch  270: Loss=51.7128, LR=4.4e-02, kappa=1.0 (fixed), |gamma|=0.1841, time=9.2min
Epoch  280: Loss=51.3853, LR=4.1e-02, kappa=1.0 (fixed), |gamma|=0.1822, time=9.6min
Epoch  290: Loss=51.0723, LR=3.8e-02, kappa=1.0 (fixed), |gamma|=0.1804, time=9.9min
Epoch  300: Loss=50.7763, LR=3.5e-02, kappa=1.0 (fixed), |gamma|=0.1784, time=10.3min
Epoch  310: Loss=50.4994, LR=3.2e-02, kappa=1.0 (fixed), |gamma|=0.1766, time=10.6min
Epoch  320: Loss=50.2426, LR=2.9e-02, kappa=1.0 (fixed), |gamma|=0.1749, time=11.0min
Epoch  330: Loss=50.0065, LR=2.6e-02, kappa=1.0 (fixed), |gamma|=0.1733, time=11.3min
Epoch  340: Loss=49.7910, LR=2.4e-02, kappa=1.0 (fixed), |gamma|=0.1719, time=11.7min
Epoch  350: Loss=49.5958, LR=2.1e-02, kappa=1.0 (fixed), |gamma|=0.1707, time=12.1min
Epoch  360: Loss=49.4202, LR=1.9e-02, kappa=1.0 (fixed), |gamma|=0.1696, time=12.4min
Epoch  370: Loss=49.2633, LR=1.6e-02, kappa=1.0 (fixed), |gamma|=0.1687, time=12.8min
Epoch  380: Loss=49.1241, LR=1.4e-02, kappa=1.0 (fixed), |gamma|=0.1679, time=13.2min
Epoch  390: Loss=49.0015, LR=1.2e-02, kappa=1.0 (fixed), |gamma|=0.1672, time=13.5min
Epoch  400: Loss=48.8942, LR=1.0e-02, kappa=1.0 (fixed), |gamma|=0.1666, time=13.9min
Epoch  410: Loss=48.8010, LR=8.5e-03, kappa=1.0 (fixed), |gamma|=0.1661, time=14.2min
Epoch  420: Loss=48.7206, LR=7.0e-03, kappa=1.0 (fixed), |gamma|=0.1657, time=14.6min
Epoch  430: Loss=48.6518, LR=5.6e-03, kappa=1.0 (fixed), |gamma|=0.1654, time=14.9min
Epoch  440: Loss=48.5933, LR=4.4e-03, kappa=1.0 (fixed), |gamma|=0.1651, time=15.3min
Epoch  450: Loss=48.5437, LR=3.3e-03, kappa=1.0 (fixed), |gamma|=0.1650, time=15.6min
Epoch  460: Loss=48.5017, LR=2.5e-03, kappa=1.0 (fixed), |gamma|=0.1648, time=16.0min
Epoch  470: Loss=48.4659, LR=1.8e-03, kappa=1.0 (fixed), |gamma|=0.1647, time=16.3min
Epoch  480: Loss=48.4348, LR=1.4e-03, kappa=1.0 (fixed), |gamma|=0.1647, time=16.6min
Epoch  490: Loss=48.4071, LR=1.1e-03, kappa=1.0 (fixed), |gamma|=0.1646, time=17.0min

Training complete: 500 epochs in 17.3 min
Final loss: 48.3838

Final params: kappa=1.0 (fixed), mean|gamma|=0.1646
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2_nokappa/enrollment_model_REPARAM_NOKAPPA_W0.0001_batch_240000_250000.pt
Batch 25 done in 17.5 min

============================================================
BATCH 26/40: 250000-260000
============================================================

============================================================
REPARAM v2 NO KAPPA: samples 250000 to 260000
Epochs: 500, LR: 0.1, grad_clip: 5.0
kappa = 1.0 (FIXED, not learned)
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 88 diseases
Cluster 1: 17 diseases
Cluster 2: 13 diseases
Cluster 3: 17 diseases
Cluster 4: 17 diseases
Cluster 5: 9 diseases
Cluster 6: 10 diseases
Cluster 7: 21 diseases
Cluster 8: 23 diseases
Cluster 9: 4 diseases
Cluster 10: 8 diseases
Cluster 11: 9 diseases
Cluster 12: 4 diseases
Cluster 13: 22 diseases
Cluster 14: 21 diseases
Cluster 15: 11 diseases
Cluster 16: 26 diseases
Cluster 17: 6 diseases
Cluster 18: 8 diseases
Cluster 19: 14 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping (NO KAPPA)...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=63.2965, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.1009, time=0.0min
Epoch   10: Loss=219.3301, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0654, time=0.4min
Epoch   20: Loss=94.6828, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0645, time=0.7min
Epoch   30: Loss=64.2876, LR=9.9e-02, kappa=1.0 (fixed), |gamma|=0.0656, time=1.1min
Epoch   40: Loss=63.4761, LR=9.8e-02, kappa=1.0 (fixed), |gamma|=0.0717, time=1.4min
Epoch   50: Loss=61.2365, LR=9.7e-02, kappa=1.0 (fixed), |gamma|=0.0789, time=1.8min
Epoch   60: Loss=59.5303, LR=9.6e-02, kappa=1.0 (fixed), |gamma|=0.0858, time=2.1min
Epoch   70: Loss=58.5395, LR=9.5e-02, kappa=1.0 (fixed), |gamma|=0.0918, time=2.5min
Epoch   80: Loss=57.9034, LR=9.4e-02, kappa=1.0 (fixed), |gamma|=0.0993, time=2.8min
Epoch   90: Loss=57.4239, LR=9.2e-02, kappa=1.0 (fixed), |gamma|=0.1071, time=3.2min
Epoch  100: Loss=57.0055, LR=9.0e-02, kappa=1.0 (fixed), |gamma|=0.1146, time=3.5min
Epoch  110: Loss=56.6097, LR=8.8e-02, kappa=1.0 (fixed), |gamma|=0.1219, time=3.9min
Epoch  120: Loss=56.2334, LR=8.6e-02, kappa=1.0 (fixed), |gamma|=0.1299, time=4.2min
Epoch  130: Loss=55.8735, LR=8.4e-02, kappa=1.0 (fixed), |gamma|=0.1375, time=4.6min
Epoch  140: Loss=55.5248, LR=8.2e-02, kappa=1.0 (fixed), |gamma|=0.1457, time=4.9min
Epoch  150: Loss=55.1849, LR=7.9e-02, kappa=1.0 (fixed), |gamma|=0.1553, time=5.3min
Epoch  160: Loss=54.8508, LR=7.7e-02, kappa=1.0 (fixed), |gamma|=0.1651, time=5.6min
Epoch  170: Loss=54.5203, LR=7.4e-02, kappa=1.0 (fixed), |gamma|=0.1764, time=6.0min
Epoch  180: Loss=54.1933, LR=7.1e-02, kappa=1.0 (fixed), |gamma|=0.1893, time=6.3min
Epoch  190: Loss=53.8713, LR=6.8e-02, kappa=1.0 (fixed), |gamma|=0.2007, time=6.7min
Epoch  200: Loss=53.5444, LR=6.6e-02, kappa=1.0 (fixed), |gamma|=0.2099, time=7.0min
Epoch  210: Loss=53.2207, LR=6.3e-02, kappa=1.0 (fixed), |gamma|=0.2187, time=7.4min
Epoch  220: Loss=52.8947, LR=5.9e-02, kappa=1.0 (fixed), |gamma|=0.2251, time=7.7min
Epoch  230: Loss=52.5667, LR=5.6e-02, kappa=1.0 (fixed), |gamma|=0.2287, time=8.1min
Epoch  240: Loss=52.2374, LR=5.3e-02, kappa=1.0 (fixed), |gamma|=0.2303, time=8.4min
Epoch  250: Loss=51.9079, LR=5.0e-02, kappa=1.0 (fixed), |gamma|=0.2304, time=8.8min
Epoch  260: Loss=51.5793, LR=4.7e-02, kappa=1.0 (fixed), |gamma|=0.2293, time=9.1min
Epoch  270: Loss=51.2558, LR=4.4e-02, kappa=1.0 (fixed), |gamma|=0.2272, time=9.5min
Epoch  280: Loss=50.9401, LR=4.1e-02, kappa=1.0 (fixed), |gamma|=0.2244, time=9.8min
Epoch  290: Loss=50.6354, LR=3.8e-02, kappa=1.0 (fixed), |gamma|=0.2213, time=10.2min
Epoch  300: Loss=50.3450, LR=3.5e-02, kappa=1.0 (fixed), |gamma|=0.2183, time=10.5min
Epoch  310: Loss=50.0708, LR=3.2e-02, kappa=1.0 (fixed), |gamma|=0.2154, time=10.9min
Epoch  320: Loss=49.8148, LR=2.9e-02, kappa=1.0 (fixed), |gamma|=0.2127, time=11.2min
Epoch  330: Loss=49.5781, LR=2.6e-02, kappa=1.0 (fixed), |gamma|=0.2103, time=11.5min
Epoch  340: Loss=49.3611, LR=2.4e-02, kappa=1.0 (fixed), |gamma|=0.2082, time=11.9min
Epoch  350: Loss=49.1637, LR=2.1e-02, kappa=1.0 (fixed), |gamma|=0.2063, time=12.2min
Epoch  360: Loss=48.9855, LR=1.9e-02, kappa=1.0 (fixed), |gamma|=0.2048, time=12.6min
Epoch  370: Loss=48.8261, LR=1.6e-02, kappa=1.0 (fixed), |gamma|=0.2035, time=12.9min
Epoch  380: Loss=48.6843, LR=1.4e-02, kappa=1.0 (fixed), |gamma|=0.2027, time=13.2min
Epoch  390: Loss=48.5593, LR=1.2e-02, kappa=1.0 (fixed), |gamma|=0.2019, time=13.6min
Epoch  400: Loss=48.4498, LR=1.0e-02, kappa=1.0 (fixed), |gamma|=0.2012, time=13.9min
Epoch  410: Loss=48.3547, LR=8.5e-03, kappa=1.0 (fixed), |gamma|=0.2007, time=14.2min
Epoch  420: Loss=48.2727, LR=7.0e-03, kappa=1.0 (fixed), |gamma|=0.2002, time=14.6min
Epoch  430: Loss=48.2025, LR=5.6e-03, kappa=1.0 (fixed), |gamma|=0.1999, time=14.9min
Epoch  440: Loss=48.1428, LR=4.4e-03, kappa=1.0 (fixed), |gamma|=0.1996, time=15.3min
Epoch  450: Loss=48.0922, LR=3.3e-03, kappa=1.0 (fixed), |gamma|=0.1994, time=15.6min
Epoch  460: Loss=48.0494, LR=2.5e-03, kappa=1.0 (fixed), |gamma|=0.1992, time=16.0min
Epoch  470: Loss=48.0130, LR=1.8e-03, kappa=1.0 (fixed), |gamma|=0.1991, time=16.3min
Epoch  480: Loss=47.9815, LR=1.4e-03, kappa=1.0 (fixed), |gamma|=0.1990, time=16.7min
Epoch  490: Loss=47.9534, LR=1.1e-03, kappa=1.0 (fixed), |gamma|=0.1989, time=17.0min

Training complete: 500 epochs in 17.3 min
Final loss: 47.9297

Final params: kappa=1.0 (fixed), mean|gamma|=0.1989
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2_nokappa/enrollment_model_REPARAM_NOKAPPA_W0.0001_batch_250000_260000.pt
Batch 26 done in 17.5 min

============================================================
BATCH 27/40: 260000-270000
============================================================

============================================================
REPARAM v2 NO KAPPA: samples 260000 to 270000
Epochs: 500, LR: 0.1, grad_clip: 5.0
kappa = 1.0 (FIXED, not learned)
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 28 diseases
Cluster 1: 16 diseases
Cluster 2: 17 diseases
Cluster 3: 14 diseases
Cluster 4: 6 diseases
Cluster 5: 23 diseases
Cluster 6: 28 diseases
Cluster 7: 8 diseases
Cluster 8: 10 diseases
Cluster 9: 9 diseases
Cluster 10: 10 diseases
Cluster 11: 65 diseases
Cluster 12: 14 diseases
Cluster 13: 13 diseases
Cluster 14: 2 diseases
Cluster 15: 6 diseases
Cluster 16: 20 diseases
Cluster 17: 5 diseases
Cluster 18: 47 diseases
Cluster 19: 7 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping (NO KAPPA)...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=63.6040, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.1009, time=0.0min
Epoch   10: Loss=219.6263, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0676, time=0.4min
Epoch   20: Loss=94.9460, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0673, time=0.7min
Epoch   30: Loss=64.5663, LR=9.9e-02, kappa=1.0 (fixed), |gamma|=0.0723, time=1.0min
Epoch   40: Loss=63.7509, LR=9.8e-02, kappa=1.0 (fixed), |gamma|=0.0765, time=1.4min
Epoch   50: Loss=61.5019, LR=9.7e-02, kappa=1.0 (fixed), |gamma|=0.0854, time=1.7min
Epoch   60: Loss=59.7880, LR=9.6e-02, kappa=1.0 (fixed), |gamma|=0.0930, time=2.1min
Epoch   70: Loss=58.7909, LR=9.5e-02, kappa=1.0 (fixed), |gamma|=0.1007, time=2.4min
Epoch   80: Loss=58.1499, LR=9.4e-02, kappa=1.0 (fixed), |gamma|=0.1083, time=2.7min
Epoch   90: Loss=57.6670, LR=9.2e-02, kappa=1.0 (fixed), |gamma|=0.1156, time=3.0min
Epoch  100: Loss=57.2447, LR=9.0e-02, kappa=1.0 (fixed), |gamma|=0.1224, time=3.4min
Epoch  110: Loss=56.8442, LR=8.8e-02, kappa=1.0 (fixed), |gamma|=0.1283, time=3.7min
Epoch  120: Loss=56.4631, LR=8.6e-02, kappa=1.0 (fixed), |gamma|=0.1344, time=4.0min
Epoch  130: Loss=56.0990, LR=8.4e-02, kappa=1.0 (fixed), |gamma|=0.1400, time=4.3min
Epoch  140: Loss=55.7472, LR=8.2e-02, kappa=1.0 (fixed), |gamma|=0.1460, time=4.6min
Epoch  150: Loss=55.4013, LR=7.9e-02, kappa=1.0 (fixed), |gamma|=0.1522, time=5.0min
Epoch  160: Loss=55.0604, LR=7.7e-02, kappa=1.0 (fixed), |gamma|=0.1587, time=5.3min
Epoch  170: Loss=54.7278, LR=7.4e-02, kappa=1.0 (fixed), |gamma|=0.1657, time=5.6min
Epoch  180: Loss=54.3891, LR=7.1e-02, kappa=1.0 (fixed), |gamma|=0.1717, time=5.9min
Epoch  190: Loss=54.0491, LR=6.8e-02, kappa=1.0 (fixed), |gamma|=0.1770, time=6.3min
Epoch  200: Loss=53.7106, LR=6.6e-02, kappa=1.0 (fixed), |gamma|=0.1823, time=6.6min
Epoch  210: Loss=53.3685, LR=6.3e-02, kappa=1.0 (fixed), |gamma|=0.1875, time=6.9min
Epoch  220: Loss=53.0283, LR=5.9e-02, kappa=1.0 (fixed), |gamma|=0.1923, time=7.2min
Epoch  230: Loss=52.6788, LR=5.6e-02, kappa=1.0 (fixed), |gamma|=0.1947, time=7.5min
Epoch  240: Loss=52.3313, LR=5.3e-02, kappa=1.0 (fixed), |gamma|=0.1954, time=7.9min
Epoch  250: Loss=51.9858, LR=5.0e-02, kappa=1.0 (fixed), |gamma|=0.1958, time=8.2min
Epoch  260: Loss=51.6425, LR=4.7e-02, kappa=1.0 (fixed), |gamma|=0.1952, time=8.5min
Epoch  270: Loss=51.3057, LR=4.4e-02, kappa=1.0 (fixed), |gamma|=0.1940, time=8.8min
Epoch  280: Loss=50.9791, LR=4.1e-02, kappa=1.0 (fixed), |gamma|=0.1923, time=9.1min
Epoch  290: Loss=50.6665, LR=3.8e-02, kappa=1.0 (fixed), |gamma|=0.1906, time=9.4min
Epoch  300: Loss=50.3703, LR=3.5e-02, kappa=1.0 (fixed), |gamma|=0.1888, time=9.8min
Epoch  310: Loss=50.0926, LR=3.2e-02, kappa=1.0 (fixed), |gamma|=0.1871, time=10.1min
Epoch  320: Loss=49.8347, LR=2.9e-02, kappa=1.0 (fixed), |gamma|=0.1855, time=10.4min
Epoch  330: Loss=49.5972, LR=2.6e-02, kappa=1.0 (fixed), |gamma|=0.1840, time=10.7min
Epoch  340: Loss=49.3803, LR=2.4e-02, kappa=1.0 (fixed), |gamma|=0.1827, time=11.0min
Epoch  350: Loss=49.1836, LR=2.1e-02, kappa=1.0 (fixed), |gamma|=0.1816, time=11.3min
Epoch  360: Loss=49.0065, LR=1.9e-02, kappa=1.0 (fixed), |gamma|=0.1806, time=11.6min
Epoch  370: Loss=48.8482, LR=1.6e-02, kappa=1.0 (fixed), |gamma|=0.1798, time=11.9min
Epoch  380: Loss=48.7077, LR=1.4e-02, kappa=1.0 (fixed), |gamma|=0.1790, time=12.2min
Epoch  390: Loss=48.5838, LR=1.2e-02, kappa=1.0 (fixed), |gamma|=0.1783, time=12.5min
Epoch  400: Loss=48.4754, LR=1.0e-02, kappa=1.0 (fixed), |gamma|=0.1777, time=12.9min
Epoch  410: Loss=48.3812, LR=8.5e-03, kappa=1.0 (fixed), |gamma|=0.1773, time=13.2min
Epoch  420: Loss=48.3001, LR=7.0e-03, kappa=1.0 (fixed), |gamma|=0.1769, time=13.5min
Epoch  430: Loss=48.2306, LR=5.6e-03, kappa=1.0 (fixed), |gamma|=0.1765, time=13.8min
Epoch  440: Loss=48.1715, LR=4.4e-03, kappa=1.0 (fixed), |gamma|=0.1763, time=14.1min
Epoch  450: Loss=48.1215, LR=3.3e-03, kappa=1.0 (fixed), |gamma|=0.1761, time=14.4min
Epoch  460: Loss=48.0792, LR=2.5e-03, kappa=1.0 (fixed), |gamma|=0.1759, time=14.7min
Epoch  470: Loss=48.0431, LR=1.8e-03, kappa=1.0 (fixed), |gamma|=0.1758, time=15.1min
Epoch  480: Loss=48.0119, LR=1.4e-03, kappa=1.0 (fixed), |gamma|=0.1758, time=15.4min
Epoch  490: Loss=47.9840, LR=1.1e-03, kappa=1.0 (fixed), |gamma|=0.1757, time=15.7min

Training complete: 500 epochs in 16.0 min
Final loss: 47.9605

Final params: kappa=1.0 (fixed), mean|gamma|=0.1757
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2_nokappa/enrollment_model_REPARAM_NOKAPPA_W0.0001_batch_260000_270000.pt
Batch 27 done in 16.2 min

============================================================
BATCH 28/40: 270000-280000
============================================================

============================================================
REPARAM v2 NO KAPPA: samples 270000 to 280000
Epochs: 500, LR: 0.1, grad_clip: 5.0
kappa = 1.0 (FIXED, not learned)
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 15 diseases
Cluster 1: 95 diseases
Cluster 2: 6 diseases
Cluster 3: 25 diseases
Cluster 4: 8 diseases
Cluster 5: 14 diseases
Cluster 6: 7 diseases
Cluster 7: 22 diseases
Cluster 8: 24 diseases
Cluster 9: 8 diseases
Cluster 10: 13 diseases
Cluster 11: 12 diseases
Cluster 12: 9 diseases
Cluster 13: 15 diseases
Cluster 14: 13 diseases
Cluster 15: 18 diseases
Cluster 16: 20 diseases
Cluster 17: 5 diseases
Cluster 18: 9 diseases
Cluster 19: 10 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping (NO KAPPA)...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=63.9523, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.1011, time=0.0min
Epoch   10: Loss=219.9272, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0678, time=0.4min
Epoch   20: Loss=95.2998, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0666, time=0.7min
Epoch   30: Loss=64.9187, LR=9.9e-02, kappa=1.0 (fixed), |gamma|=0.0695, time=1.1min
Epoch   40: Loss=64.0816, LR=9.8e-02, kappa=1.0 (fixed), |gamma|=0.0732, time=1.4min
Epoch   50: Loss=61.8274, LR=9.7e-02, kappa=1.0 (fixed), |gamma|=0.0804, time=1.8min
Epoch   60: Loss=60.1126, LR=9.6e-02, kappa=1.0 (fixed), |gamma|=0.0871, time=2.1min
Epoch   70: Loss=59.1166, LR=9.5e-02, kappa=1.0 (fixed), |gamma|=0.0954, time=2.4min
Epoch   80: Loss=58.4785, LR=9.4e-02, kappa=1.0 (fixed), |gamma|=0.1038, time=2.8min
Epoch   90: Loss=57.9995, LR=9.2e-02, kappa=1.0 (fixed), |gamma|=0.1108, time=3.1min
Epoch  100: Loss=57.5820, LR=9.0e-02, kappa=1.0 (fixed), |gamma|=0.1176, time=3.4min
Epoch  110: Loss=57.1889, LR=8.8e-02, kappa=1.0 (fixed), |gamma|=0.1244, time=3.8min
Epoch  120: Loss=56.8157, LR=8.6e-02, kappa=1.0 (fixed), |gamma|=0.1309, time=4.1min
Epoch  130: Loss=56.4602, LR=8.4e-02, kappa=1.0 (fixed), |gamma|=0.1380, time=4.4min
Epoch  140: Loss=56.1164, LR=8.2e-02, kappa=1.0 (fixed), |gamma|=0.1444, time=4.7min
Epoch  150: Loss=55.7814, LR=7.9e-02, kappa=1.0 (fixed), |gamma|=0.1505, time=5.1min
Epoch  160: Loss=55.4521, LR=7.7e-02, kappa=1.0 (fixed), |gamma|=0.1556, time=5.4min
Epoch  170: Loss=55.1261, LR=7.4e-02, kappa=1.0 (fixed), |gamma|=0.1611, time=5.7min
Epoch  180: Loss=54.8119, LR=7.1e-02, kappa=1.0 (fixed), |gamma|=0.1673, time=6.0min
Epoch  190: Loss=54.4810, LR=6.8e-02, kappa=1.0 (fixed), |gamma|=0.1724, time=6.4min
Epoch  200: Loss=54.1534, LR=6.6e-02, kappa=1.0 (fixed), |gamma|=0.1761, time=6.7min
Epoch  210: Loss=53.8222, LR=6.3e-02, kappa=1.0 (fixed), |gamma|=0.1787, time=7.0min
Epoch  220: Loss=53.4886, LR=5.9e-02, kappa=1.0 (fixed), |gamma|=0.1803, time=7.3min
Epoch  230: Loss=53.1518, LR=5.6e-02, kappa=1.0 (fixed), |gamma|=0.1818, time=7.7min
Epoch  240: Loss=52.8132, LR=5.3e-02, kappa=1.0 (fixed), |gamma|=0.1820, time=8.0min
Epoch  250: Loss=52.4711, LR=5.0e-02, kappa=1.0 (fixed), |gamma|=0.1800, time=8.4min
Epoch  260: Loss=52.1295, LR=4.7e-02, kappa=1.0 (fixed), |gamma|=0.1782, time=8.7min
Epoch  270: Loss=51.7908, LR=4.4e-02, kappa=1.0 (fixed), |gamma|=0.1757, time=9.0min
Epoch  280: Loss=51.4586, LR=4.1e-02, kappa=1.0 (fixed), |gamma|=0.1729, time=9.4min
Epoch  290: Loss=51.1362, LR=3.8e-02, kappa=1.0 (fixed), |gamma|=0.1700, time=9.7min
Epoch  300: Loss=50.8280, LR=3.5e-02, kappa=1.0 (fixed), |gamma|=0.1670, time=10.0min
Epoch  310: Loss=50.5358, LR=3.2e-02, kappa=1.0 (fixed), |gamma|=0.1642, time=10.4min
Epoch  320: Loss=50.2618, LR=2.9e-02, kappa=1.0 (fixed), |gamma|=0.1615, time=10.7min
Epoch  330: Loss=50.0081, LR=2.6e-02, kappa=1.0 (fixed), |gamma|=0.1591, time=11.0min
Epoch  340: Loss=49.7751, LR=2.4e-02, kappa=1.0 (fixed), |gamma|=0.1569, time=11.4min
Epoch  350: Loss=49.5630, LR=2.1e-02, kappa=1.0 (fixed), |gamma|=0.1549, time=11.7min
Epoch  360: Loss=49.3715, LR=1.9e-02, kappa=1.0 (fixed), |gamma|=0.1531, time=12.0min
Epoch  370: Loss=49.1999, LR=1.6e-02, kappa=1.0 (fixed), |gamma|=0.1515, time=12.3min
Epoch  380: Loss=49.0473, LR=1.4e-02, kappa=1.0 (fixed), |gamma|=0.1502, time=12.6min
Epoch  390: Loss=48.9126, LR=1.2e-02, kappa=1.0 (fixed), |gamma|=0.1491, time=12.9min
Epoch  400: Loss=48.7947, LR=1.0e-02, kappa=1.0 (fixed), |gamma|=0.1482, time=13.3min
Epoch  410: Loss=48.6922, LR=8.5e-03, kappa=1.0 (fixed), |gamma|=0.1474, time=13.6min
Epoch  420: Loss=48.6038, LR=7.0e-03, kappa=1.0 (fixed), |gamma|=0.1467, time=13.9min
Epoch  430: Loss=48.5282, LR=5.6e-03, kappa=1.0 (fixed), |gamma|=0.1461, time=14.2min
Epoch  440: Loss=48.4639, LR=4.4e-03, kappa=1.0 (fixed), |gamma|=0.1457, time=14.6min
Epoch  450: Loss=48.4094, LR=3.3e-03, kappa=1.0 (fixed), |gamma|=0.1454, time=14.9min
Epoch  460: Loss=48.3633, LR=2.5e-03, kappa=1.0 (fixed), |gamma|=0.1451, time=15.2min
Epoch  470: Loss=48.3241, LR=1.8e-03, kappa=1.0 (fixed), |gamma|=0.1449, time=15.5min
Epoch  480: Loss=48.2902, LR=1.4e-03, kappa=1.0 (fixed), |gamma|=0.1447, time=15.8min
Epoch  490: Loss=48.2599, LR=1.1e-03, kappa=1.0 (fixed), |gamma|=0.1446, time=16.2min

Training complete: 500 epochs in 16.5 min
Final loss: 48.2344

Final params: kappa=1.0 (fixed), mean|gamma|=0.1445
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2_nokappa/enrollment_model_REPARAM_NOKAPPA_W0.0001_batch_270000_280000.pt
Batch 28 done in 16.6 min

============================================================
BATCH 29/40: 280000-290000
============================================================

============================================================
REPARAM v2 NO KAPPA: samples 280000 to 290000
Epochs: 500, LR: 0.1, grad_clip: 5.0
kappa = 1.0 (FIXED, not learned)
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 13 diseases
Cluster 1: 15 diseases
Cluster 2: 11 diseases
Cluster 3: 102 diseases
Cluster 4: 17 diseases
Cluster 5: 26 diseases
Cluster 6: 9 diseases
Cluster 7: 14 diseases
Cluster 8: 7 diseases
Cluster 9: 10 diseases
Cluster 10: 28 diseases
Cluster 11: 5 diseases
Cluster 12: 16 diseases
Cluster 13: 24 diseases
Cluster 14: 8 diseases
Cluster 15: 5 diseases
Cluster 16: 10 diseases
Cluster 17: 12 diseases
Cluster 18: 9 diseases
Cluster 19: 7 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping (NO KAPPA)...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=63.6860, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.1012, time=0.0min
Epoch   10: Loss=219.6735, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0694, time=0.4min
Epoch   20: Loss=95.0347, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0670, time=0.7min
Epoch   30: Loss=64.6458, LR=9.9e-02, kappa=1.0 (fixed), |gamma|=0.0689, time=1.0min
Epoch   40: Loss=63.8241, LR=9.8e-02, kappa=1.0 (fixed), |gamma|=0.0715, time=1.4min
Epoch   50: Loss=61.5736, LR=9.7e-02, kappa=1.0 (fixed), |gamma|=0.0780, time=1.7min
Epoch   60: Loss=59.8609, LR=9.6e-02, kappa=1.0 (fixed), |gamma|=0.0841, time=2.1min
Epoch   70: Loss=58.8671, LR=9.5e-02, kappa=1.0 (fixed), |gamma|=0.0915, time=2.4min
Epoch   80: Loss=58.2307, LR=9.4e-02, kappa=1.0 (fixed), |gamma|=0.0993, time=2.7min
Epoch   90: Loss=57.7524, LR=9.2e-02, kappa=1.0 (fixed), |gamma|=0.1070, time=3.1min
Epoch  100: Loss=57.3336, LR=9.0e-02, kappa=1.0 (fixed), |gamma|=0.1155, time=3.4min
Epoch  110: Loss=56.9375, LR=8.8e-02, kappa=1.0 (fixed), |gamma|=0.1239, time=3.7min
Epoch  120: Loss=56.5588, LR=8.6e-02, kappa=1.0 (fixed), |gamma|=0.1316, time=4.1min
Epoch  130: Loss=56.1967, LR=8.4e-02, kappa=1.0 (fixed), |gamma|=0.1388, time=4.4min
Epoch  140: Loss=55.8452, LR=8.2e-02, kappa=1.0 (fixed), |gamma|=0.1477, time=4.7min
Epoch  150: Loss=55.5013, LR=7.9e-02, kappa=1.0 (fixed), |gamma|=0.1576, time=5.1min
Epoch  160: Loss=55.1617, LR=7.7e-02, kappa=1.0 (fixed), |gamma|=0.1671, time=5.4min
Epoch  170: Loss=54.8248, LR=7.4e-02, kappa=1.0 (fixed), |gamma|=0.1764, time=5.7min
Epoch  180: Loss=54.4907, LR=7.1e-02, kappa=1.0 (fixed), |gamma|=0.1853, time=6.1min
Epoch  190: Loss=54.1565, LR=6.8e-02, kappa=1.0 (fixed), |gamma|=0.1933, time=6.4min
Epoch  200: Loss=53.8220, LR=6.6e-02, kappa=1.0 (fixed), |gamma|=0.2001, time=6.8min
Epoch  210: Loss=53.4864, LR=6.3e-02, kappa=1.0 (fixed), |gamma|=0.2060, time=7.1min
Epoch  220: Loss=53.1486, LR=5.9e-02, kappa=1.0 (fixed), |gamma|=0.2099, time=7.5min
Epoch  230: Loss=52.8138, LR=5.6e-02, kappa=1.0 (fixed), |gamma|=0.2117, time=7.8min
Epoch  240: Loss=52.4819, LR=5.3e-02, kappa=1.0 (fixed), |gamma|=0.2122, time=8.2min
Epoch  250: Loss=52.1528, LR=5.0e-02, kappa=1.0 (fixed), |gamma|=0.2115, time=8.5min
Epoch  260: Loss=51.8322, LR=4.7e-02, kappa=1.0 (fixed), |gamma|=0.2100, time=8.9min
Epoch  270: Loss=51.5222, LR=4.4e-02, kappa=1.0 (fixed), |gamma|=0.2085, time=9.2min
Epoch  280: Loss=51.2221, LR=4.1e-02, kappa=1.0 (fixed), |gamma|=0.2068, time=9.6min
Epoch  290: Loss=50.9375, LR=3.8e-02, kappa=1.0 (fixed), |gamma|=0.2053, time=9.9min
Epoch  300: Loss=50.6673, LR=3.5e-02, kappa=1.0 (fixed), |gamma|=0.2040, time=10.2min
Epoch  310: Loss=50.4145, LR=3.2e-02, kappa=1.0 (fixed), |gamma|=0.2027, time=10.6min
Epoch  320: Loss=50.1794, LR=2.9e-02, kappa=1.0 (fixed), |gamma|=0.2015, time=10.9min
Epoch  330: Loss=49.9623, LR=2.6e-02, kappa=1.0 (fixed), |gamma|=0.2003, time=11.3min
Epoch  340: Loss=49.7634, LR=2.4e-02, kappa=1.0 (fixed), |gamma|=0.1991, time=11.6min
Epoch  350: Loss=49.5823, LR=2.1e-02, kappa=1.0 (fixed), |gamma|=0.1981, time=11.9min
Epoch  360: Loss=49.4186, LR=1.9e-02, kappa=1.0 (fixed), |gamma|=0.1972, time=12.3min
Epoch  370: Loss=49.2718, LR=1.6e-02, kappa=1.0 (fixed), |gamma|=0.1963, time=12.6min
Epoch  380: Loss=49.1410, LR=1.4e-02, kappa=1.0 (fixed), |gamma|=0.1956, time=13.0min
Epoch  390: Loss=49.0253, LR=1.2e-02, kappa=1.0 (fixed), |gamma|=0.1950, time=13.3min
Epoch  400: Loss=48.9237, LR=1.0e-02, kappa=1.0 (fixed), |gamma|=0.1944, time=13.6min
Epoch  410: Loss=48.8353, LR=8.5e-03, kappa=1.0 (fixed), |gamma|=0.1940, time=14.0min
Epoch  420: Loss=48.7589, LR=7.0e-03, kappa=1.0 (fixed), |gamma|=0.1936, time=14.3min
Epoch  430: Loss=48.6933, LR=5.6e-03, kappa=1.0 (fixed), |gamma|=0.1933, time=14.7min
Epoch  440: Loss=48.6375, LR=4.4e-03, kappa=1.0 (fixed), |gamma|=0.1930, time=15.0min
Epoch  450: Loss=48.5902, LR=3.3e-03, kappa=1.0 (fixed), |gamma|=0.1928, time=15.3min
Epoch  460: Loss=48.5501, LR=2.5e-03, kappa=1.0 (fixed), |gamma|=0.1927, time=15.7min
Epoch  470: Loss=48.5160, LR=1.8e-03, kappa=1.0 (fixed), |gamma|=0.1926, time=16.0min
Epoch  480: Loss=48.4865, LR=1.4e-03, kappa=1.0 (fixed), |gamma|=0.1925, time=16.4min
Epoch  490: Loss=48.4601, LR=1.1e-03, kappa=1.0 (fixed), |gamma|=0.1924, time=16.7min

Training complete: 500 epochs in 17.1 min
Final loss: 48.4379

Final params: kappa=1.0 (fixed), mean|gamma|=0.1924
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2_nokappa/enrollment_model_REPARAM_NOKAPPA_W0.0001_batch_280000_290000.pt
Batch 29 done in 17.2 min

============================================================
BATCH 30/40: 290000-300000
============================================================

============================================================
REPARAM v2 NO KAPPA: samples 290000 to 300000
Epochs: 500, LR: 0.1, grad_clip: 5.0
kappa = 1.0 (FIXED, not learned)
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 16 diseases
Cluster 1: 11 diseases
Cluster 2: 7 diseases
Cluster 3: 13 diseases
Cluster 4: 16 diseases
Cluster 5: 8 diseases
Cluster 6: 5 diseases
Cluster 7: 31 diseases
Cluster 8: 28 diseases
Cluster 9: 9 diseases
Cluster 10: 13 diseases
Cluster 11: 9 diseases
Cluster 12: 8 diseases
Cluster 13: 5 diseases
Cluster 14: 18 diseases
Cluster 15: 80 diseases
Cluster 16: 16 diseases
Cluster 17: 9 diseases
Cluster 18: 14 diseases
Cluster 19: 32 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping (NO KAPPA)...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=63.6402, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.1008, time=0.0min
Epoch   10: Loss=219.6722, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0666, time=0.4min
Epoch   20: Loss=95.0288, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0646, time=0.8min
Epoch   30: Loss=64.6490, LR=9.9e-02, kappa=1.0 (fixed), |gamma|=0.0655, time=1.2min
Epoch   40: Loss=63.8373, LR=9.8e-02, kappa=1.0 (fixed), |gamma|=0.0704, time=1.5min
Epoch   50: Loss=61.5927, LR=9.7e-02, kappa=1.0 (fixed), |gamma|=0.0774, time=1.9min
Epoch   60: Loss=59.8830, LR=9.6e-02, kappa=1.0 (fixed), |gamma|=0.0844, time=2.3min
Epoch   70: Loss=58.8906, LR=9.5e-02, kappa=1.0 (fixed), |gamma|=0.0919, time=2.6min
Epoch   80: Loss=58.2545, LR=9.4e-02, kappa=1.0 (fixed), |gamma|=0.0992, time=3.0min
Epoch   90: Loss=57.7742, LR=9.2e-02, kappa=1.0 (fixed), |gamma|=0.1058, time=3.4min
Epoch  100: Loss=57.3559, LR=9.0e-02, kappa=1.0 (fixed), |gamma|=0.1137, time=3.7min
Epoch  110: Loss=56.9613, LR=8.8e-02, kappa=1.0 (fixed), |gamma|=0.1220, time=4.1min
Epoch  120: Loss=56.5851, LR=8.6e-02, kappa=1.0 (fixed), |gamma|=0.1300, time=4.5min
Epoch  130: Loss=56.2255, LR=8.4e-02, kappa=1.0 (fixed), |gamma|=0.1381, time=4.8min
Epoch  140: Loss=55.8771, LR=8.2e-02, kappa=1.0 (fixed), |gamma|=0.1471, time=5.2min
Epoch  150: Loss=55.5375, LR=7.9e-02, kappa=1.0 (fixed), |gamma|=0.1559, time=5.6min
Epoch  160: Loss=55.2023, LR=7.7e-02, kappa=1.0 (fixed), |gamma|=0.1647, time=5.9min
Epoch  170: Loss=54.8717, LR=7.4e-02, kappa=1.0 (fixed), |gamma|=0.1740, time=6.3min
Epoch  180: Loss=54.5440, LR=7.1e-02, kappa=1.0 (fixed), |gamma|=0.1832, time=6.7min
Epoch  190: Loss=54.2140, LR=6.8e-02, kappa=1.0 (fixed), |gamma|=0.1920, time=7.0min
Epoch  200: Loss=53.8834, LR=6.6e-02, kappa=1.0 (fixed), |gamma|=0.1999, time=7.4min
Epoch  210: Loss=53.5492, LR=6.3e-02, kappa=1.0 (fixed), |gamma|=0.2066, time=7.7min
Epoch  220: Loss=53.2115, LR=5.9e-02, kappa=1.0 (fixed), |gamma|=0.2120, time=8.1min
Epoch  230: Loss=52.8707, LR=5.6e-02, kappa=1.0 (fixed), |gamma|=0.2159, time=8.5min
Epoch  240: Loss=52.5274, LR=5.3e-02, kappa=1.0 (fixed), |gamma|=0.2181, time=8.9min
Epoch  250: Loss=52.1828, LR=5.0e-02, kappa=1.0 (fixed), |gamma|=0.2188, time=9.2min
Epoch  260: Loss=51.8431, LR=4.7e-02, kappa=1.0 (fixed), |gamma|=0.2182, time=9.6min
Epoch  270: Loss=51.5055, LR=4.4e-02, kappa=1.0 (fixed), |gamma|=0.2171, time=10.0min
Epoch  280: Loss=51.1779, LR=4.1e-02, kappa=1.0 (fixed), |gamma|=0.2154, time=10.4min
Epoch  290: Loss=50.8632, LR=3.8e-02, kappa=1.0 (fixed), |gamma|=0.2134, time=10.7min
Epoch  300: Loss=50.5639, LR=3.5e-02, kappa=1.0 (fixed), |gamma|=0.2111, time=11.1min
Epoch  310: Loss=50.2819, LR=3.2e-02, kappa=1.0 (fixed), |gamma|=0.2087, time=11.5min
Epoch  320: Loss=50.0189, LR=2.9e-02, kappa=1.0 (fixed), |gamma|=0.2062, time=11.9min
Epoch  330: Loss=49.7759, LR=2.6e-02, kappa=1.0 (fixed), |gamma|=0.2037, time=12.2min
Epoch  340: Loss=49.5531, LR=2.4e-02, kappa=1.0 (fixed), |gamma|=0.2014, time=12.6min
Epoch  350: Loss=49.3505, LR=2.1e-02, kappa=1.0 (fixed), |gamma|=0.1992, time=13.0min
Epoch  360: Loss=49.1677, LR=1.9e-02, kappa=1.0 (fixed), |gamma|=0.1972, time=13.3min
Epoch  370: Loss=49.0039, LR=1.6e-02, kappa=1.0 (fixed), |gamma|=0.1954, time=13.7min
Epoch  380: Loss=48.8583, LR=1.4e-02, kappa=1.0 (fixed), |gamma|=0.1938, time=14.1min
Epoch  390: Loss=48.7297, LR=1.2e-02, kappa=1.0 (fixed), |gamma|=0.1925, time=14.4min
Epoch  400: Loss=48.6170, LR=1.0e-02, kappa=1.0 (fixed), |gamma|=0.1913, time=14.8min
Epoch  410: Loss=48.5190, LR=8.5e-03, kappa=1.0 (fixed), |gamma|=0.1902, time=15.2min
Epoch  420: Loss=48.4344, LR=7.0e-03, kappa=1.0 (fixed), |gamma|=0.1894, time=15.6min
Epoch  430: Loss=48.3619, LR=5.6e-03, kappa=1.0 (fixed), |gamma|=0.1887, time=15.9min
Epoch  440: Loss=48.3002, LR=4.4e-03, kappa=1.0 (fixed), |gamma|=0.1881, time=16.3min
Epoch  450: Loss=48.2479, LR=3.3e-03, kappa=1.0 (fixed), |gamma|=0.1877, time=16.7min
Epoch  460: Loss=48.2035, LR=2.5e-03, kappa=1.0 (fixed), |gamma|=0.1873, time=17.1min
Epoch  470: Loss=48.1657, LR=1.8e-03, kappa=1.0 (fixed), |gamma|=0.1870, time=17.5min
Epoch  480: Loss=48.1329, LR=1.4e-03, kappa=1.0 (fixed), |gamma|=0.1869, time=17.8min
Epoch  490: Loss=48.1036, LR=1.1e-03, kappa=1.0 (fixed), |gamma|=0.1867, time=18.2min

Training complete: 500 epochs in 18.6 min
Final loss: 48.0789

Final params: kappa=1.0 (fixed), mean|gamma|=0.1866
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2_nokappa/enrollment_model_REPARAM_NOKAPPA_W0.0001_batch_290000_300000.pt
Batch 30 done in 18.7 min

============================================================
BATCH 31/40: 300000-310000
============================================================

============================================================
REPARAM v2 NO KAPPA: samples 300000 to 310000
Epochs: 500, LR: 0.1, grad_clip: 5.0
kappa = 1.0 (FIXED, not learned)
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 14 diseases
Cluster 1: 27 diseases
Cluster 2: 12 diseases
Cluster 3: 11 diseases
Cluster 4: 27 diseases
Cluster 5: 8 diseases
Cluster 6: 4 diseases
Cluster 7: 13 diseases
Cluster 8: 8 diseases
Cluster 9: 8 diseases
Cluster 10: 7 diseases
Cluster 11: 7 diseases
Cluster 12: 32 diseases
Cluster 13: 12 diseases
Cluster 14: 20 diseases
Cluster 15: 22 diseases
Cluster 16: 10 diseases
Cluster 17: 15 diseases
Cluster 18: 85 diseases
Cluster 19: 6 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping (NO KAPPA)...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=62.7682, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.1011, time=0.0min
Epoch   10: Loss=218.8472, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0710, time=0.4min
Epoch   20: Loss=94.1832, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0663, time=0.8min
Epoch   30: Loss=63.8109, LR=9.9e-02, kappa=1.0 (fixed), |gamma|=0.0700, time=1.2min
Epoch   40: Loss=63.0158, LR=9.8e-02, kappa=1.0 (fixed), |gamma|=0.0737, time=1.6min
Epoch   50: Loss=60.7888, LR=9.7e-02, kappa=1.0 (fixed), |gamma|=0.0800, time=2.0min
Epoch   60: Loss=59.0945, LR=9.6e-02, kappa=1.0 (fixed), |gamma|=0.0870, time=2.4min
Epoch   70: Loss=58.1165, LR=9.5e-02, kappa=1.0 (fixed), |gamma|=0.0945, time=2.7min
Epoch   80: Loss=57.4940, LR=9.4e-02, kappa=1.0 (fixed), |gamma|=0.1021, time=3.1min
Epoch   90: Loss=57.0275, LR=9.2e-02, kappa=1.0 (fixed), |gamma|=0.1089, time=3.5min
Epoch  100: Loss=56.6192, LR=9.0e-02, kappa=1.0 (fixed), |gamma|=0.1163, time=3.9min
Epoch  110: Loss=56.2320, LR=8.8e-02, kappa=1.0 (fixed), |gamma|=0.1236, time=4.3min
Epoch  120: Loss=55.8620, LR=8.6e-02, kappa=1.0 (fixed), |gamma|=0.1309, time=4.7min
Epoch  130: Loss=55.5082, LR=8.4e-02, kappa=1.0 (fixed), |gamma|=0.1381, time=5.1min
Epoch  140: Loss=55.1648, LR=8.2e-02, kappa=1.0 (fixed), |gamma|=0.1450, time=5.5min
Epoch  150: Loss=54.8287, LR=7.9e-02, kappa=1.0 (fixed), |gamma|=0.1523, time=5.9min
Epoch  160: Loss=54.4983, LR=7.7e-02, kappa=1.0 (fixed), |gamma|=0.1603, time=6.3min
Epoch  170: Loss=54.1737, LR=7.4e-02, kappa=1.0 (fixed), |gamma|=0.1684, time=6.7min
Epoch  180: Loss=53.8532, LR=7.1e-02, kappa=1.0 (fixed), |gamma|=0.1765, time=7.1min
Epoch  190: Loss=53.5351, LR=6.8e-02, kappa=1.0 (fixed), |gamma|=0.1843, time=7.5min
Epoch  200: Loss=53.2199, LR=6.6e-02, kappa=1.0 (fixed), |gamma|=0.1912, time=7.9min
Epoch  210: Loss=52.9050, LR=6.3e-02, kappa=1.0 (fixed), |gamma|=0.1971, time=8.3min
Epoch  220: Loss=52.5897, LR=5.9e-02, kappa=1.0 (fixed), |gamma|=0.2020, time=8.7min
Epoch  230: Loss=52.2732, LR=5.6e-02, kappa=1.0 (fixed), |gamma|=0.2050, time=9.1min
Epoch  240: Loss=51.9562, LR=5.3e-02, kappa=1.0 (fixed), |gamma|=0.2066, time=9.5min
Epoch  250: Loss=51.6357, LR=5.0e-02, kappa=1.0 (fixed), |gamma|=0.2070, time=9.9min
Epoch  260: Loss=51.3145, LR=4.7e-02, kappa=1.0 (fixed), |gamma|=0.2061, time=10.3min
Epoch  270: Loss=50.9968, LR=4.4e-02, kappa=1.0 (fixed), |gamma|=0.2048, time=10.7min
Epoch  280: Loss=50.6848, LR=4.1e-02, kappa=1.0 (fixed), |gamma|=0.2029, time=11.1min
Epoch  290: Loss=50.3798, LR=3.8e-02, kappa=1.0 (fixed), |gamma|=0.2007, time=11.5min
Epoch  300: Loss=50.0864, LR=3.5e-02, kappa=1.0 (fixed), |gamma|=0.1980, time=11.9min
Epoch  310: Loss=49.8083, LR=3.2e-02, kappa=1.0 (fixed), |gamma|=0.1955, time=12.3min
Epoch  320: Loss=49.5474, LR=2.9e-02, kappa=1.0 (fixed), |gamma|=0.1929, time=12.7min
Epoch  330: Loss=49.3053, LR=2.6e-02, kappa=1.0 (fixed), |gamma|=0.1905, time=13.1min
Epoch  340: Loss=49.0826, LR=2.4e-02, kappa=1.0 (fixed), |gamma|=0.1884, time=13.5min
Epoch  350: Loss=48.8798, LR=2.1e-02, kappa=1.0 (fixed), |gamma|=0.1865, time=13.9min
Epoch  360: Loss=48.6967, LR=1.9e-02, kappa=1.0 (fixed), |gamma|=0.1848, time=14.3min
Epoch  370: Loss=48.5326, LR=1.6e-02, kappa=1.0 (fixed), |gamma|=0.1831, time=14.7min
Epoch  380: Loss=48.3867, LR=1.4e-02, kappa=1.0 (fixed), |gamma|=0.1817, time=15.1min
Epoch  390: Loss=48.2580, LR=1.2e-02, kappa=1.0 (fixed), |gamma|=0.1805, time=15.5min
Epoch  400: Loss=48.1454, LR=1.0e-02, kappa=1.0 (fixed), |gamma|=0.1795, time=15.9min
Epoch  410: Loss=48.0475, LR=8.5e-03, kappa=1.0 (fixed), |gamma|=0.1787, time=16.3min
Epoch  420: Loss=47.9631, LR=7.0e-03, kappa=1.0 (fixed), |gamma|=0.1780, time=16.7min
Epoch  430: Loss=47.8909, LR=5.6e-03, kappa=1.0 (fixed), |gamma|=0.1774, time=17.1min
Epoch  440: Loss=47.8295, LR=4.4e-03, kappa=1.0 (fixed), |gamma|=0.1770, time=17.5min
Epoch  450: Loss=47.7775, LR=3.3e-03, kappa=1.0 (fixed), |gamma|=0.1767, time=17.9min
Epoch  460: Loss=47.7336, LR=2.5e-03, kappa=1.0 (fixed), |gamma|=0.1764, time=18.3min
Epoch  470: Loss=47.6962, LR=1.8e-03, kappa=1.0 (fixed), |gamma|=0.1762, time=18.7min
Epoch  480: Loss=47.6639, LR=1.4e-03, kappa=1.0 (fixed), |gamma|=0.1761, time=19.1min
Epoch  490: Loss=47.6351, LR=1.1e-03, kappa=1.0 (fixed), |gamma|=0.1760, time=19.4min

Training complete: 500 epochs in 19.8 min
Final loss: 47.6109

Final params: kappa=1.0 (fixed), mean|gamma|=0.1759
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2_nokappa/enrollment_model_REPARAM_NOKAPPA_W0.0001_batch_300000_310000.pt
Batch 31 done in 20.0 min

============================================================
BATCH 32/40: 310000-320000
============================================================

============================================================
REPARAM v2 NO KAPPA: samples 310000 to 320000
Epochs: 500, LR: 0.1, grad_clip: 5.0
kappa = 1.0 (FIXED, not learned)
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 26 diseases
Cluster 1: 24 diseases
Cluster 2: 21 diseases
Cluster 3: 7 diseases
Cluster 4: 8 diseases
Cluster 5: 5 diseases
Cluster 6: 86 diseases
Cluster 7: 9 diseases
Cluster 8: 8 diseases
Cluster 9: 7 diseases
Cluster 10: 25 diseases
Cluster 11: 21 diseases
Cluster 12: 13 diseases
Cluster 13: 10 diseases
Cluster 14: 5 diseases
Cluster 15: 13 diseases
Cluster 16: 11 diseases
Cluster 17: 8 diseases
Cluster 18: 29 diseases
Cluster 19: 12 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping (NO KAPPA)...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=63.3621, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.1014, time=0.0min
Epoch   10: Loss=219.3478, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0708, time=0.4min
Epoch   20: Loss=94.7022, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0724, time=0.8min
Epoch   30: Loss=64.3345, LR=9.9e-02, kappa=1.0 (fixed), |gamma|=0.0782, time=1.2min
Epoch   40: Loss=63.5233, LR=9.8e-02, kappa=1.0 (fixed), |gamma|=0.0821, time=1.6min
Epoch   50: Loss=61.2802, LR=9.7e-02, kappa=1.0 (fixed), |gamma|=0.0883, time=2.0min
Epoch   60: Loss=59.5760, LR=9.6e-02, kappa=1.0 (fixed), |gamma|=0.0944, time=2.4min
Epoch   70: Loss=58.5895, LR=9.5e-02, kappa=1.0 (fixed), |gamma|=0.1012, time=2.8min
Epoch   80: Loss=57.9586, LR=9.4e-02, kappa=1.0 (fixed), |gamma|=0.1076, time=3.2min
Epoch   90: Loss=57.4827, LR=9.2e-02, kappa=1.0 (fixed), |gamma|=0.1139, time=3.6min
Epoch  100: Loss=57.0630, LR=9.0e-02, kappa=1.0 (fixed), |gamma|=0.1181, time=4.0min
Epoch  110: Loss=56.6679, LR=8.8e-02, kappa=1.0 (fixed), |gamma|=0.1239, time=4.4min
Epoch  120: Loss=56.2908, LR=8.6e-02, kappa=1.0 (fixed), |gamma|=0.1301, time=4.8min
Epoch  130: Loss=55.9303, LR=8.4e-02, kappa=1.0 (fixed), |gamma|=0.1363, time=5.1min
Epoch  140: Loss=55.5814, LR=8.2e-02, kappa=1.0 (fixed), |gamma|=0.1426, time=5.6min
Epoch  150: Loss=55.2377, LR=7.9e-02, kappa=1.0 (fixed), |gamma|=0.1491, time=6.0min
Epoch  160: Loss=54.8981, LR=7.7e-02, kappa=1.0 (fixed), |gamma|=0.1552, time=6.4min
Epoch  170: Loss=54.5601, LR=7.4e-02, kappa=1.0 (fixed), |gamma|=0.1609, time=6.8min
Epoch  180: Loss=54.2206, LR=7.1e-02, kappa=1.0 (fixed), |gamma|=0.1662, time=7.2min
Epoch  190: Loss=53.8739, LR=6.8e-02, kappa=1.0 (fixed), |gamma|=0.1705, time=7.7min
Epoch  200: Loss=53.5223, LR=6.6e-02, kappa=1.0 (fixed), |gamma|=0.1730, time=8.1min
Epoch  210: Loss=53.1620, LR=6.3e-02, kappa=1.0 (fixed), |gamma|=0.1743, time=8.5min
Epoch  220: Loss=52.7938, LR=5.9e-02, kappa=1.0 (fixed), |gamma|=0.1747, time=9.0min
Epoch  230: Loss=52.4228, LR=5.6e-02, kappa=1.0 (fixed), |gamma|=0.1736, time=9.4min
Epoch  240: Loss=52.0509, LR=5.3e-02, kappa=1.0 (fixed), |gamma|=0.1718, time=9.8min
Epoch  250: Loss=51.6817, LR=5.0e-02, kappa=1.0 (fixed), |gamma|=0.1694, time=10.3min
Epoch  260: Loss=51.3192, LR=4.7e-02, kappa=1.0 (fixed), |gamma|=0.1667, time=10.7min
Epoch  270: Loss=50.9670, LR=4.4e-02, kappa=1.0 (fixed), |gamma|=0.1643, time=11.1min
Epoch  280: Loss=50.6286, LR=4.1e-02, kappa=1.0 (fixed), |gamma|=0.1618, time=11.6min
Epoch  290: Loss=50.3074, LR=3.8e-02, kappa=1.0 (fixed), |gamma|=0.1594, time=12.0min
Epoch  300: Loss=50.0052, LR=3.5e-02, kappa=1.0 (fixed), |gamma|=0.1572, time=12.4min
Epoch  310: Loss=49.7235, LR=3.2e-02, kappa=1.0 (fixed), |gamma|=0.1552, time=12.8min
Epoch  320: Loss=49.4632, LR=2.9e-02, kappa=1.0 (fixed), |gamma|=0.1534, time=13.2min
Epoch  330: Loss=49.2245, LR=2.6e-02, kappa=1.0 (fixed), |gamma|=0.1517, time=13.6min
Epoch  340: Loss=49.0072, LR=2.4e-02, kappa=1.0 (fixed), |gamma|=0.1503, time=14.0min
Epoch  350: Loss=48.8108, LR=2.1e-02, kappa=1.0 (fixed), |gamma|=0.1490, time=14.4min
Epoch  360: Loss=48.6344, LR=1.9e-02, kappa=1.0 (fixed), |gamma|=0.1479, time=14.8min
Epoch  370: Loss=48.4771, LR=1.6e-02, kappa=1.0 (fixed), |gamma|=0.1469, time=15.2min
Epoch  380: Loss=48.3377, LR=1.4e-02, kappa=1.0 (fixed), |gamma|=0.1461, time=15.6min
Epoch  390: Loss=48.2151, LR=1.2e-02, kappa=1.0 (fixed), |gamma|=0.1453, time=16.0min
Epoch  400: Loss=48.1079, LR=1.0e-02, kappa=1.0 (fixed), |gamma|=0.1446, time=16.4min
Epoch  410: Loss=48.0148, LR=8.5e-03, kappa=1.0 (fixed), |gamma|=0.1440, time=16.8min
Epoch  420: Loss=47.9347, LR=7.0e-03, kappa=1.0 (fixed), |gamma|=0.1435, time=17.2min
Epoch  430: Loss=47.8663, LR=5.6e-03, kappa=1.0 (fixed), |gamma|=0.1432, time=17.6min
Epoch  440: Loss=47.8081, LR=4.4e-03, kappa=1.0 (fixed), |gamma|=0.1430, time=18.0min
Epoch  450: Loss=47.7589, LR=3.3e-03, kappa=1.0 (fixed), |gamma|=0.1428, time=18.4min
Epoch  460: Loss=47.7173, LR=2.5e-03, kappa=1.0 (fixed), |gamma|=0.1426, time=18.8min
Epoch  470: Loss=47.6819, LR=1.8e-03, kappa=1.0 (fixed), |gamma|=0.1425, time=19.2min
Epoch  480: Loss=47.6513, LR=1.4e-03, kappa=1.0 (fixed), |gamma|=0.1424, time=19.6min
Epoch  490: Loss=47.6239, LR=1.1e-03, kappa=1.0 (fixed), |gamma|=0.1424, time=20.0min

Training complete: 500 epochs in 20.3 min
Final loss: 47.6009

Final params: kappa=1.0 (fixed), mean|gamma|=0.1423
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2_nokappa/enrollment_model_REPARAM_NOKAPPA_W0.0001_batch_310000_320000.pt
Batch 32 done in 20.5 min

============================================================
BATCH 33/40: 320000-330000
============================================================

============================================================
REPARAM v2 NO KAPPA: samples 320000 to 330000
Epochs: 500, LR: 0.1, grad_clip: 5.0
kappa = 1.0 (FIXED, not learned)
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 14 diseases
Cluster 1: 14 diseases
Cluster 2: 26 diseases
Cluster 3: 16 diseases
Cluster 4: 5 diseases
Cluster 5: 21 diseases
Cluster 6: 8 diseases
Cluster 7: 7 diseases
Cluster 8: 8 diseases
Cluster 9: 10 diseases
Cluster 10: 26 diseases
Cluster 11: 104 diseases
Cluster 12: 13 diseases
Cluster 13: 6 diseases
Cluster 14: 5 diseases
Cluster 15: 16 diseases
Cluster 16: 11 diseases
Cluster 17: 15 diseases
Cluster 18: 13 diseases
Cluster 19: 10 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping (NO KAPPA)...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=64.3223, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.1006, time=0.0min
Epoch   10: Loss=220.2682, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0670, time=0.5min
Epoch   20: Loss=95.6626, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0671, time=0.9min
Epoch   30: Loss=65.2825, LR=9.9e-02, kappa=1.0 (fixed), |gamma|=0.0726, time=1.3min
Epoch   40: Loss=64.4508, LR=9.8e-02, kappa=1.0 (fixed), |gamma|=0.0768, time=1.7min
Epoch   50: Loss=62.1966, LR=9.7e-02, kappa=1.0 (fixed), |gamma|=0.0840, time=2.1min
Epoch   60: Loss=60.4805, LR=9.6e-02, kappa=1.0 (fixed), |gamma|=0.0917, time=2.5min
Epoch   70: Loss=59.4814, LR=9.5e-02, kappa=1.0 (fixed), |gamma|=0.0998, time=2.9min
Epoch   80: Loss=58.8386, LR=9.4e-02, kappa=1.0 (fixed), |gamma|=0.1068, time=3.3min
Epoch   90: Loss=58.3528, LR=9.2e-02, kappa=1.0 (fixed), |gamma|=0.1127, time=3.7min
Epoch  100: Loss=57.9272, LR=9.0e-02, kappa=1.0 (fixed), |gamma|=0.1187, time=4.1min
Epoch  110: Loss=57.5252, LR=8.8e-02, kappa=1.0 (fixed), |gamma|=0.1244, time=4.5min
Epoch  120: Loss=57.1429, LR=8.6e-02, kappa=1.0 (fixed), |gamma|=0.1303, time=4.9min
Epoch  130: Loss=56.7763, LR=8.4e-02, kappa=1.0 (fixed), |gamma|=0.1358, time=5.3min
Epoch  140: Loss=56.4227, LR=8.2e-02, kappa=1.0 (fixed), |gamma|=0.1426, time=5.6min
Epoch  150: Loss=56.0759, LR=7.9e-02, kappa=1.0 (fixed), |gamma|=0.1498, time=6.0min
Epoch  160: Loss=55.7362, LR=7.7e-02, kappa=1.0 (fixed), |gamma|=0.1575, time=6.4min
Epoch  170: Loss=55.3961, LR=7.4e-02, kappa=1.0 (fixed), |gamma|=0.1660, time=6.8min
Epoch  180: Loss=55.0542, LR=7.1e-02, kappa=1.0 (fixed), |gamma|=0.1744, time=7.2min
Epoch  190: Loss=54.7085, LR=6.8e-02, kappa=1.0 (fixed), |gamma|=0.1811, time=7.6min
Epoch  200: Loss=54.3557, LR=6.6e-02, kappa=1.0 (fixed), |gamma|=0.1843, time=7.9min
Epoch  210: Loss=53.9909, LR=6.3e-02, kappa=1.0 (fixed), |gamma|=0.1882, time=8.3min
Epoch  220: Loss=53.6235, LR=5.9e-02, kappa=1.0 (fixed), |gamma|=0.1928, time=8.7min
Epoch  230: Loss=53.2473, LR=5.6e-02, kappa=1.0 (fixed), |gamma|=0.1965, time=9.1min
Epoch  240: Loss=52.8663, LR=5.3e-02, kappa=1.0 (fixed), |gamma|=0.1989, time=9.5min
Epoch  250: Loss=52.4864, LR=5.0e-02, kappa=1.0 (fixed), |gamma|=0.2005, time=9.9min
Epoch  260: Loss=52.1121, LR=4.7e-02, kappa=1.0 (fixed), |gamma|=0.2012, time=10.2min
Epoch  270: Loss=51.7459, LR=4.4e-02, kappa=1.0 (fixed), |gamma|=0.2011, time=10.6min
Epoch  280: Loss=51.3942, LR=4.1e-02, kappa=1.0 (fixed), |gamma|=0.2007, time=10.9min
Epoch  290: Loss=51.0597, LR=3.8e-02, kappa=1.0 (fixed), |gamma|=0.1999, time=11.3min
Epoch  300: Loss=50.7451, LR=3.5e-02, kappa=1.0 (fixed), |gamma|=0.1991, time=11.6min
Epoch  310: Loss=50.4521, LR=3.2e-02, kappa=1.0 (fixed), |gamma|=0.1982, time=12.0min
Epoch  320: Loss=50.1818, LR=2.9e-02, kappa=1.0 (fixed), |gamma|=0.1974, time=12.3min
Epoch  330: Loss=49.9342, LR=2.6e-02, kappa=1.0 (fixed), |gamma|=0.1965, time=12.7min
Epoch  340: Loss=49.7092, LR=2.4e-02, kappa=1.0 (fixed), |gamma|=0.1958, time=13.0min
Epoch  350: Loss=49.5061, LR=2.1e-02, kappa=1.0 (fixed), |gamma|=0.1951, time=13.4min
Epoch  360: Loss=49.3240, LR=1.9e-02, kappa=1.0 (fixed), |gamma|=0.1945, time=13.7min
Epoch  370: Loss=49.1618, LR=1.6e-02, kappa=1.0 (fixed), |gamma|=0.1940, time=14.1min
Epoch  380: Loss=49.0182, LR=1.4e-02, kappa=1.0 (fixed), |gamma|=0.1936, time=14.4min
Epoch  390: Loss=48.8919, LR=1.2e-02, kappa=1.0 (fixed), |gamma|=0.1932, time=14.8min
Epoch  400: Loss=48.7817, LR=1.0e-02, kappa=1.0 (fixed), |gamma|=0.1929, time=15.1min
Epoch  410: Loss=48.6861, LR=8.5e-03, kappa=1.0 (fixed), |gamma|=0.1926, time=15.5min
Epoch  420: Loss=48.6037, LR=7.0e-03, kappa=1.0 (fixed), |gamma|=0.1924, time=15.8min
Epoch  430: Loss=48.5334, LR=5.6e-03, kappa=1.0 (fixed), |gamma|=0.1922, time=16.2min
Epoch  440: Loss=48.4735, LR=4.4e-03, kappa=1.0 (fixed), |gamma|=0.1920, time=16.5min
Epoch  450: Loss=48.4229, LR=3.3e-03, kappa=1.0 (fixed), |gamma|=0.1919, time=16.9min
Epoch  460: Loss=48.3800, LR=2.5e-03, kappa=1.0 (fixed), |gamma|=0.1918, time=17.2min
Epoch  470: Loss=48.3435, LR=1.8e-03, kappa=1.0 (fixed), |gamma|=0.1918, time=17.6min
Epoch  480: Loss=48.3118, LR=1.4e-03, kappa=1.0 (fixed), |gamma|=0.1917, time=17.9min
Epoch  490: Loss=48.2836, LR=1.1e-03, kappa=1.0 (fixed), |gamma|=0.1917, time=18.3min

Training complete: 500 epochs in 18.6 min
Final loss: 48.2598

Final params: kappa=1.0 (fixed), mean|gamma|=0.1917
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2_nokappa/enrollment_model_REPARAM_NOKAPPA_W0.0001_batch_320000_330000.pt
Batch 33 done in 18.8 min

============================================================
BATCH 34/40: 330000-340000
============================================================

============================================================
REPARAM v2 NO KAPPA: samples 330000 to 340000
Epochs: 500, LR: 0.1, grad_clip: 5.0
kappa = 1.0 (FIXED, not learned)
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 11 diseases
Cluster 1: 12 diseases
Cluster 2: 7 diseases
Cluster 3: 11 diseases
Cluster 4: 9 diseases
Cluster 5: 14 diseases
Cluster 6: 98 diseases
Cluster 7: 14 diseases
Cluster 8: 13 diseases
Cluster 9: 27 diseases
Cluster 10: 17 diseases
Cluster 11: 17 diseases
Cluster 12: 5 diseases
Cluster 13: 7 diseases
Cluster 14: 9 diseases
Cluster 15: 24 diseases
Cluster 16: 22 diseases
Cluster 17: 5 diseases
Cluster 18: 12 diseases
Cluster 19: 14 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping (NO KAPPA)...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=63.9938, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.1009, time=0.0min
Epoch   10: Loss=219.9513, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0667, time=0.4min
Epoch   20: Loss=95.3473, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0638, time=0.8min
Epoch   30: Loss=64.9585, LR=9.9e-02, kappa=1.0 (fixed), |gamma|=0.0652, time=1.2min
Epoch   40: Loss=64.1313, LR=9.8e-02, kappa=1.0 (fixed), |gamma|=0.0685, time=1.5min
Epoch   50: Loss=61.8790, LR=9.7e-02, kappa=1.0 (fixed), |gamma|=0.0754, time=1.9min
Epoch   60: Loss=60.1613, LR=9.6e-02, kappa=1.0 (fixed), |gamma|=0.0827, time=2.3min
Epoch   70: Loss=59.1606, LR=9.5e-02, kappa=1.0 (fixed), |gamma|=0.0907, time=2.6min
Epoch   80: Loss=58.5164, LR=9.4e-02, kappa=1.0 (fixed), |gamma|=0.0991, time=3.0min
Epoch   90: Loss=58.0309, LR=9.2e-02, kappa=1.0 (fixed), |gamma|=0.1067, time=3.3min
Epoch  100: Loss=57.6066, LR=9.0e-02, kappa=1.0 (fixed), |gamma|=0.1136, time=3.7min
Epoch  110: Loss=57.2061, LR=8.8e-02, kappa=1.0 (fixed), |gamma|=0.1208, time=4.1min
Epoch  120: Loss=56.8258, LR=8.6e-02, kappa=1.0 (fixed), |gamma|=0.1283, time=4.4min
Epoch  130: Loss=56.4641, LR=8.4e-02, kappa=1.0 (fixed), |gamma|=0.1357, time=4.8min
Epoch  140: Loss=56.1136, LR=8.2e-02, kappa=1.0 (fixed), |gamma|=0.1437, time=5.1min
Epoch  150: Loss=55.7722, LR=7.9e-02, kappa=1.0 (fixed), |gamma|=0.1514, time=5.5min
Epoch  160: Loss=55.4376, LR=7.7e-02, kappa=1.0 (fixed), |gamma|=0.1593, time=5.8min
Epoch  170: Loss=55.1088, LR=7.4e-02, kappa=1.0 (fixed), |gamma|=0.1674, time=6.2min
Epoch  180: Loss=54.7825, LR=7.1e-02, kappa=1.0 (fixed), |gamma|=0.1751, time=6.5min
Epoch  190: Loss=54.4567, LR=6.8e-02, kappa=1.0 (fixed), |gamma|=0.1820, time=6.8min
Epoch  200: Loss=54.1301, LR=6.6e-02, kappa=1.0 (fixed), |gamma|=0.1877, time=7.2min
Epoch  210: Loss=53.8035, LR=6.3e-02, kappa=1.0 (fixed), |gamma|=0.1924, time=7.5min
Epoch  220: Loss=53.4683, LR=5.9e-02, kappa=1.0 (fixed), |gamma|=0.1957, time=7.9min
Epoch  230: Loss=53.1293, LR=5.6e-02, kappa=1.0 (fixed), |gamma|=0.1976, time=8.2min
Epoch  240: Loss=52.7876, LR=5.3e-02, kappa=1.0 (fixed), |gamma|=0.1983, time=8.6min
Epoch  250: Loss=52.4440, LR=5.0e-02, kappa=1.0 (fixed), |gamma|=0.1975, time=8.9min
Epoch  260: Loss=52.1036, LR=4.7e-02, kappa=1.0 (fixed), |gamma|=0.1955, time=9.3min
Epoch  270: Loss=51.7690, LR=4.4e-02, kappa=1.0 (fixed), |gamma|=0.1930, time=9.6min
Epoch  280: Loss=51.4433, LR=4.1e-02, kappa=1.0 (fixed), |gamma|=0.1900, time=9.9min
Epoch  290: Loss=51.1308, LR=3.8e-02, kappa=1.0 (fixed), |gamma|=0.1871, time=10.3min
Epoch  300: Loss=50.8339, LR=3.5e-02, kappa=1.0 (fixed), |gamma|=0.1842, time=10.6min
Epoch  310: Loss=50.5547, LR=3.2e-02, kappa=1.0 (fixed), |gamma|=0.1814, time=11.0min
Epoch  320: Loss=50.2946, LR=2.9e-02, kappa=1.0 (fixed), |gamma|=0.1788, time=11.3min
Epoch  330: Loss=50.0545, LR=2.6e-02, kappa=1.0 (fixed), |gamma|=0.1765, time=11.7min
Epoch  340: Loss=49.8334, LR=2.4e-02, kappa=1.0 (fixed), |gamma|=0.1744, time=12.0min
Epoch  350: Loss=49.6330, LR=2.1e-02, kappa=1.0 (fixed), |gamma|=0.1727, time=12.3min
Epoch  360: Loss=49.4516, LR=1.9e-02, kappa=1.0 (fixed), |gamma|=0.1715, time=12.7min
Epoch  370: Loss=49.2889, LR=1.6e-02, kappa=1.0 (fixed), |gamma|=0.1703, time=13.0min
Epoch  380: Loss=49.1441, LR=1.4e-02, kappa=1.0 (fixed), |gamma|=0.1693, time=13.4min
Epoch  390: Loss=49.0163, LR=1.2e-02, kappa=1.0 (fixed), |gamma|=0.1684, time=13.7min
Epoch  400: Loss=48.9042, LR=1.0e-02, kappa=1.0 (fixed), |gamma|=0.1677, time=14.0min
Epoch  410: Loss=48.8068, LR=8.5e-03, kappa=1.0 (fixed), |gamma|=0.1671, time=14.4min
Epoch  420: Loss=48.7228, LR=7.0e-03, kappa=1.0 (fixed), |gamma|=0.1666, time=14.7min
Epoch  430: Loss=48.6508, LR=5.6e-03, kappa=1.0 (fixed), |gamma|=0.1662, time=15.1min
Epoch  440: Loss=48.5896, LR=4.4e-03, kappa=1.0 (fixed), |gamma|=0.1658, time=15.4min
Epoch  450: Loss=48.5377, LR=3.3e-03, kappa=1.0 (fixed), |gamma|=0.1656, time=15.8min
Epoch  460: Loss=48.4938, LR=2.5e-03, kappa=1.0 (fixed), |gamma|=0.1654, time=16.1min
Epoch  470: Loss=48.4565, LR=1.8e-03, kappa=1.0 (fixed), |gamma|=0.1652, time=16.5min
Epoch  480: Loss=48.4241, LR=1.4e-03, kappa=1.0 (fixed), |gamma|=0.1651, time=16.8min
Epoch  490: Loss=48.3952, LR=1.1e-03, kappa=1.0 (fixed), |gamma|=0.1651, time=17.2min

Training complete: 500 epochs in 17.5 min
Final loss: 48.3709

Final params: kappa=1.0 (fixed), mean|gamma|=0.1650
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2_nokappa/enrollment_model_REPARAM_NOKAPPA_W0.0001_batch_330000_340000.pt
Batch 34 done in 17.6 min

============================================================
BATCH 35/40: 340000-350000
============================================================

============================================================
REPARAM v2 NO KAPPA: samples 340000 to 350000
Epochs: 500, LR: 0.1, grad_clip: 5.0
kappa = 1.0 (FIXED, not learned)
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 10 diseases
Cluster 1: 15 diseases
Cluster 2: 11 diseases
Cluster 3: 18 diseases
Cluster 4: 9 diseases
Cluster 5: 5 diseases
Cluster 6: 5 diseases
Cluster 7: 17 diseases
Cluster 8: 7 diseases
Cluster 9: 8 diseases
Cluster 10: 28 diseases
Cluster 11: 30 diseases
Cluster 12: 12 diseases
Cluster 13: 99 diseases
Cluster 14: 24 diseases
Cluster 15: 18 diseases
Cluster 16: 9 diseases
Cluster 17: 13 diseases
Cluster 18: 4 diseases
Cluster 19: 6 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping (NO KAPPA)...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=63.3389, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.1007, time=0.0min
Epoch   10: Loss=219.4221, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0649, time=0.4min
Epoch   20: Loss=94.7426, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0666, time=0.8min
Epoch   30: Loss=64.3523, LR=9.9e-02, kappa=1.0 (fixed), |gamma|=0.0680, time=1.1min
Epoch   40: Loss=63.5516, LR=9.8e-02, kappa=1.0 (fixed), |gamma|=0.0714, time=1.5min
Epoch   50: Loss=61.3212, LR=9.7e-02, kappa=1.0 (fixed), |gamma|=0.0768, time=1.8min
Epoch   60: Loss=59.6236, LR=9.6e-02, kappa=1.0 (fixed), |gamma|=0.0837, time=2.2min
Epoch   70: Loss=58.6427, LR=9.5e-02, kappa=1.0 (fixed), |gamma|=0.0905, time=2.5min
Epoch   80: Loss=58.0182, LR=9.4e-02, kappa=1.0 (fixed), |gamma|=0.0971, time=2.9min
Epoch   90: Loss=57.5511, LR=9.2e-02, kappa=1.0 (fixed), |gamma|=0.1021, time=3.2min
Epoch  100: Loss=57.1430, LR=9.0e-02, kappa=1.0 (fixed), |gamma|=0.1071, time=3.5min
Epoch  110: Loss=56.7543, LR=8.8e-02, kappa=1.0 (fixed), |gamma|=0.1126, time=3.9min
Epoch  120: Loss=56.3844, LR=8.6e-02, kappa=1.0 (fixed), |gamma|=0.1197, time=4.2min
Epoch  130: Loss=56.0293, LR=8.4e-02, kappa=1.0 (fixed), |gamma|=0.1277, time=4.6min
Epoch  140: Loss=55.6857, LR=8.2e-02, kappa=1.0 (fixed), |gamma|=0.1352, time=4.9min
Epoch  150: Loss=55.3478, LR=7.9e-02, kappa=1.0 (fixed), |gamma|=0.1434, time=5.2min
Epoch  160: Loss=55.0154, LR=7.7e-02, kappa=1.0 (fixed), |gamma|=0.1508, time=5.6min
Epoch  170: Loss=54.6853, LR=7.4e-02, kappa=1.0 (fixed), |gamma|=0.1581, time=5.9min
Epoch  180: Loss=54.3531, LR=7.1e-02, kappa=1.0 (fixed), |gamma|=0.1649, time=6.3min
Epoch  190: Loss=54.0166, LR=6.8e-02, kappa=1.0 (fixed), |gamma|=0.1709, time=6.6min
Epoch  200: Loss=53.6798, LR=6.6e-02, kappa=1.0 (fixed), |gamma|=0.1757, time=6.9min
Epoch  210: Loss=53.3359, LR=6.3e-02, kappa=1.0 (fixed), |gamma|=0.1794, time=7.3min
Epoch  220: Loss=52.9870, LR=5.9e-02, kappa=1.0 (fixed), |gamma|=0.1811, time=7.6min
Epoch  230: Loss=52.6349, LR=5.6e-02, kappa=1.0 (fixed), |gamma|=0.1818, time=7.9min
Epoch  240: Loss=52.2803, LR=5.3e-02, kappa=1.0 (fixed), |gamma|=0.1810, time=8.3min
Epoch  250: Loss=51.9249, LR=5.0e-02, kappa=1.0 (fixed), |gamma|=0.1796, time=8.6min
Epoch  260: Loss=51.5760, LR=4.7e-02, kappa=1.0 (fixed), |gamma|=0.1778, time=9.0min
Epoch  270: Loss=51.2278, LR=4.4e-02, kappa=1.0 (fixed), |gamma|=0.1756, time=9.3min
Epoch  280: Loss=50.8928, LR=4.1e-02, kappa=1.0 (fixed), |gamma|=0.1733, time=9.7min
Epoch  290: Loss=50.5717, LR=3.8e-02, kappa=1.0 (fixed), |gamma|=0.1708, time=10.0min
Epoch  300: Loss=50.2673, LR=3.5e-02, kappa=1.0 (fixed), |gamma|=0.1684, time=10.4min
Epoch  310: Loss=49.9819, LR=3.2e-02, kappa=1.0 (fixed), |gamma|=0.1661, time=10.7min
Epoch  320: Loss=49.7171, LR=2.9e-02, kappa=1.0 (fixed), |gamma|=0.1639, time=11.1min
Epoch  330: Loss=49.4734, LR=2.6e-02, kappa=1.0 (fixed), |gamma|=0.1619, time=11.4min
Epoch  340: Loss=49.2510, LR=2.4e-02, kappa=1.0 (fixed), |gamma|=0.1601, time=11.7min
Epoch  350: Loss=49.0496, LR=2.1e-02, kappa=1.0 (fixed), |gamma|=0.1585, time=12.1min
Epoch  360: Loss=48.8685, LR=1.9e-02, kappa=1.0 (fixed), |gamma|=0.1571, time=12.4min
Epoch  370: Loss=48.7069, LR=1.6e-02, kappa=1.0 (fixed), |gamma|=0.1559, time=12.7min
Epoch  380: Loss=48.5635, LR=1.4e-02, kappa=1.0 (fixed), |gamma|=0.1548, time=13.1min
Epoch  390: Loss=48.4373, LR=1.2e-02, kappa=1.0 (fixed), |gamma|=0.1539, time=13.4min
Epoch  400: Loss=48.3270, LR=1.0e-02, kappa=1.0 (fixed), |gamma|=0.1531, time=13.8min
Epoch  410: Loss=48.2312, LR=8.5e-03, kappa=1.0 (fixed), |gamma|=0.1524, time=14.1min
Epoch  420: Loss=48.1486, LR=7.0e-03, kappa=1.0 (fixed), |gamma|=0.1519, time=14.4min
Epoch  430: Loss=48.0780, LR=5.6e-03, kappa=1.0 (fixed), |gamma|=0.1514, time=14.8min
Epoch  440: Loss=48.0180, LR=4.4e-03, kappa=1.0 (fixed), |gamma|=0.1511, time=15.1min
Epoch  450: Loss=47.9672, LR=3.3e-03, kappa=1.0 (fixed), |gamma|=0.1508, time=15.4min
Epoch  460: Loss=47.9241, LR=2.5e-03, kappa=1.0 (fixed), |gamma|=0.1506, time=15.8min
Epoch  470: Loss=47.8875, LR=1.8e-03, kappa=1.0 (fixed), |gamma|=0.1505, time=16.1min
Epoch  480: Loss=47.8557, LR=1.4e-03, kappa=1.0 (fixed), |gamma|=0.1503, time=16.4min
Epoch  490: Loss=47.8273, LR=1.1e-03, kappa=1.0 (fixed), |gamma|=0.1503, time=16.8min

Training complete: 500 epochs in 17.1 min
Final loss: 47.8035

Final params: kappa=1.0 (fixed), mean|gamma|=0.1502
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2_nokappa/enrollment_model_REPARAM_NOKAPPA_W0.0001_batch_340000_350000.pt
Batch 35 done in 17.3 min

============================================================
BATCH 36/40: 350000-360000
============================================================

============================================================
REPARAM v2 NO KAPPA: samples 350000 to 360000
Epochs: 500, LR: 0.1, grad_clip: 5.0
kappa = 1.0 (FIXED, not learned)
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 5 diseases
Cluster 1: 9 diseases
Cluster 2: 14 diseases
Cluster 3: 14 diseases
Cluster 4: 17 diseases
Cluster 5: 113 diseases
Cluster 6: 9 diseases
Cluster 7: 13 diseases
Cluster 8: 8 diseases
Cluster 9: 13 diseases
Cluster 10: 12 diseases
Cluster 11: 20 diseases
Cluster 12: 29 diseases
Cluster 13: 9 diseases
Cluster 14: 18 diseases
Cluster 15: 5 diseases
Cluster 16: 15 diseases
Cluster 17: 12 diseases
Cluster 18: 7 diseases
Cluster 19: 6 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping (NO KAPPA)...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=63.6311, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.1007, time=0.0min
Epoch   10: Loss=219.6591, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0710, time=0.4min
Epoch   20: Loss=95.0237, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0734, time=0.8min
Epoch   30: Loss=64.6327, LR=9.9e-02, kappa=1.0 (fixed), |gamma|=0.0760, time=1.2min
Epoch   40: Loss=63.8228, LR=9.8e-02, kappa=1.0 (fixed), |gamma|=0.0821, time=1.5min
Epoch   50: Loss=61.5862, LR=9.7e-02, kappa=1.0 (fixed), |gamma|=0.0897, time=1.9min
Epoch   60: Loss=59.8835, LR=9.6e-02, kappa=1.0 (fixed), |gamma|=0.0968, time=2.2min
Epoch   70: Loss=58.8961, LR=9.5e-02, kappa=1.0 (fixed), |gamma|=0.1028, time=2.6min
Epoch   80: Loss=58.2630, LR=9.4e-02, kappa=1.0 (fixed), |gamma|=0.1078, time=3.0min
Epoch   90: Loss=57.7860, LR=9.2e-02, kappa=1.0 (fixed), |gamma|=0.1133, time=3.3min
Epoch  100: Loss=57.3677, LR=9.0e-02, kappa=1.0 (fixed), |gamma|=0.1186, time=3.7min
Epoch  110: Loss=56.9715, LR=8.8e-02, kappa=1.0 (fixed), |gamma|=0.1240, time=4.0min
Epoch  120: Loss=56.5937, LR=8.6e-02, kappa=1.0 (fixed), |gamma|=0.1296, time=4.4min
Epoch  130: Loss=56.2320, LR=8.4e-02, kappa=1.0 (fixed), |gamma|=0.1350, time=4.7min
Epoch  140: Loss=55.8811, LR=8.2e-02, kappa=1.0 (fixed), |gamma|=0.1408, time=5.0min
Epoch  150: Loss=55.5360, LR=7.9e-02, kappa=1.0 (fixed), |gamma|=0.1459, time=5.4min
Epoch  160: Loss=55.1948, LR=7.7e-02, kappa=1.0 (fixed), |gamma|=0.1504, time=5.7min
Epoch  170: Loss=54.8562, LR=7.4e-02, kappa=1.0 (fixed), |gamma|=0.1543, time=6.1min
Epoch  180: Loss=54.5130, LR=7.1e-02, kappa=1.0 (fixed), |gamma|=0.1568, time=6.4min
Epoch  190: Loss=54.1655, LR=6.8e-02, kappa=1.0 (fixed), |gamma|=0.1585, time=6.8min
Epoch  200: Loss=53.8119, LR=6.6e-02, kappa=1.0 (fixed), |gamma|=0.1593, time=7.1min
Epoch  210: Loss=53.4508, LR=6.3e-02, kappa=1.0 (fixed), |gamma|=0.1592, time=7.5min
Epoch  220: Loss=53.0860, LR=5.9e-02, kappa=1.0 (fixed), |gamma|=0.1581, time=7.8min
Epoch  230: Loss=52.7192, LR=5.6e-02, kappa=1.0 (fixed), |gamma|=0.1566, time=8.2min
Epoch  240: Loss=52.3496, LR=5.3e-02, kappa=1.0 (fixed), |gamma|=0.1549, time=8.5min
Epoch  250: Loss=51.9834, LR=5.0e-02, kappa=1.0 (fixed), |gamma|=0.1530, time=8.8min
Epoch  260: Loss=51.6238, LR=4.7e-02, kappa=1.0 (fixed), |gamma|=0.1511, time=9.2min
Epoch  270: Loss=51.2747, LR=4.4e-02, kappa=1.0 (fixed), |gamma|=0.1492, time=9.5min
Epoch  280: Loss=50.9405, LR=4.1e-02, kappa=1.0 (fixed), |gamma|=0.1474, time=9.9min
Epoch  290: Loss=50.6229, LR=3.8e-02, kappa=1.0 (fixed), |gamma|=0.1458, time=10.2min
Epoch  300: Loss=50.3247, LR=3.5e-02, kappa=1.0 (fixed), |gamma|=0.1443, time=10.5min
Epoch  310: Loss=50.0474, LR=3.2e-02, kappa=1.0 (fixed), |gamma|=0.1430, time=10.9min
Epoch  320: Loss=49.7914, LR=2.9e-02, kappa=1.0 (fixed), |gamma|=0.1418, time=11.2min
Epoch  330: Loss=49.5570, LR=2.6e-02, kappa=1.0 (fixed), |gamma|=0.1408, time=11.6min
Epoch  340: Loss=49.3438, LR=2.4e-02, kappa=1.0 (fixed), |gamma|=0.1399, time=11.9min
Epoch  350: Loss=49.1511, LR=2.1e-02, kappa=1.0 (fixed), |gamma|=0.1391, time=12.2min
Epoch  360: Loss=48.9781, LR=1.9e-02, kappa=1.0 (fixed), |gamma|=0.1384, time=12.6min
Epoch  370: Loss=48.8238, LR=1.6e-02, kappa=1.0 (fixed), |gamma|=0.1379, time=12.9min
Epoch  380: Loss=48.6870, LR=1.4e-02, kappa=1.0 (fixed), |gamma|=0.1374, time=13.2min
Epoch  390: Loss=48.5666, LR=1.2e-02, kappa=1.0 (fixed), |gamma|=0.1369, time=13.6min
Epoch  400: Loss=48.4614, LR=1.0e-02, kappa=1.0 (fixed), |gamma|=0.1366, time=14.0min
Epoch  410: Loss=48.3700, LR=8.5e-03, kappa=1.0 (fixed), |gamma|=0.1362, time=14.3min
Epoch  420: Loss=48.2913, LR=7.0e-03, kappa=1.0 (fixed), |gamma|=0.1360, time=14.7min
Epoch  430: Loss=48.2239, LR=5.6e-03, kappa=1.0 (fixed), |gamma|=0.1358, time=15.1min
Epoch  440: Loss=48.1666, LR=4.4e-03, kappa=1.0 (fixed), |gamma|=0.1356, time=15.4min
Epoch  450: Loss=48.1181, LR=3.3e-03, kappa=1.0 (fixed), |gamma|=0.1355, time=15.8min
Epoch  460: Loss=48.0770, LR=2.5e-03, kappa=1.0 (fixed), |gamma|=0.1354, time=16.2min
Epoch  470: Loss=48.0419, LR=1.8e-03, kappa=1.0 (fixed), |gamma|=0.1354, time=16.5min
Epoch  480: Loss=48.0116, LR=1.4e-03, kappa=1.0 (fixed), |gamma|=0.1353, time=16.9min
Epoch  490: Loss=47.9845, LR=1.1e-03, kappa=1.0 (fixed), |gamma|=0.1353, time=17.3min

Training complete: 500 epochs in 17.6 min
Final loss: 47.9616

Final params: kappa=1.0 (fixed), mean|gamma|=0.1353
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2_nokappa/enrollment_model_REPARAM_NOKAPPA_W0.0001_batch_350000_360000.pt
Batch 36 done in 17.8 min

============================================================
BATCH 37/40: 360000-370000
============================================================

============================================================
REPARAM v2 NO KAPPA: samples 360000 to 370000
Epochs: 500, LR: 0.1, grad_clip: 5.0
kappa = 1.0 (FIXED, not learned)
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 30 diseases
Cluster 1: 14 diseases
Cluster 2: 15 diseases
Cluster 3: 13 diseases
Cluster 4: 8 diseases
Cluster 5: 5 diseases
Cluster 6: 9 diseases
Cluster 7: 11 diseases
Cluster 8: 30 diseases
Cluster 9: 86 diseases
Cluster 10: 8 diseases
Cluster 11: 21 diseases
Cluster 12: 11 diseases
Cluster 13: 15 diseases
Cluster 14: 25 diseases
Cluster 15: 12 diseases
Cluster 16: 5 diseases
Cluster 17: 5 diseases
Cluster 18: 16 diseases
Cluster 19: 9 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping (NO KAPPA)...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=62.5406, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.1008, time=0.0min
Epoch   10: Loss=218.5984, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0704, time=0.4min
Epoch   20: Loss=93.9571, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0707, time=0.8min
Epoch   30: Loss=63.5969, LR=9.9e-02, kappa=1.0 (fixed), |gamma|=0.0723, time=1.1min
Epoch   40: Loss=62.7979, LR=9.8e-02, kappa=1.0 (fixed), |gamma|=0.0789, time=1.5min
Epoch   50: Loss=60.5703, LR=9.7e-02, kappa=1.0 (fixed), |gamma|=0.0885, time=1.8min
Epoch   60: Loss=58.8786, LR=9.6e-02, kappa=1.0 (fixed), |gamma|=0.0977, time=2.2min
Epoch   70: Loss=57.9032, LR=9.5e-02, kappa=1.0 (fixed), |gamma|=0.1057, time=2.6min
Epoch   80: Loss=57.2825, LR=9.4e-02, kappa=1.0 (fixed), |gamma|=0.1126, time=2.9min
Epoch   90: Loss=56.8173, LR=9.2e-02, kappa=1.0 (fixed), |gamma|=0.1189, time=3.3min
Epoch  100: Loss=56.4097, LR=9.0e-02, kappa=1.0 (fixed), |gamma|=0.1250, time=3.6min
Epoch  110: Loss=56.0233, LR=8.8e-02, kappa=1.0 (fixed), |gamma|=0.1316, time=3.9min
Epoch  120: Loss=55.6532, LR=8.6e-02, kappa=1.0 (fixed), |gamma|=0.1384, time=4.2min
Epoch  130: Loss=55.2992, LR=8.4e-02, kappa=1.0 (fixed), |gamma|=0.1463, time=4.6min
Epoch  140: Loss=54.9561, LR=8.2e-02, kappa=1.0 (fixed), |gamma|=0.1539, time=4.9min
Epoch  150: Loss=54.6204, LR=7.9e-02, kappa=1.0 (fixed), |gamma|=0.1617, time=5.2min
Epoch  160: Loss=54.2911, LR=7.7e-02, kappa=1.0 (fixed), |gamma|=0.1699, time=5.6min
Epoch  170: Loss=53.9659, LR=7.4e-02, kappa=1.0 (fixed), |gamma|=0.1785, time=5.9min
Epoch  180: Loss=53.6437, LR=7.1e-02, kappa=1.0 (fixed), |gamma|=0.1866, time=6.2min
Epoch  190: Loss=53.3255, LR=6.8e-02, kappa=1.0 (fixed), |gamma|=0.1944, time=6.5min
Epoch  200: Loss=52.9997, LR=6.6e-02, kappa=1.0 (fixed), |gamma|=0.2009, time=6.9min
Epoch  210: Loss=52.6739, LR=6.3e-02, kappa=1.0 (fixed), |gamma|=0.2061, time=7.2min
Epoch  220: Loss=52.3430, LR=5.9e-02, kappa=1.0 (fixed), |gamma|=0.2092, time=7.5min
Epoch  230: Loss=52.0077, LR=5.6e-02, kappa=1.0 (fixed), |gamma|=0.2099, time=7.9min
Epoch  240: Loss=51.6674, LR=5.3e-02, kappa=1.0 (fixed), |gamma|=0.2099, time=8.2min
Epoch  250: Loss=51.3238, LR=5.0e-02, kappa=1.0 (fixed), |gamma|=0.2092, time=8.5min
Epoch  260: Loss=50.9813, LR=4.7e-02, kappa=1.0 (fixed), |gamma|=0.2077, time=8.9min
Epoch  270: Loss=50.6427, LR=4.4e-02, kappa=1.0 (fixed), |gamma|=0.2057, time=9.2min
Epoch  280: Loss=50.3129, LR=4.1e-02, kappa=1.0 (fixed), |gamma|=0.2032, time=9.6min
Epoch  290: Loss=49.9946, LR=3.8e-02, kappa=1.0 (fixed), |gamma|=0.2005, time=9.9min
Epoch  300: Loss=49.6916, LR=3.5e-02, kappa=1.0 (fixed), |gamma|=0.1976, time=10.3min
Epoch  310: Loss=49.4067, LR=3.2e-02, kappa=1.0 (fixed), |gamma|=0.1947, time=10.7min
Epoch  320: Loss=49.1417, LR=2.9e-02, kappa=1.0 (fixed), |gamma|=0.1919, time=11.0min
Epoch  330: Loss=48.8976, LR=2.6e-02, kappa=1.0 (fixed), |gamma|=0.1893, time=11.4min
Epoch  340: Loss=48.6749, LR=2.4e-02, kappa=1.0 (fixed), |gamma|=0.1870, time=11.7min
Epoch  350: Loss=48.4732, LR=2.1e-02, kappa=1.0 (fixed), |gamma|=0.1849, time=12.1min
Epoch  360: Loss=48.2919, LR=1.9e-02, kappa=1.0 (fixed), |gamma|=0.1831, time=12.4min
Epoch  370: Loss=48.1302, LR=1.6e-02, kappa=1.0 (fixed), |gamma|=0.1815, time=12.8min
Epoch  380: Loss=47.9869, LR=1.4e-02, kappa=1.0 (fixed), |gamma|=0.1801, time=13.1min
Epoch  390: Loss=47.8608, LR=1.2e-02, kappa=1.0 (fixed), |gamma|=0.1790, time=13.5min
Epoch  400: Loss=47.7506, LR=1.0e-02, kappa=1.0 (fixed), |gamma|=0.1780, time=13.8min
Epoch  410: Loss=47.6550, LR=8.5e-03, kappa=1.0 (fixed), |gamma|=0.1771, time=14.2min
Epoch  420: Loss=47.5728, LR=7.0e-03, kappa=1.0 (fixed), |gamma|=0.1764, time=14.5min
Epoch  430: Loss=47.5025, LR=5.6e-03, kappa=1.0 (fixed), |gamma|=0.1759, time=14.9min
Epoch  440: Loss=47.4428, LR=4.4e-03, kappa=1.0 (fixed), |gamma|=0.1755, time=15.2min
Epoch  450: Loss=47.3923, LR=3.3e-03, kappa=1.0 (fixed), |gamma|=0.1752, time=15.6min
Epoch  460: Loss=47.3496, LR=2.5e-03, kappa=1.0 (fixed), |gamma|=0.1750, time=16.0min
Epoch  470: Loss=47.3133, LR=1.8e-03, kappa=1.0 (fixed), |gamma|=0.1748, time=16.3min
Epoch  480: Loss=47.2819, LR=1.4e-03, kappa=1.0 (fixed), |gamma|=0.1747, time=16.7min
Epoch  490: Loss=47.2540, LR=1.1e-03, kappa=1.0 (fixed), |gamma|=0.1746, time=17.0min

Training complete: 500 epochs in 17.3 min
Final loss: 47.2305

Final params: kappa=1.0 (fixed), mean|gamma|=0.1746
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2_nokappa/enrollment_model_REPARAM_NOKAPPA_W0.0001_batch_360000_370000.pt
Batch 37 done in 17.5 min

============================================================
BATCH 38/40: 370000-380000
============================================================

============================================================
REPARAM v2 NO KAPPA: samples 370000 to 380000
Epochs: 500, LR: 0.1, grad_clip: 5.0
kappa = 1.0 (FIXED, not learned)
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 19 diseases
Cluster 1: 13 diseases
Cluster 2: 6 diseases
Cluster 3: 22 diseases
Cluster 4: 15 diseases
Cluster 5: 9 diseases
Cluster 6: 13 diseases
Cluster 7: 5 diseases
Cluster 8: 5 diseases
Cluster 9: 19 diseases
Cluster 10: 7 diseases
Cluster 11: 25 diseases
Cluster 12: 19 diseases
Cluster 13: 15 diseases
Cluster 14: 14 diseases
Cluster 15: 10 diseases
Cluster 16: 10 diseases
Cluster 17: 5 diseases
Cluster 18: 14 diseases
Cluster 19: 103 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping (NO KAPPA)...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=63.7181, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.1008, time=0.0min
Epoch   10: Loss=219.8252, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0668, time=0.4min
Epoch   20: Loss=95.1076, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0630, time=0.7min
Epoch   30: Loss=64.7049, LR=9.9e-02, kappa=1.0 (fixed), |gamma|=0.0653, time=1.1min
Epoch   40: Loss=63.8960, LR=9.8e-02, kappa=1.0 (fixed), |gamma|=0.0690, time=1.4min
Epoch   50: Loss=61.6493, LR=9.7e-02, kappa=1.0 (fixed), |gamma|=0.0759, time=1.8min
Epoch   60: Loss=59.9353, LR=9.6e-02, kappa=1.0 (fixed), |gamma|=0.0826, time=2.1min
Epoch   70: Loss=58.9381, LR=9.5e-02, kappa=1.0 (fixed), |gamma|=0.0900, time=2.5min
Epoch   80: Loss=58.2975, LR=9.4e-02, kappa=1.0 (fixed), |gamma|=0.0983, time=2.8min
Epoch   90: Loss=57.8149, LR=9.2e-02, kappa=1.0 (fixed), |gamma|=0.1070, time=3.2min
Epoch  100: Loss=57.3916, LR=9.0e-02, kappa=1.0 (fixed), |gamma|=0.1160, time=3.5min
Epoch  110: Loss=56.9901, LR=8.8e-02, kappa=1.0 (fixed), |gamma|=0.1255, time=3.9min
Epoch  120: Loss=56.6069, LR=8.6e-02, kappa=1.0 (fixed), |gamma|=0.1336, time=4.2min
Epoch  130: Loss=56.2404, LR=8.4e-02, kappa=1.0 (fixed), |gamma|=0.1410, time=4.6min
Epoch  140: Loss=55.8851, LR=8.2e-02, kappa=1.0 (fixed), |gamma|=0.1480, time=4.9min
Epoch  150: Loss=55.5385, LR=7.9e-02, kappa=1.0 (fixed), |gamma|=0.1559, time=5.3min
Epoch  160: Loss=55.1964, LR=7.7e-02, kappa=1.0 (fixed), |gamma|=0.1639, time=5.6min
Epoch  170: Loss=54.8594, LR=7.4e-02, kappa=1.0 (fixed), |gamma|=0.1726, time=6.0min
Epoch  180: Loss=54.5232, LR=7.1e-02, kappa=1.0 (fixed), |gamma|=0.1817, time=6.3min
Epoch  190: Loss=54.1970, LR=6.8e-02, kappa=1.0 (fixed), |gamma|=0.1909, time=6.7min
Epoch  200: Loss=53.8592, LR=6.6e-02, kappa=1.0 (fixed), |gamma|=0.1984, time=7.0min
Epoch  210: Loss=53.5236, LR=6.3e-02, kappa=1.0 (fixed), |gamma|=0.2052, time=7.3min
Epoch  220: Loss=53.1912, LR=5.9e-02, kappa=1.0 (fixed), |gamma|=0.2106, time=7.7min
Epoch  230: Loss=52.8591, LR=5.6e-02, kappa=1.0 (fixed), |gamma|=0.2148, time=8.0min
Epoch  240: Loss=52.5276, LR=5.3e-02, kappa=1.0 (fixed), |gamma|=0.2177, time=8.4min
Epoch  250: Loss=52.1984, LR=5.0e-02, kappa=1.0 (fixed), |gamma|=0.2203, time=8.8min
Epoch  260: Loss=51.8687, LR=4.7e-02, kappa=1.0 (fixed), |gamma|=0.2231, time=9.1min
Epoch  270: Loss=51.5464, LR=4.4e-02, kappa=1.0 (fixed), |gamma|=0.2254, time=9.4min
Epoch  280: Loss=51.2325, LR=4.1e-02, kappa=1.0 (fixed), |gamma|=0.2276, time=9.8min
Epoch  290: Loss=50.9301, LR=3.8e-02, kappa=1.0 (fixed), |gamma|=0.2269, time=10.1min
Epoch  300: Loss=50.6417, LR=3.5e-02, kappa=1.0 (fixed), |gamma|=0.2249, time=10.5min
Epoch  310: Loss=50.3696, LR=3.2e-02, kappa=1.0 (fixed), |gamma|=0.2231, time=10.8min
Epoch  320: Loss=50.1154, LR=2.9e-02, kappa=1.0 (fixed), |gamma|=0.2218, time=11.2min
Epoch  330: Loss=49.8801, LR=2.6e-02, kappa=1.0 (fixed), |gamma|=0.2209, time=11.5min
Epoch  340: Loss=49.6641, LR=2.4e-02, kappa=1.0 (fixed), |gamma|=0.2200, time=11.9min
Epoch  350: Loss=49.4673, LR=2.1e-02, kappa=1.0 (fixed), |gamma|=0.2195, time=12.2min
Epoch  360: Loss=49.2895, LR=1.9e-02, kappa=1.0 (fixed), |gamma|=0.2192, time=12.6min
Epoch  370: Loss=49.1300, LR=1.6e-02, kappa=1.0 (fixed), |gamma|=0.2192, time=12.9min
Epoch  380: Loss=48.9879, LR=1.4e-02, kappa=1.0 (fixed), |gamma|=0.2192, time=13.3min
Epoch  390: Loss=48.8622, LR=1.2e-02, kappa=1.0 (fixed), |gamma|=0.2193, time=13.7min
Epoch  400: Loss=48.7519, LR=1.0e-02, kappa=1.0 (fixed), |gamma|=0.2194, time=14.0min
Epoch  410: Loss=48.6558, LR=8.5e-03, kappa=1.0 (fixed), |gamma|=0.2195, time=14.4min
Epoch  420: Loss=48.5719, LR=7.0e-03, kappa=1.0 (fixed), |gamma|=0.2196, time=14.7min
Epoch  430: Loss=48.4972, LR=5.6e-03, kappa=1.0 (fixed), |gamma|=0.2198, time=15.1min
Epoch  440: Loss=48.4356, LR=4.4e-03, kappa=1.0 (fixed), |gamma|=0.2195, time=15.4min
Epoch  450: Loss=48.3832, LR=3.3e-03, kappa=1.0 (fixed), |gamma|=0.2194, time=15.8min
Epoch  460: Loss=48.3389, LR=2.5e-03, kappa=1.0 (fixed), |gamma|=0.2194, time=16.1min
Epoch  470: Loss=48.3012, LR=1.8e-03, kappa=1.0 (fixed), |gamma|=0.2193, time=16.5min
Epoch  480: Loss=48.2685, LR=1.4e-03, kappa=1.0 (fixed), |gamma|=0.2193, time=16.8min
Epoch  490: Loss=48.2393, LR=1.1e-03, kappa=1.0 (fixed), |gamma|=0.2194, time=17.2min

Training complete: 500 epochs in 17.5 min
Final loss: 48.2148

Final params: kappa=1.0 (fixed), mean|gamma|=0.2194
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2_nokappa/enrollment_model_REPARAM_NOKAPPA_W0.0001_batch_370000_380000.pt
Batch 38 done in 17.7 min

============================================================
BATCH 39/40: 380000-390000
============================================================

============================================================
REPARAM v2 NO KAPPA: samples 380000 to 390000
Epochs: 500, LR: 0.1, grad_clip: 5.0
kappa = 1.0 (FIXED, not learned)
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 15 diseases
Cluster 1: 23 diseases
Cluster 2: 10 diseases
Cluster 3: 6 diseases
Cluster 4: 14 diseases
Cluster 5: 12 diseases
Cluster 6: 5 diseases
Cluster 7: 10 diseases
Cluster 8: 12 diseases
Cluster 9: 8 diseases
Cluster 10: 41 diseases
Cluster 11: 11 diseases
Cluster 12: 9 diseases
Cluster 13: 25 diseases
Cluster 14: 8 diseases
Cluster 15: 14 diseases
Cluster 16: 25 diseases
Cluster 17: 80 diseases
Cluster 18: 11 diseases
Cluster 19: 9 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping (NO KAPPA)...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=63.5337, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.1009, time=0.0min
Epoch   10: Loss=219.5727, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0696, time=0.4min
Epoch   20: Loss=94.9189, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0689, time=0.7min
Epoch   30: Loss=64.5352, LR=9.9e-02, kappa=1.0 (fixed), |gamma|=0.0711, time=1.1min
Epoch   40: Loss=63.7290, LR=9.8e-02, kappa=1.0 (fixed), |gamma|=0.0750, time=1.4min
Epoch   50: Loss=61.4890, LR=9.7e-02, kappa=1.0 (fixed), |gamma|=0.0822, time=1.8min
Epoch   60: Loss=59.7821, LR=9.6e-02, kappa=1.0 (fixed), |gamma|=0.0901, time=2.1min
Epoch   70: Loss=58.7916, LR=9.5e-02, kappa=1.0 (fixed), |gamma|=0.0980, time=2.5min
Epoch   80: Loss=58.1557, LR=9.4e-02, kappa=1.0 (fixed), |gamma|=0.1052, time=2.8min
Epoch   90: Loss=57.6777, LR=9.2e-02, kappa=1.0 (fixed), |gamma|=0.1117, time=3.2min
Epoch  100: Loss=57.2580, LR=9.0e-02, kappa=1.0 (fixed), |gamma|=0.1169, time=3.5min
Epoch  110: Loss=56.8611, LR=8.8e-02, kappa=1.0 (fixed), |gamma|=0.1228, time=3.9min
Epoch  120: Loss=56.4831, LR=8.6e-02, kappa=1.0 (fixed), |gamma|=0.1288, time=4.2min
Epoch  130: Loss=56.1216, LR=8.4e-02, kappa=1.0 (fixed), |gamma|=0.1354, time=4.6min
Epoch  140: Loss=55.7710, LR=8.2e-02, kappa=1.0 (fixed), |gamma|=0.1424, time=4.9min
Epoch  150: Loss=55.4278, LR=7.9e-02, kappa=1.0 (fixed), |gamma|=0.1494, time=5.3min
Epoch  160: Loss=55.0908, LR=7.7e-02, kappa=1.0 (fixed), |gamma|=0.1563, time=5.6min
Epoch  170: Loss=54.7603, LR=7.4e-02, kappa=1.0 (fixed), |gamma|=0.1631, time=5.9min
Epoch  180: Loss=54.4266, LR=7.1e-02, kappa=1.0 (fixed), |gamma|=0.1682, time=6.3min
Epoch  190: Loss=54.0936, LR=6.8e-02, kappa=1.0 (fixed), |gamma|=0.1724, time=6.6min
Epoch  200: Loss=53.7578, LR=6.6e-02, kappa=1.0 (fixed), |gamma|=0.1753, time=7.0min
Epoch  210: Loss=53.4147, LR=6.3e-02, kappa=1.0 (fixed), |gamma|=0.1764, time=7.3min
Epoch  220: Loss=53.0665, LR=5.9e-02, kappa=1.0 (fixed), |gamma|=0.1764, time=7.7min
Epoch  230: Loss=52.7128, LR=5.6e-02, kappa=1.0 (fixed), |gamma|=0.1752, time=8.0min
Epoch  240: Loss=52.3536, LR=5.3e-02, kappa=1.0 (fixed), |gamma|=0.1734, time=8.4min
Epoch  250: Loss=51.9957, LR=5.0e-02, kappa=1.0 (fixed), |gamma|=0.1711, time=8.7min
Epoch  260: Loss=51.6396, LR=4.7e-02, kappa=1.0 (fixed), |gamma|=0.1689, time=9.1min
Epoch  270: Loss=51.2917, LR=4.4e-02, kappa=1.0 (fixed), |gamma|=0.1667, time=9.4min
Epoch  280: Loss=50.9554, LR=4.1e-02, kappa=1.0 (fixed), |gamma|=0.1644, time=9.8min
Epoch  290: Loss=50.6345, LR=3.8e-02, kappa=1.0 (fixed), |gamma|=0.1623, time=10.1min
Epoch  300: Loss=50.3320, LR=3.5e-02, kappa=1.0 (fixed), |gamma|=0.1604, time=10.5min
Epoch  310: Loss=50.0490, LR=3.2e-02, kappa=1.0 (fixed), |gamma|=0.1587, time=10.8min
Epoch  320: Loss=49.7874, LR=2.9e-02, kappa=1.0 (fixed), |gamma|=0.1572, time=11.2min
Epoch  330: Loss=49.5472, LR=2.6e-02, kappa=1.0 (fixed), |gamma|=0.1558, time=11.5min
Epoch  340: Loss=49.3285, LR=2.4e-02, kappa=1.0 (fixed), |gamma|=0.1546, time=11.9min
Epoch  350: Loss=49.1307, LR=2.1e-02, kappa=1.0 (fixed), |gamma|=0.1534, time=12.2min
Epoch  360: Loss=48.9529, LR=1.9e-02, kappa=1.0 (fixed), |gamma|=0.1524, time=12.5min
Epoch  370: Loss=48.7944, LR=1.6e-02, kappa=1.0 (fixed), |gamma|=0.1516, time=12.9min
Epoch  380: Loss=48.6539, LR=1.4e-02, kappa=1.0 (fixed), |gamma|=0.1509, time=13.2min
Epoch  390: Loss=48.5302, LR=1.2e-02, kappa=1.0 (fixed), |gamma|=0.1504, time=13.6min
Epoch  400: Loss=48.4220, LR=1.0e-02, kappa=1.0 (fixed), |gamma|=0.1499, time=13.9min
Epoch  410: Loss=48.3282, LR=8.5e-03, kappa=1.0 (fixed), |gamma|=0.1495, time=14.3min
Epoch  420: Loss=48.2474, LR=7.0e-03, kappa=1.0 (fixed), |gamma|=0.1492, time=14.6min
Epoch  430: Loss=48.1782, LR=5.6e-03, kappa=1.0 (fixed), |gamma|=0.1489, time=15.0min
Epoch  440: Loss=48.1194, LR=4.4e-03, kappa=1.0 (fixed), |gamma|=0.1488, time=15.3min
Epoch  450: Loss=48.0696, LR=3.3e-03, kappa=1.0 (fixed), |gamma|=0.1486, time=15.6min
Epoch  460: Loss=48.0274, LR=2.5e-03, kappa=1.0 (fixed), |gamma|=0.1485, time=16.0min
Epoch  470: Loss=47.9915, LR=1.8e-03, kappa=1.0 (fixed), |gamma|=0.1485, time=16.3min
Epoch  480: Loss=47.9603, LR=1.4e-03, kappa=1.0 (fixed), |gamma|=0.1484, time=16.7min
Epoch  490: Loss=47.9326, LR=1.1e-03, kappa=1.0 (fixed), |gamma|=0.1484, time=17.0min

Training complete: 500 epochs in 17.3 min
Final loss: 47.9091

Final params: kappa=1.0 (fixed), mean|gamma|=0.1483
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2_nokappa/enrollment_model_REPARAM_NOKAPPA_W0.0001_batch_380000_390000.pt
Batch 39 done in 17.5 min

============================================================
BATCH 40/40: 390000-400000
============================================================

============================================================
REPARAM v2 NO KAPPA: samples 390000 to 400000
Epochs: 500, LR: 0.1, grad_clip: 5.0
kappa = 1.0 (FIXED, not learned)
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 12 diseases
Cluster 1: 18 diseases
Cluster 2: 40 diseases
Cluster 3: 15 diseases
Cluster 4: 7 diseases
Cluster 5: 9 diseases
Cluster 6: 92 diseases
Cluster 7: 14 diseases
Cluster 8: 5 diseases
Cluster 9: 7 diseases
Cluster 10: 9 diseases
Cluster 11: 5 diseases
Cluster 12: 27 diseases
Cluster 13: 17 diseases
Cluster 14: 14 diseases
Cluster 15: 8 diseases
Cluster 16: 7 diseases
Cluster 17: 11 diseases
Cluster 18: 21 diseases
Cluster 19: 10 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping (NO KAPPA)...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=62.6000, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.1009, time=0.0min
Epoch   10: Loss=218.6879, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0695, time=0.4min
Epoch   20: Loss=94.0234, LR=1.0e-01, kappa=1.0 (fixed), |gamma|=0.0686, time=0.7min
Epoch   30: Loss=63.6520, LR=9.9e-02, kappa=1.0 (fixed), |gamma|=0.0692, time=1.0min
Epoch   40: Loss=62.8631, LR=9.8e-02, kappa=1.0 (fixed), |gamma|=0.0731, time=1.3min
Epoch   50: Loss=60.6389, LR=9.7e-02, kappa=1.0 (fixed), |gamma|=0.0799, time=1.7min
Epoch   60: Loss=58.9492, LR=9.6e-02, kappa=1.0 (fixed), |gamma|=0.0860, time=2.0min
Epoch   70: Loss=57.9732, LR=9.5e-02, kappa=1.0 (fixed), |gamma|=0.0919, time=2.3min
Epoch   80: Loss=57.3516, LR=9.4e-02, kappa=1.0 (fixed), |gamma|=0.0989, time=2.7min
Epoch   90: Loss=56.8856, LR=9.2e-02, kappa=1.0 (fixed), |gamma|=0.1060, time=3.0min
Epoch  100: Loss=56.4779, LR=9.0e-02, kappa=1.0 (fixed), |gamma|=0.1130, time=3.4min
Epoch  110: Loss=56.0914, LR=8.8e-02, kappa=1.0 (fixed), |gamma|=0.1194, time=3.7min
Epoch  120: Loss=55.7221, LR=8.6e-02, kappa=1.0 (fixed), |gamma|=0.1254, time=4.0min
Epoch  130: Loss=55.3684, LR=8.4e-02, kappa=1.0 (fixed), |gamma|=0.1314, time=4.4min
Epoch  140: Loss=55.0244, LR=8.2e-02, kappa=1.0 (fixed), |gamma|=0.1372, time=4.7min
Epoch  150: Loss=54.6845, LR=7.9e-02, kappa=1.0 (fixed), |gamma|=0.1428, time=5.1min
Epoch  160: Loss=54.3505, LR=7.7e-02, kappa=1.0 (fixed), |gamma|=0.1492, time=5.4min
Epoch  170: Loss=54.0125, LR=7.4e-02, kappa=1.0 (fixed), |gamma|=0.1549, time=5.8min
Epoch  180: Loss=53.6697, LR=7.1e-02, kappa=1.0 (fixed), |gamma|=0.1606, time=6.1min
Epoch  190: Loss=53.3243, LR=6.8e-02, kappa=1.0 (fixed), |gamma|=0.1657, time=6.5min
Epoch  200: Loss=52.9665, LR=6.6e-02, kappa=1.0 (fixed), |gamma|=0.1694, time=6.8min
Epoch  210: Loss=52.6028, LR=6.3e-02, kappa=1.0 (fixed), |gamma|=0.1717, time=7.2min
Epoch  220: Loss=52.2285, LR=5.9e-02, kappa=1.0 (fixed), |gamma|=0.1725, time=7.6min
Epoch  230: Loss=51.8487, LR=5.6e-02, kappa=1.0 (fixed), |gamma|=0.1719, time=8.0min
Epoch  240: Loss=51.4649, LR=5.3e-02, kappa=1.0 (fixed), |gamma|=0.1702, time=8.3min
Epoch  250: Loss=51.0828, LR=5.0e-02, kappa=1.0 (fixed), |gamma|=0.1678, time=8.7min
Epoch  260: Loss=50.7054, LR=4.7e-02, kappa=1.0 (fixed), |gamma|=0.1650, time=9.1min
Epoch  270: Loss=50.3372, LR=4.4e-02, kappa=1.0 (fixed), |gamma|=0.1620, time=9.4min
Epoch  280: Loss=49.9832, LR=4.1e-02, kappa=1.0 (fixed), |gamma|=0.1591, time=9.8min
Epoch  290: Loss=49.6467, LR=3.8e-02, kappa=1.0 (fixed), |gamma|=0.1563, time=10.1min
Epoch  300: Loss=49.3299, LR=3.5e-02, kappa=1.0 (fixed), |gamma|=0.1539, time=10.5min
Epoch  310: Loss=49.0345, LR=3.2e-02, kappa=1.0 (fixed), |gamma|=0.1515, time=10.9min
Epoch  320: Loss=48.7617, LR=2.9e-02, kappa=1.0 (fixed), |gamma|=0.1492, time=11.2min
Epoch  330: Loss=48.5117, LR=2.6e-02, kappa=1.0 (fixed), |gamma|=0.1473, time=11.6min
Epoch  340: Loss=48.2844, LR=2.4e-02, kappa=1.0 (fixed), |gamma|=0.1454, time=11.9min
Epoch  350: Loss=48.0793, LR=2.1e-02, kappa=1.0 (fixed), |gamma|=0.1439, time=12.3min
Epoch  360: Loss=47.8955, LR=1.9e-02, kappa=1.0 (fixed), |gamma|=0.1425, time=12.6min
Epoch  370: Loss=47.7318, LR=1.6e-02, kappa=1.0 (fixed), |gamma|=0.1412, time=13.0min
Epoch  380: Loss=47.5871, LR=1.4e-02, kappa=1.0 (fixed), |gamma|=0.1401, time=13.3min
Epoch  390: Loss=47.4600, LR=1.2e-02, kappa=1.0 (fixed), |gamma|=0.1393, time=13.6min
Epoch  400: Loss=47.3492, LR=1.0e-02, kappa=1.0 (fixed), |gamma|=0.1385, time=14.0min
Epoch  410: Loss=47.2532, LR=8.5e-03, kappa=1.0 (fixed), |gamma|=0.1379, time=14.3min
Epoch  420: Loss=47.1706, LR=7.0e-03, kappa=1.0 (fixed), |gamma|=0.1373, time=14.7min
Epoch  430: Loss=47.1002, LR=5.6e-03, kappa=1.0 (fixed), |gamma|=0.1369, time=15.0min
Epoch  440: Loss=47.0405, LR=4.4e-03, kappa=1.0 (fixed), |gamma|=0.1365, time=15.4min
Epoch  450: Loss=46.9900, LR=3.3e-03, kappa=1.0 (fixed), |gamma|=0.1362, time=15.8min
Epoch  460: Loss=46.9474, LR=2.5e-03, kappa=1.0 (fixed), |gamma|=0.1360, time=16.1min
Epoch  470: Loss=46.9111, LR=1.8e-03, kappa=1.0 (fixed), |gamma|=0.1359, time=16.5min
Epoch  480: Loss=46.8799, LR=1.4e-03, kappa=1.0 (fixed), |gamma|=0.1357, time=16.9min
Epoch  490: Loss=46.8520, LR=1.1e-03, kappa=1.0 (fixed), |gamma|=0.1356, time=17.2min

Training complete: 500 epochs in 17.5 min
Final loss: 46.8285

Final params: kappa=1.0 (fixed), mean|gamma|=0.1356
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2_nokappa/enrollment_model_REPARAM_NOKAPPA_W0.0001_batch_390000_400000.pt
Batch 40 done in 17.7 min

All 40 batches complete in 755 min
