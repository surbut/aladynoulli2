======================================================================
REPARAM v2: Training all 40 batches (400k patients)
Settings: 500 epochs, cosine annealing, grad_clip=5.0, patience=75
Estimated time: ~10 hours
Resuming from batch 1
======================================================================


======================================================================
BATCH 2/40: samples 10000-20000
Elapsed: 0 min | Est remaining: ~585 min (9.8 hrs)
======================================================================

============================================================
REPARAM v2 Training: samples 10000 to 20000
Epochs: 500, LR: 0.1, grad_clip: 5.0
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 25 diseases
Cluster 1: 25 diseases
Cluster 2: 7 diseases
Cluster 3: 23 diseases
Cluster 4: 18 diseases
Cluster 5: 98 diseases
Cluster 6: 8 diseases
Cluster 7: 9 diseases
Cluster 8: 9 diseases
Cluster 9: 5 diseases
Cluster 10: 11 diseases
Cluster 11: 16 diseases
Cluster 12: 9 diseases
Cluster 13: 11 diseases
Cluster 14: 5 diseases
Cluster 15: 12 diseases
Cluster 16: 25 diseases
Cluster 17: 7 diseases
Cluster 18: 16 diseases
Cluster 19: 9 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=63.3284, LR=1.0e-01, kappa=1.100, |gamma|=0.1007, time=0.0min
Epoch   10: Loss=139.5329, LR=1.0e-01, kappa=2.021, |gamma|=0.0987, time=0.2min
Epoch   20: Loss=100.3205, LR=1.0e-01, kappa=2.654, |gamma|=0.1372, time=0.5min
Epoch   30: Loss=72.2074, LR=9.9e-02, kappa=2.980, |gamma|=0.1557, time=0.7min
Epoch   40: Loss=62.4740, LR=9.8e-02, kappa=3.095, |gamma|=0.1564, time=0.9min
Epoch   50: Loss=59.5212, LR=9.7e-02, kappa=3.107, |gamma|=0.1550, time=1.1min
Epoch   60: Loss=58.2366, LR=9.6e-02, kappa=3.091, |gamma|=0.1538, time=1.3min
Epoch   70: Loss=57.4731, LR=9.5e-02, kappa=3.084, |gamma|=0.1524, time=1.6min
Epoch   80: Loss=57.0173, LR=9.4e-02, kappa=3.095, |gamma|=0.1509, time=1.8min
Epoch   90: Loss=56.6677, LR=9.2e-02, kappa=3.122, |gamma|=0.1501, time=2.0min
Epoch  100: Loss=56.2699, LR=9.0e-02, kappa=3.156, |gamma|=0.1500, time=2.3min
Epoch  110: Loss=55.8490, LR=8.8e-02, kappa=3.198, |gamma|=0.1500, time=2.5min
Epoch  120: Loss=55.3842, LR=8.6e-02, kappa=3.252, |gamma|=0.1498, time=2.7min
Epoch  130: Loss=54.8711, LR=8.4e-02, kappa=3.322, |gamma|=0.1483, time=3.0min
Epoch  140: Loss=54.2964, LR=8.2e-02, kappa=3.412, |gamma|=0.1466, time=3.2min
Epoch  150: Loss=53.6744, LR=7.9e-02, kappa=3.520, |gamma|=0.1435, time=3.5min
Epoch  160: Loss=53.0139, LR=7.7e-02, kappa=3.646, |gamma|=0.1419, time=3.8min
Epoch  170: Loss=52.3275, LR=7.4e-02, kappa=3.783, |gamma|=0.1397, time=4.1min
Epoch  180: Loss=51.6628, LR=7.1e-02, kappa=3.926, |gamma|=0.1380, time=4.4min
Epoch  190: Loss=50.9857, LR=6.8e-02, kappa=4.068, |gamma|=0.1352, time=4.7min
Epoch  200: Loss=50.3469, LR=6.6e-02, kappa=4.206, |gamma|=0.1328, time=5.0min
Epoch  210: Loss=49.7526, LR=6.3e-02, kappa=4.337, |gamma|=0.1316, time=5.3min
Epoch  220: Loss=49.2071, LR=5.9e-02, kappa=4.459, |gamma|=0.1307, time=5.6min
Epoch  230: Loss=48.7090, LR=5.6e-02, kappa=4.572, |gamma|=0.1300, time=5.9min
Epoch  240: Loss=48.2571, LR=5.3e-02, kappa=4.674, |gamma|=0.1294, time=6.2min
Epoch  250: Loss=47.8485, LR=5.0e-02, kappa=4.767, |gamma|=0.1288, time=6.5min
Epoch  260: Loss=47.4800, LR=4.7e-02, kappa=4.850, |gamma|=0.1282, time=6.7min
Epoch  270: Loss=47.1479, LR=4.4e-02, kappa=4.925, |gamma|=0.1276, time=7.0min
Epoch  280: Loss=46.8490, LR=4.1e-02, kappa=4.992, |gamma|=0.1269, time=7.3min
Epoch  290: Loss=46.5800, LR=3.8e-02, kappa=5.051, |gamma|=0.1263, time=7.6min
Epoch  300: Loss=46.3380, LR=3.5e-02, kappa=5.105, |gamma|=0.1257, time=7.9min
Epoch  310: Loss=46.1206, LR=3.2e-02, kappa=5.153, |gamma|=0.1252, time=8.2min
Epoch  320: Loss=45.9255, LR=2.9e-02, kappa=5.195, |gamma|=0.1247, time=8.5min
Epoch  330: Loss=45.7506, LR=2.6e-02, kappa=5.233, |gamma|=0.1244, time=8.7min
Epoch  340: Loss=45.5940, LR=2.4e-02, kappa=5.266, |gamma|=0.1243, time=9.0min
Epoch  350: Loss=45.4518, LR=2.1e-02, kappa=5.296, |gamma|=0.1247, time=9.3min
Epoch  360: Loss=45.3273, LR=1.9e-02, kappa=5.321, |gamma|=0.1247, time=9.6min
Epoch  370: Loss=45.2177, LR=1.6e-02, kappa=5.344, |gamma|=0.1244, time=9.9min
Epoch  380: Loss=45.1216, LR=1.4e-02, kappa=5.363, |gamma|=0.1241, time=10.2min
Epoch  390: Loss=45.0378, LR=1.2e-02, kappa=5.380, |gamma|=0.1239, time=10.4min
Epoch  400: Loss=44.9652, LR=1.0e-02, kappa=5.394, |gamma|=0.1237, time=10.7min
Epoch  410: Loss=44.9028, LR=8.5e-03, kappa=5.406, |gamma|=0.1235, time=11.0min
Epoch  420: Loss=44.8499, LR=7.0e-03, kappa=5.416, |gamma|=0.1234, time=11.3min
Epoch  430: Loss=44.8054, LR=5.6e-03, kappa=5.424, |gamma|=0.1233, time=11.6min
Epoch  440: Loss=44.7682, LR=4.4e-03, kappa=5.430, |gamma|=0.1232, time=11.8min
Epoch  450: Loss=44.7375, LR=3.3e-03, kappa=5.435, |gamma|=0.1231, time=12.1min
Epoch  460: Loss=44.7122, LR=2.5e-03, kappa=5.439, |gamma|=0.1230, time=12.4min
Epoch  470: Loss=44.6913, LR=1.8e-03, kappa=5.441, |gamma|=0.1229, time=12.7min
Epoch  480: Loss=44.6738, LR=1.4e-03, kappa=5.443, |gamma|=0.1229, time=13.0min
Epoch  490: Loss=44.6586, LR=1.1e-03, kappa=5.445, |gamma|=0.1228, time=13.2min

Training complete: 500 epochs in 13.5 min
Final loss: 44.6460

Final params: kappa=5.4462, mean|gamma|=0.1228
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2/enrollment_model_REPARAM_W0.0001_batch_10000_20000.pt

Batch 2 DONE in 13.6 min (1/39 completed)

======================================================================
BATCH 3/40: samples 20000-30000
Elapsed: 14 min | Est remaining: ~570 min (9.5 hrs)
======================================================================

============================================================
REPARAM v2 Training: samples 20000 to 30000
Epochs: 500, LR: 0.1, grad_clip: 5.0
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 12 diseases
Cluster 1: 29 diseases
Cluster 2: 11 diseases
Cluster 3: 12 diseases
Cluster 4: 16 diseases
Cluster 5: 7 diseases
Cluster 6: 10 diseases
Cluster 7: 6 diseases
Cluster 8: 96 diseases
Cluster 9: 14 diseases
Cluster 10: 8 diseases
Cluster 11: 7 diseases
Cluster 12: 16 diseases
Cluster 13: 36 diseases
Cluster 14: 6 diseases
Cluster 15: 8 diseases
Cluster 16: 11 diseases
Cluster 17: 27 diseases
Cluster 18: 11 diseases
Cluster 19: 5 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=62.4698, LR=1.0e-01, kappa=1.100, |gamma|=0.1007, time=0.0min
Epoch   10: Loss=141.5353, LR=1.0e-01, kappa=2.019, |gamma|=0.0989, time=0.3min
Epoch   20: Loss=100.2910, LR=1.0e-01, kappa=2.648, |gamma|=0.1387, time=0.6min
Epoch   30: Loss=71.2403, LR=9.9e-02, kappa=2.975, |gamma|=0.1609, time=0.9min
Epoch   40: Loss=61.5472, LR=9.8e-02, kappa=3.097, |gamma|=0.1635, time=1.2min
Epoch   50: Loss=58.6959, LR=9.7e-02, kappa=3.115, |gamma|=0.1634, time=1.5min
Epoch   60: Loss=57.4677, LR=9.6e-02, kappa=3.106, |gamma|=0.1638, time=1.8min
Epoch   70: Loss=56.7124, LR=9.5e-02, kappa=3.104, |gamma|=0.1649, time=2.1min
Epoch   80: Loss=56.2393, LR=9.4e-02, kappa=3.119, |gamma|=0.1657, time=2.4min
Epoch   90: Loss=55.8785, LR=9.2e-02, kappa=3.146, |gamma|=0.1651, time=2.7min
Epoch  100: Loss=55.4792, LR=9.0e-02, kappa=3.183, |gamma|=0.1645, time=3.0min
Epoch  110: Loss=55.0434, LR=8.8e-02, kappa=3.233, |gamma|=0.1637, time=3.3min
Epoch  120: Loss=54.5643, LR=8.6e-02, kappa=3.298, |gamma|=0.1619, time=3.5min
Epoch  130: Loss=54.0201, LR=8.4e-02, kappa=3.384, |gamma|=0.1604, time=3.8min
Epoch  140: Loss=53.4229, LR=8.2e-02, kappa=3.493, |gamma|=0.1580, time=4.1min
Epoch  150: Loss=52.7497, LR=7.9e-02, kappa=3.625, |gamma|=0.1557, time=4.4min
Epoch  160: Loss=52.0369, LR=7.7e-02, kappa=3.776, |gamma|=0.1540, time=4.7min
Epoch  170: Loss=51.3002, LR=7.4e-02, kappa=3.940, |gamma|=0.1520, time=4.9min
Epoch  180: Loss=50.5634, LR=7.1e-02, kappa=4.107, |gamma|=0.1504, time=5.2min
Epoch  190: Loss=49.8520, LR=6.8e-02, kappa=4.273, |gamma|=0.1481, time=5.5min
Epoch  200: Loss=49.1821, LR=6.6e-02, kappa=4.432, |gamma|=0.1450, time=5.8min
Epoch  210: Loss=48.5642, LR=6.3e-02, kappa=4.580, |gamma|=0.1424, time=6.1min
Epoch  220: Loss=48.0018, LR=5.9e-02, kappa=4.715, |gamma|=0.1400, time=6.4min
Epoch  230: Loss=47.4925, LR=5.6e-02, kappa=4.838, |gamma|=0.1372, time=6.6min
Epoch  240: Loss=47.0387, LR=5.3e-02, kappa=4.948, |gamma|=0.1351, time=6.9min
Epoch  250: Loss=46.6341, LR=5.0e-02, kappa=5.045, |gamma|=0.1332, time=7.2min
Epoch  260: Loss=46.2734, LR=4.7e-02, kappa=5.131, |gamma|=0.1316, time=7.5min
Epoch  270: Loss=45.9518, LR=4.4e-02, kappa=5.208, |gamma|=0.1302, time=7.7min
Epoch  280: Loss=45.6648, LR=4.1e-02, kappa=5.275, |gamma|=0.1288, time=8.0min
Epoch  290: Loss=45.4086, LR=3.8e-02, kappa=5.335, |gamma|=0.1277, time=8.3min
Epoch  300: Loss=45.1798, LR=3.5e-02, kappa=5.388, |gamma|=0.1266, time=8.6min
Epoch  310: Loss=44.9755, LR=3.2e-02, kappa=5.434, |gamma|=0.1257, time=8.9min
Epoch  320: Loss=44.7933, LR=2.9e-02, kappa=5.475, |gamma|=0.1249, time=9.1min
Epoch  330: Loss=44.6310, LR=2.6e-02, kappa=5.512, |gamma|=0.1242, time=9.5min
Epoch  340: Loss=44.4867, LR=2.4e-02, kappa=5.544, |gamma|=0.1238, time=9.8min
Epoch  350: Loss=44.3588, LR=2.1e-02, kappa=5.572, |gamma|=0.1235, time=10.1min
Epoch  360: Loss=44.2459, LR=1.9e-02, kappa=5.596, |gamma|=0.1233, time=10.3min
Epoch  370: Loss=44.1465, LR=1.6e-02, kappa=5.617, |gamma|=0.1231, time=10.6min
Epoch  380: Loss=44.0596, LR=1.4e-02, kappa=5.635, |gamma|=0.1229, time=10.9min
Epoch  390: Loss=43.9840, LR=1.2e-02, kappa=5.651, |gamma|=0.1227, time=11.2min
Epoch  400: Loss=43.9187, LR=1.0e-02, kappa=5.664, |gamma|=0.1225, time=11.5min
Epoch  410: Loss=43.8628, LR=8.5e-03, kappa=5.675, |gamma|=0.1224, time=11.8min
Epoch  420: Loss=43.8153, LR=7.0e-03, kappa=5.684, |gamma|=0.1222, time=12.0min
Epoch  430: Loss=43.7753, LR=5.6e-03, kappa=5.692, |gamma|=0.1221, time=12.3min
Epoch  440: Loss=43.7420, LR=4.4e-03, kappa=5.698, |gamma|=0.1220, time=12.6min
Epoch  450: Loss=43.7144, LR=3.3e-03, kappa=5.702, |gamma|=0.1219, time=12.9min
Epoch  460: Loss=43.6917, LR=2.5e-03, kappa=5.705, |gamma|=0.1219, time=13.2min
Epoch  470: Loss=43.6729, LR=1.8e-03, kappa=5.708, |gamma|=0.1218, time=13.4min
Epoch  480: Loss=43.6572, LR=1.4e-03, kappa=5.710, |gamma|=0.1218, time=13.7min
Epoch  490: Loss=43.6435, LR=1.1e-03, kappa=5.711, |gamma|=0.1217, time=14.0min

Training complete: 500 epochs in 14.2 min
Final loss: 43.6322

Final params: kappa=5.7123, mean|gamma|=0.1217
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2/enrollment_model_REPARAM_W0.0001_batch_20000_30000.pt

Batch 3 DONE in 14.4 min (2/39 completed)

======================================================================
BATCH 4/40: samples 30000-40000
Elapsed: 28 min | Est remaining: ~555 min (9.2 hrs)
======================================================================

============================================================
REPARAM v2 Training: samples 30000 to 40000
Epochs: 500, LR: 0.1, grad_clip: 5.0
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 28 diseases
Cluster 1: 11 diseases
Cluster 2: 13 diseases
Cluster 3: 11 diseases
Cluster 4: 18 diseases
Cluster 5: 6 diseases
Cluster 6: 9 diseases
Cluster 7: 8 diseases
Cluster 8: 16 diseases
Cluster 9: 9 diseases
Cluster 10: 6 diseases
Cluster 11: 26 diseases
Cluster 12: 14 diseases
Cluster 13: 18 diseases
Cluster 14: 22 diseases
Cluster 15: 12 diseases
Cluster 16: 31 diseases
Cluster 17: 4 diseases
Cluster 18: 17 diseases
Cluster 19: 69 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=64.1886, LR=1.0e-01, kappa=1.100, |gamma|=0.1010, time=0.0min
Epoch   10: Loss=138.1514, LR=1.0e-01, kappa=2.024, |gamma|=0.0970, time=0.3min
Epoch   20: Loss=100.4309, LR=1.0e-01, kappa=2.664, |gamma|=0.1320, time=0.6min
Epoch   30: Loss=73.0397, LR=9.9e-02, kappa=3.001, |gamma|=0.1513, time=0.9min
Epoch   40: Loss=63.2884, LR=9.8e-02, kappa=3.130, |gamma|=0.1586, time=1.1min
Epoch   50: Loss=60.2516, LR=9.7e-02, kappa=3.152, |gamma|=0.1609, time=1.4min
Epoch   60: Loss=58.9155, LR=9.6e-02, kappa=3.144, |gamma|=0.1602, time=1.7min
Epoch   70: Loss=58.1363, LR=9.5e-02, kappa=3.142, |gamma|=0.1610, time=2.0min
Epoch   80: Loss=57.6799, LR=9.4e-02, kappa=3.156, |gamma|=0.1622, time=2.3min
Epoch   90: Loss=57.3089, LR=9.2e-02, kappa=3.183, |gamma|=0.1632, time=2.6min
Epoch  100: Loss=56.8892, LR=9.0e-02, kappa=3.220, |gamma|=0.1638, time=2.9min
Epoch  110: Loss=56.4259, LR=8.8e-02, kappa=3.269, |gamma|=0.1626, time=3.1min
Epoch  120: Loss=55.9141, LR=8.6e-02, kappa=3.334, |gamma|=0.1605, time=3.4min
Epoch  130: Loss=55.3405, LR=8.4e-02, kappa=3.417, |gamma|=0.1591, time=3.8min
Epoch  140: Loss=54.7123, LR=8.2e-02, kappa=3.518, |gamma|=0.1573, time=4.0min
Epoch  150: Loss=54.0668, LR=7.9e-02, kappa=3.634, |gamma|=0.1562, time=4.3min
Epoch  160: Loss=53.3692, LR=7.7e-02, kappa=3.760, |gamma|=0.1548, time=4.6min
Epoch  170: Loss=52.6673, LR=7.4e-02, kappa=3.891, |gamma|=0.1538, time=4.9min
Epoch  180: Loss=51.9819, LR=7.1e-02, kappa=4.026, |gamma|=0.1521, time=5.2min
Epoch  190: Loss=51.3235, LR=6.8e-02, kappa=4.163, |gamma|=0.1518, time=5.4min
Epoch  200: Loss=50.7050, LR=6.6e-02, kappa=4.295, |gamma|=0.1517, time=5.7min
Epoch  210: Loss=50.1329, LR=6.3e-02, kappa=4.419, |gamma|=0.1512, time=6.0min
Epoch  220: Loss=49.6126, LR=5.9e-02, kappa=4.533, |gamma|=0.1510, time=6.4min
Epoch  230: Loss=49.1372, LR=5.6e-02, kappa=4.637, |gamma|=0.1506, time=6.7min
Epoch  240: Loss=48.7071, LR=5.3e-02, kappa=4.731, |gamma|=0.1501, time=7.0min
Epoch  250: Loss=48.3206, LR=5.0e-02, kappa=4.815, |gamma|=0.1497, time=7.3min
Epoch  260: Loss=47.9732, LR=4.7e-02, kappa=4.890, |gamma|=0.1492, time=7.6min
Epoch  270: Loss=47.6612, LR=4.4e-02, kappa=4.958, |gamma|=0.1487, time=7.9min
Epoch  280: Loss=47.3810, LR=4.1e-02, kappa=5.017, |gamma|=0.1481, time=8.2min
Epoch  290: Loss=47.1294, LR=3.8e-02, kappa=5.071, |gamma|=0.1477, time=8.5min
Epoch  300: Loss=46.9037, LR=3.5e-02, kappa=5.118, |gamma|=0.1473, time=8.8min
Epoch  310: Loss=46.7014, LR=3.2e-02, kappa=5.160, |gamma|=0.1469, time=9.0min
Epoch  320: Loss=46.5203, LR=2.9e-02, kappa=5.197, |gamma|=0.1466, time=9.3min
Epoch  330: Loss=46.3586, LR=2.6e-02, kappa=5.230, |gamma|=0.1462, time=9.6min
Epoch  340: Loss=46.2144, LR=2.4e-02, kappa=5.259, |gamma|=0.1460, time=9.9min
Epoch  350: Loss=46.0863, LR=2.1e-02, kappa=5.284, |gamma|=0.1457, time=10.2min
Epoch  360: Loss=45.9729, LR=1.9e-02, kappa=5.306, |gamma|=0.1454, time=10.5min
Epoch  370: Loss=45.8731, LR=1.6e-02, kappa=5.326, |gamma|=0.1451, time=10.8min
Epoch  380: Loss=45.7856, LR=1.4e-02, kappa=5.342, |gamma|=0.1449, time=11.1min
Epoch  390: Loss=45.7094, LR=1.2e-02, kappa=5.357, |gamma|=0.1447, time=11.4min
Epoch  400: Loss=45.6434, LR=1.0e-02, kappa=5.369, |gamma|=0.1445, time=11.7min
Epoch  410: Loss=45.5869, LR=8.5e-03, kappa=5.379, |gamma|=0.1443, time=11.9min
Epoch  420: Loss=45.5388, LR=7.0e-03, kappa=5.387, |gamma|=0.1442, time=12.2min
Epoch  430: Loss=45.4982, LR=5.6e-03, kappa=5.394, |gamma|=0.1441, time=12.5min
Epoch  440: Loss=45.4644, LR=4.4e-03, kappa=5.399, |gamma|=0.1439, time=12.8min
Epoch  450: Loss=45.4363, LR=3.3e-03, kappa=5.403, |gamma|=0.1438, time=13.1min
Epoch  460: Loss=45.4130, LR=2.5e-03, kappa=5.406, |gamma|=0.1438, time=13.4min
Epoch  470: Loss=45.3938, LR=1.8e-03, kappa=5.409, |gamma|=0.1437, time=13.7min
Epoch  480: Loss=45.3776, LR=1.4e-03, kappa=5.410, |gamma|=0.1436, time=14.0min
Epoch  490: Loss=45.3635, LR=1.1e-03, kappa=5.412, |gamma|=0.1436, time=14.3min

Training complete: 500 epochs in 14.5 min
Final loss: 45.3518

Final params: kappa=5.4126, mean|gamma|=0.1435
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2/enrollment_model_REPARAM_W0.0001_batch_30000_40000.pt

Batch 4 DONE in 14.7 min (3/39 completed)

======================================================================
BATCH 5/40: samples 40000-50000
Elapsed: 43 min | Est remaining: ~540 min (9.0 hrs)
======================================================================

============================================================
REPARAM v2 Training: samples 40000 to 50000
Epochs: 500, LR: 0.1, grad_clip: 5.0
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 12 diseases
Cluster 1: 15 diseases
Cluster 2: 16 diseases
Cluster 3: 52 diseases
Cluster 4: 5 diseases
Cluster 5: 15 diseases
Cluster 6: 13 diseases
Cluster 7: 12 diseases
Cluster 8: 76 diseases
Cluster 9: 5 diseases
Cluster 10: 9 diseases
Cluster 11: 5 diseases
Cluster 12: 7 diseases
Cluster 13: 25 diseases
Cluster 14: 6 diseases
Cluster 15: 11 diseases
Cluster 16: 7 diseases
Cluster 17: 13 diseases
Cluster 18: 12 diseases
Cluster 19: 32 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=64.3720, LR=1.0e-01, kappa=1.100, |gamma|=0.1011, time=0.0min
Epoch   10: Loss=136.4675, LR=1.0e-01, kappa=2.027, |gamma|=0.0923, time=0.3min
Epoch   20: Loss=99.9842, LR=1.0e-01, kappa=2.673, |gamma|=0.1302, time=0.6min
Epoch   30: Loss=73.2049, LR=9.9e-02, kappa=3.014, |gamma|=0.1477, time=0.9min
Epoch   40: Loss=63.4407, LR=9.8e-02, kappa=3.146, |gamma|=0.1548, time=1.2min
Epoch   50: Loss=60.3378, LR=9.7e-02, kappa=3.171, |gamma|=0.1585, time=1.5min
Epoch   60: Loss=58.9644, LR=9.6e-02, kappa=3.163, |gamma|=0.1607, time=1.7min
Epoch   70: Loss=58.1825, LR=9.5e-02, kappa=3.161, |gamma|=0.1613, time=2.0min
Epoch   80: Loss=57.7387, LR=9.4e-02, kappa=3.174, |gamma|=0.1614, time=2.3min
Epoch   90: Loss=57.3737, LR=9.2e-02, kappa=3.201, |gamma|=0.1619, time=2.6min
Epoch  100: Loss=56.9528, LR=9.0e-02, kappa=3.243, |gamma|=0.1614, time=2.9min
Epoch  110: Loss=56.5039, LR=8.8e-02, kappa=3.298, |gamma|=0.1606, time=3.1min
Epoch  120: Loss=56.0065, LR=8.6e-02, kappa=3.367, |gamma|=0.1594, time=3.4min
Epoch  130: Loss=55.4438, LR=8.4e-02, kappa=3.456, |gamma|=0.1575, time=3.7min
Epoch  140: Loss=54.8198, LR=8.2e-02, kappa=3.566, |gamma|=0.1544, time=4.0min
Epoch  150: Loss=54.1446, LR=7.9e-02, kappa=3.696, |gamma|=0.1511, time=4.3min
Epoch  160: Loss=53.4222, LR=7.7e-02, kappa=3.844, |gamma|=0.1469, time=4.5min
Epoch  170: Loss=52.6724, LR=7.4e-02, kappa=4.004, |gamma|=0.1428, time=4.8min
Epoch  180: Loss=51.9277, LR=7.1e-02, kappa=4.168, |gamma|=0.1385, time=5.1min
Epoch  190: Loss=51.2098, LR=6.8e-02, kappa=4.333, |gamma|=0.1363, time=5.4min
Epoch  200: Loss=50.5355, LR=6.6e-02, kappa=4.492, |gamma|=0.1327, time=5.6min
Epoch  210: Loss=49.9134, LR=6.3e-02, kappa=4.642, |gamma|=0.1295, time=5.9min
Epoch  220: Loss=49.3470, LR=5.9e-02, kappa=4.779, |gamma|=0.1272, time=6.2min
Epoch  230: Loss=48.8397, LR=5.6e-02, kappa=4.903, |gamma|=0.1254, time=6.5min
Epoch  240: Loss=48.3872, LR=5.3e-02, kappa=5.015, |gamma|=0.1239, time=6.8min
Epoch  250: Loss=47.9839, LR=5.0e-02, kappa=5.114, |gamma|=0.1228, time=7.0min
Epoch  260: Loss=47.6247, LR=4.7e-02, kappa=5.202, |gamma|=0.1218, time=7.3min
Epoch  270: Loss=47.3045, LR=4.4e-02, kappa=5.280, |gamma|=0.1210, time=7.6min
Epoch  280: Loss=47.0187, LR=4.1e-02, kappa=5.349, |gamma|=0.1204, time=7.9min
Epoch  290: Loss=46.7635, LR=3.8e-02, kappa=5.410, |gamma|=0.1199, time=8.1min
Epoch  300: Loss=46.5354, LR=3.5e-02, kappa=5.464, |gamma|=0.1196, time=8.4min
Epoch  310: Loss=46.3317, LR=3.2e-02, kappa=5.511, |gamma|=0.1192, time=8.7min
Epoch  320: Loss=46.1498, LR=2.9e-02, kappa=5.553, |gamma|=0.1188, time=9.0min
Epoch  330: Loss=45.9877, LR=2.6e-02, kappa=5.590, |gamma|=0.1184, time=9.3min
Epoch  340: Loss=45.8435, LR=2.4e-02, kappa=5.623, |gamma|=0.1180, time=9.5min
Epoch  350: Loss=45.7156, LR=2.1e-02, kappa=5.651, |gamma|=0.1177, time=9.8min
Epoch  360: Loss=45.6024, LR=1.9e-02, kappa=5.676, |gamma|=0.1174, time=10.1min
Epoch  370: Loss=45.5028, LR=1.6e-02, kappa=5.697, |gamma|=0.1171, time=10.4min
Epoch  380: Loss=45.4156, LR=1.4e-02, kappa=5.716, |gamma|=0.1169, time=10.6min
Epoch  390: Loss=45.3397, LR=1.2e-02, kappa=5.732, |gamma|=0.1166, time=10.9min
Epoch  400: Loss=45.2740, LR=1.0e-02, kappa=5.745, |gamma|=0.1164, time=11.2min
Epoch  410: Loss=45.2177, LR=8.5e-03, kappa=5.756, |gamma|=0.1163, time=11.5min
Epoch  420: Loss=45.1698, LR=7.0e-03, kappa=5.765, |gamma|=0.1161, time=11.8min
Epoch  430: Loss=45.1294, LR=5.6e-03, kappa=5.773, |gamma|=0.1160, time=12.1min
Epoch  440: Loss=45.0957, LR=4.4e-03, kappa=5.778, |gamma|=0.1159, time=12.3min
Epoch  450: Loss=45.0677, LR=3.3e-03, kappa=5.783, |gamma|=0.1158, time=12.6min
Epoch  460: Loss=45.0447, LR=2.5e-03, kappa=5.786, |gamma|=0.1157, time=12.9min
Epoch  470: Loss=45.0255, LR=1.8e-03, kappa=5.789, |gamma|=0.1156, time=13.2min
Epoch  480: Loss=45.0095, LR=1.4e-03, kappa=5.791, |gamma|=0.1156, time=13.4min
Epoch  490: Loss=44.9955, LR=1.1e-03, kappa=5.792, |gamma|=0.1155, time=13.7min

Training complete: 500 epochs in 14.0 min
Final loss: 44.9838

Final params: kappa=5.7934, mean|gamma|=0.1155
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2/enrollment_model_REPARAM_W0.0001_batch_40000_50000.pt

Batch 5 DONE in 14.1 min (4/39 completed)

======================================================================
BATCH 6/40: samples 50000-60000
Elapsed: 57 min | Est remaining: ~525 min (8.8 hrs)
======================================================================

============================================================
REPARAM v2 Training: samples 50000 to 60000
Epochs: 500, LR: 0.1, grad_clip: 5.0
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 22 diseases
Cluster 1: 24 diseases
Cluster 2: 16 diseases
Cluster 3: 5 diseases
Cluster 4: 8 diseases
Cluster 5: 9 diseases
Cluster 6: 31 diseases
Cluster 7: 16 diseases
Cluster 8: 9 diseases
Cluster 9: 8 diseases
Cluster 10: 13 diseases
Cluster 11: 17 diseases
Cluster 12: 14 diseases
Cluster 13: 8 diseases
Cluster 14: 5 diseases
Cluster 15: 12 diseases
Cluster 16: 12 diseases
Cluster 17: 88 diseases
Cluster 18: 9 diseases
Cluster 19: 22 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=63.1814, LR=1.0e-01, kappa=1.100, |gamma|=0.1009, time=0.0min
Epoch   10: Loss=139.6304, LR=1.0e-01, kappa=2.022, |gamma|=0.0994, time=0.3min
Epoch   20: Loss=100.2326, LR=1.0e-01, kappa=2.657, |gamma|=0.1374, time=0.6min
Epoch   30: Loss=72.0205, LR=9.9e-02, kappa=2.987, |gamma|=0.1564, time=0.8min
Epoch   40: Loss=62.2869, LR=9.8e-02, kappa=3.107, |gamma|=0.1602, time=1.1min
Epoch   50: Loss=59.3379, LR=9.7e-02, kappa=3.122, |gamma|=0.1622, time=1.4min
Epoch   60: Loss=58.0510, LR=9.6e-02, kappa=3.109, |gamma|=0.1618, time=1.7min
Epoch   70: Loss=57.2809, LR=9.5e-02, kappa=3.105, |gamma|=0.1608, time=2.0min
Epoch   80: Loss=56.8179, LR=9.4e-02, kappa=3.118, |gamma|=0.1614, time=2.3min
Epoch   90: Loss=56.4652, LR=9.2e-02, kappa=3.145, |gamma|=0.1627, time=2.6min
Epoch  100: Loss=56.0566, LR=9.0e-02, kappa=3.180, |gamma|=0.1625, time=2.8min
Epoch  110: Loss=55.6207, LR=8.8e-02, kappa=3.228, |gamma|=0.1621, time=3.1min
Epoch  120: Loss=55.1404, LR=8.6e-02, kappa=3.290, |gamma|=0.1619, time=3.4min
Epoch  130: Loss=54.6000, LR=8.4e-02, kappa=3.369, |gamma|=0.1609, time=3.7min
Epoch  140: Loss=54.0383, LR=8.2e-02, kappa=3.466, |gamma|=0.1602, time=4.0min
Epoch  150: Loss=53.3831, LR=7.9e-02, kappa=3.578, |gamma|=0.1588, time=4.3min
Epoch  160: Loss=52.7163, LR=7.7e-02, kappa=3.705, |gamma|=0.1576, time=4.6min
Epoch  170: Loss=52.0363, LR=7.4e-02, kappa=3.842, |gamma|=0.1562, time=4.9min
Epoch  180: Loss=51.3657, LR=7.1e-02, kappa=3.983, |gamma|=0.1551, time=5.2min
Epoch  190: Loss=50.7254, LR=6.8e-02, kappa=4.124, |gamma|=0.1537, time=5.5min
Epoch  200: Loss=50.1090, LR=6.6e-02, kappa=4.259, |gamma|=0.1520, time=5.7min
Epoch  210: Loss=49.5336, LR=6.3e-02, kappa=4.387, |gamma|=0.1502, time=6.0min
Epoch  220: Loss=49.0047, LR=5.9e-02, kappa=4.504, |gamma|=0.1483, time=6.3min
Epoch  230: Loss=48.5266, LR=5.6e-02, kappa=4.612, |gamma|=0.1470, time=6.6min
Epoch  240: Loss=48.0955, LR=5.3e-02, kappa=4.709, |gamma|=0.1461, time=6.9min
Epoch  250: Loss=47.7078, LR=5.0e-02, kappa=4.796, |gamma|=0.1452, time=7.1min
Epoch  260: Loss=47.3594, LR=4.7e-02, kappa=4.875, |gamma|=0.1444, time=7.4min
Epoch  270: Loss=47.0464, LR=4.4e-02, kappa=4.944, |gamma|=0.1436, time=7.7min
Epoch  280: Loss=46.7653, LR=4.1e-02, kappa=5.006, |gamma|=0.1427, time=8.0min
Epoch  290: Loss=46.5128, LR=3.8e-02, kappa=5.062, |gamma|=0.1419, time=8.2min
Epoch  300: Loss=46.2863, LR=3.5e-02, kappa=5.111, |gamma|=0.1411, time=8.5min
Epoch  310: Loss=46.0831, LR=3.2e-02, kappa=5.154, |gamma|=0.1403, time=8.8min
Epoch  320: Loss=45.9012, LR=2.9e-02, kappa=5.193, |gamma|=0.1397, time=9.1min
Epoch  330: Loss=45.7386, LR=2.6e-02, kappa=5.228, |gamma|=0.1391, time=9.3min
Epoch  340: Loss=45.5937, LR=2.4e-02, kappa=5.258, |gamma|=0.1385, time=9.6min
Epoch  350: Loss=45.4650, LR=2.1e-02, kappa=5.285, |gamma|=0.1380, time=9.9min
Epoch  360: Loss=45.3511, LR=1.9e-02, kappa=5.308, |gamma|=0.1375, time=10.2min
Epoch  370: Loss=45.2507, LR=1.6e-02, kappa=5.328, |gamma|=0.1371, time=10.5min
Epoch  380: Loss=45.1628, LR=1.4e-02, kappa=5.346, |gamma|=0.1368, time=10.7min
Epoch  390: Loss=45.0862, LR=1.2e-02, kappa=5.361, |gamma|=0.1364, time=11.0min
Epoch  400: Loss=45.0200, LR=1.0e-02, kappa=5.374, |gamma|=0.1361, time=11.3min
Epoch  410: Loss=44.9632, LR=8.5e-03, kappa=5.385, |gamma|=0.1359, time=11.6min
Epoch  420: Loss=44.9150, LR=7.0e-03, kappa=5.393, |gamma|=0.1357, time=11.8min
Epoch  430: Loss=44.8743, LR=5.6e-03, kappa=5.401, |gamma|=0.1355, time=12.1min
Epoch  440: Loss=44.8404, LR=4.4e-03, kappa=5.406, |gamma|=0.1353, time=12.4min
Epoch  450: Loss=44.8123, LR=3.3e-03, kappa=5.411, |gamma|=0.1352, time=12.7min
Epoch  460: Loss=44.7891, LR=2.5e-03, kappa=5.414, |gamma|=0.1351, time=13.0min
Epoch  470: Loss=44.7699, LR=1.8e-03, kappa=5.417, |gamma|=0.1350, time=13.2min
Epoch  480: Loss=44.7538, LR=1.4e-03, kappa=5.418, |gamma|=0.1349, time=13.5min
Epoch  490: Loss=44.7398, LR=1.1e-03, kappa=5.420, |gamma|=0.1349, time=13.8min

Training complete: 500 epochs in 14.0 min
Final loss: 44.7282

Final params: kappa=5.4209, mean|gamma|=0.1348
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2/enrollment_model_REPARAM_W0.0001_batch_50000_60000.pt

Batch 6 DONE in 14.2 min (5/39 completed)

======================================================================
BATCH 7/40: samples 60000-70000
Elapsed: 71 min | Est remaining: ~510 min (8.5 hrs)
======================================================================

============================================================
REPARAM v2 Training: samples 60000 to 70000
Epochs: 500, LR: 0.1, grad_clip: 5.0
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 13 diseases
Cluster 1: 14 diseases
Cluster 2: 28 diseases
Cluster 3: 80 diseases
Cluster 4: 9 diseases
Cluster 5: 14 diseases
Cluster 6: 13 diseases
Cluster 7: 12 diseases
Cluster 8: 17 diseases
Cluster 9: 14 diseases
Cluster 10: 7 diseases
Cluster 11: 22 diseases
Cluster 12: 9 diseases
Cluster 13: 5 diseases
Cluster 14: 35 diseases
Cluster 15: 5 diseases
Cluster 16: 29 diseases
Cluster 17: 7 diseases
Cluster 18: 7 diseases
Cluster 19: 8 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=64.3034, LR=1.0e-01, kappa=1.100, |gamma|=0.1009, time=0.0min
Epoch   10: Loss=136.6680, LR=1.0e-01, kappa=2.026, |gamma|=0.0922, time=0.3min
Epoch   20: Loss=100.0282, LR=1.0e-01, kappa=2.669, |gamma|=0.1329, time=0.6min
Epoch   30: Loss=73.1675, LR=9.9e-02, kappa=3.009, |gamma|=0.1518, time=0.8min
Epoch   40: Loss=63.3989, LR=9.8e-02, kappa=3.140, |gamma|=0.1584, time=1.1min
Epoch   50: Loss=60.2988, LR=9.7e-02, kappa=3.161, |gamma|=0.1599, time=1.4min
Epoch   60: Loss=58.9265, LR=9.6e-02, kappa=3.150, |gamma|=0.1622, time=1.7min
Epoch   70: Loss=58.1434, LR=9.5e-02, kappa=3.146, |gamma|=0.1623, time=2.0min
Epoch   80: Loss=57.6982, LR=9.4e-02, kappa=3.158, |gamma|=0.1629, time=2.3min
Epoch   90: Loss=57.3364, LR=9.2e-02, kappa=3.184, |gamma|=0.1619, time=2.6min
Epoch  100: Loss=56.9199, LR=9.0e-02, kappa=3.220, |gamma|=0.1614, time=2.9min
Epoch  110: Loss=56.4783, LR=8.8e-02, kappa=3.267, |gamma|=0.1606, time=3.1min
Epoch  120: Loss=56.0031, LR=8.6e-02, kappa=3.328, |gamma|=0.1596, time=3.4min
Epoch  130: Loss=55.4442, LR=8.4e-02, kappa=3.406, |gamma|=0.1574, time=3.7min
Epoch  140: Loss=54.8417, LR=8.2e-02, kappa=3.504, |gamma|=0.1552, time=4.0min
Epoch  150: Loss=54.1878, LR=7.9e-02, kappa=3.622, |gamma|=0.1524, time=4.3min
Epoch  160: Loss=53.5009, LR=7.7e-02, kappa=3.756, |gamma|=0.1499, time=4.6min
Epoch  170: Loss=52.7844, LR=7.4e-02, kappa=3.903, |gamma|=0.1480, time=4.9min
Epoch  180: Loss=52.0658, LR=7.1e-02, kappa=4.058, |gamma|=0.1461, time=5.2min
Epoch  190: Loss=51.3647, LR=6.8e-02, kappa=4.215, |gamma|=0.1442, time=5.5min
Epoch  200: Loss=50.6952, LR=6.6e-02, kappa=4.370, |gamma|=0.1421, time=5.7min
Epoch  210: Loss=50.0736, LR=6.3e-02, kappa=4.517, |gamma|=0.1408, time=6.0min
Epoch  220: Loss=49.5029, LR=5.9e-02, kappa=4.654, |gamma|=0.1389, time=6.3min
Epoch  230: Loss=48.9794, LR=5.6e-02, kappa=4.779, |gamma|=0.1373, time=6.6min
Epoch  240: Loss=48.5103, LR=5.3e-02, kappa=4.892, |gamma|=0.1358, time=6.9min
Epoch  250: Loss=48.0914, LR=5.0e-02, kappa=4.993, |gamma|=0.1342, time=7.2min
Epoch  260: Loss=47.7177, LR=4.7e-02, kappa=5.084, |gamma|=0.1327, time=7.4min
Epoch  270: Loss=47.3844, LR=4.4e-02, kappa=5.164, |gamma|=0.1314, time=7.7min
Epoch  280: Loss=47.0872, LR=4.1e-02, kappa=5.235, |gamma|=0.1302, time=8.0min
Epoch  290: Loss=46.8220, LR=3.8e-02, kappa=5.297, |gamma|=0.1293, time=8.3min
Epoch  300: Loss=46.5853, LR=3.5e-02, kappa=5.352, |gamma|=0.1285, time=8.6min
Epoch  310: Loss=46.3740, LR=3.2e-02, kappa=5.401, |gamma|=0.1278, time=8.8min
Epoch  320: Loss=46.1856, LR=2.9e-02, kappa=5.444, |gamma|=0.1270, time=9.1min
Epoch  330: Loss=46.0178, LR=2.6e-02, kappa=5.482, |gamma|=0.1264, time=9.4min
Epoch  340: Loss=45.8688, LR=2.4e-02, kappa=5.515, |gamma|=0.1258, time=9.7min
Epoch  350: Loss=45.7367, LR=2.1e-02, kappa=5.545, |gamma|=0.1253, time=10.0min
Epoch  360: Loss=45.6200, LR=1.9e-02, kappa=5.570, |gamma|=0.1248, time=10.2min
Epoch  370: Loss=45.5174, LR=1.6e-02, kappa=5.592, |gamma|=0.1243, time=10.5min
Epoch  380: Loss=45.4276, LR=1.4e-02, kappa=5.611, |gamma|=0.1239, time=10.8min
Epoch  390: Loss=45.3495, LR=1.2e-02, kappa=5.627, |gamma|=0.1235, time=11.1min
Epoch  400: Loss=45.2819, LR=1.0e-02, kappa=5.641, |gamma|=0.1232, time=11.4min
Epoch  410: Loss=45.2240, LR=8.5e-03, kappa=5.652, |gamma|=0.1229, time=11.6min
Epoch  420: Loss=45.1746, LR=7.0e-03, kappa=5.662, |gamma|=0.1226, time=11.9min
Epoch  430: Loss=45.1327, LR=5.6e-03, kappa=5.669, |gamma|=0.1223, time=12.2min
Epoch  440: Loss=45.0967, LR=4.4e-03, kappa=5.675, |gamma|=0.1220, time=12.5min
Epoch  450: Loss=45.0653, LR=3.3e-03, kappa=5.680, |gamma|=0.1215, time=12.8min
Epoch  460: Loss=45.0399, LR=2.5e-03, kappa=5.684, |gamma|=0.1211, time=13.1min
Epoch  470: Loss=45.0192, LR=1.8e-03, kappa=5.687, |gamma|=0.1209, time=13.4min
Epoch  480: Loss=45.0019, LR=1.4e-03, kappa=5.689, |gamma|=0.1209, time=13.6min
Epoch  490: Loss=44.9869, LR=1.1e-03, kappa=5.691, |gamma|=0.1209, time=13.9min

Training complete: 500 epochs in 14.2 min
Final loss: 44.9745

Final params: kappa=5.6917, mean|gamma|=0.1209
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2/enrollment_model_REPARAM_W0.0001_batch_60000_70000.pt

Batch 7 DONE in 14.3 min (6/39 completed)

======================================================================
BATCH 8/40: samples 70000-80000
Elapsed: 85 min | Est remaining: ~495 min (8.2 hrs)
======================================================================

============================================================
REPARAM v2 Training: samples 70000 to 80000
Epochs: 500, LR: 0.1, grad_clip: 5.0
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 11 diseases
Cluster 1: 14 diseases
Cluster 2: 17 diseases
Cluster 3: 8 diseases
Cluster 4: 16 diseases
Cluster 5: 114 diseases
Cluster 6: 13 diseases
Cluster 7: 10 diseases
Cluster 8: 16 diseases
Cluster 9: 9 diseases
Cluster 10: 6 diseases
Cluster 11: 10 diseases
Cluster 12: 9 diseases
Cluster 13: 11 diseases
Cluster 14: 5 diseases
Cluster 15: 9 diseases
Cluster 16: 16 diseases
Cluster 17: 24 diseases
Cluster 18: 12 diseases
Cluster 19: 18 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=63.4252, LR=1.0e-01, kappa=1.100, |gamma|=0.1006, time=0.0min
Epoch   10: Loss=138.8593, LR=1.0e-01, kappa=2.023, |gamma|=0.0948, time=0.3min
Epoch   20: Loss=100.1717, LR=1.0e-01, kappa=2.659, |gamma|=0.1313, time=0.6min
Epoch   30: Loss=72.3099, LR=9.9e-02, kappa=2.991, |gamma|=0.1453, time=0.9min
Epoch   40: Loss=62.5657, LR=9.8e-02, kappa=3.120, |gamma|=0.1483, time=1.2min
Epoch   50: Loss=59.5813, LR=9.7e-02, kappa=3.147, |gamma|=0.1489, time=1.5min
Epoch   60: Loss=58.2729, LR=9.6e-02, kappa=3.143, |gamma|=0.1474, time=1.7min
Epoch   70: Loss=57.4954, LR=9.5e-02, kappa=3.146, |gamma|=0.1462, time=2.0min
Epoch   80: Loss=57.0290, LR=9.4e-02, kappa=3.165, |gamma|=0.1456, time=2.3min
Epoch   90: Loss=56.6543, LR=9.2e-02, kappa=3.200, |gamma|=0.1447, time=2.6min
Epoch  100: Loss=56.2242, LR=9.0e-02, kappa=3.249, |gamma|=0.1441, time=2.8min
Epoch  110: Loss=55.7492, LR=8.8e-02, kappa=3.313, |gamma|=0.1419, time=3.1min
Epoch  120: Loss=55.2149, LR=8.6e-02, kappa=3.396, |gamma|=0.1397, time=3.4min
Epoch  130: Loss=54.6034, LR=8.4e-02, kappa=3.501, |gamma|=0.1368, time=3.7min
Epoch  140: Loss=53.9266, LR=8.2e-02, kappa=3.631, |gamma|=0.1336, time=4.0min
Epoch  150: Loss=53.1864, LR=7.9e-02, kappa=3.781, |gamma|=0.1304, time=4.2min
Epoch  160: Loss=52.4213, LR=7.7e-02, kappa=3.946, |gamma|=0.1276, time=4.5min
Epoch  170: Loss=51.6609, LR=7.4e-02, kappa=4.116, |gamma|=0.1250, time=4.8min
Epoch  180: Loss=50.9202, LR=7.1e-02, kappa=4.286, |gamma|=0.1226, time=5.1min
Epoch  190: Loss=50.2345, LR=6.8e-02, kappa=4.448, |gamma|=0.1212, time=5.3min
Epoch  200: Loss=49.5958, LR=6.6e-02, kappa=4.600, |gamma|=0.1194, time=5.6min
Epoch  210: Loss=49.0165, LR=6.3e-02, kappa=4.738, |gamma|=0.1180, time=5.9min
Epoch  220: Loss=48.4973, LR=5.9e-02, kappa=4.864, |gamma|=0.1168, time=6.1min
Epoch  230: Loss=48.0329, LR=5.6e-02, kappa=4.976, |gamma|=0.1160, time=6.4min
Epoch  240: Loss=47.6173, LR=5.3e-02, kappa=5.075, |gamma|=0.1154, time=6.7min
Epoch  250: Loss=47.2451, LR=5.0e-02, kappa=5.164, |gamma|=0.1149, time=7.0min
Epoch  260: Loss=46.9113, LR=4.7e-02, kappa=5.242, |gamma|=0.1144, time=7.2min
Epoch  270: Loss=46.6114, LR=4.4e-02, kappa=5.311, |gamma|=0.1138, time=7.5min
Epoch  280: Loss=46.3413, LR=4.1e-02, kappa=5.373, |gamma|=0.1131, time=7.8min
Epoch  290: Loss=46.0986, LR=3.8e-02, kappa=5.427, |gamma|=0.1126, time=8.1min
Epoch  300: Loss=45.8801, LR=3.5e-02, kappa=5.475, |gamma|=0.1120, time=8.3min
Epoch  310: Loss=45.6837, LR=3.2e-02, kappa=5.518, |gamma|=0.1114, time=8.6min
Epoch  320: Loss=45.5075, LR=2.9e-02, kappa=5.556, |gamma|=0.1109, time=8.9min
Epoch  330: Loss=45.3497, LR=2.6e-02, kappa=5.589, |gamma|=0.1104, time=9.2min
Epoch  340: Loss=45.2088, LR=2.4e-02, kappa=5.618, |gamma|=0.1100, time=9.4min
Epoch  350: Loss=45.0834, LR=2.1e-02, kappa=5.644, |gamma|=0.1096, time=9.7min
Epoch  360: Loss=44.9723, LR=1.9e-02, kappa=5.667, |gamma|=0.1092, time=10.0min
Epoch  370: Loss=44.8743, LR=1.6e-02, kappa=5.686, |gamma|=0.1089, time=10.3min
Epoch  380: Loss=44.7883, LR=1.4e-02, kappa=5.703, |gamma|=0.1087, time=10.5min
Epoch  390: Loss=44.7134, LR=1.2e-02, kappa=5.717, |gamma|=0.1085, time=10.8min
Epoch  400: Loss=44.6486, LR=1.0e-02, kappa=5.730, |gamma|=0.1084, time=11.1min
Epoch  410: Loss=44.5930, LR=8.5e-03, kappa=5.740, |gamma|=0.1083, time=11.4min
Epoch  420: Loss=44.5457, LR=7.0e-03, kappa=5.748, |gamma|=0.1081, time=11.6min
Epoch  430: Loss=44.5058, LR=5.6e-03, kappa=5.755, |gamma|=0.1081, time=11.9min
Epoch  440: Loss=44.4725, LR=4.4e-03, kappa=5.760, |gamma|=0.1080, time=12.2min
Epoch  450: Loss=44.4449, LR=3.3e-03, kappa=5.765, |gamma|=0.1079, time=12.4min
Epoch  460: Loss=44.4222, LR=2.5e-03, kappa=5.768, |gamma|=0.1079, time=12.7min
Epoch  470: Loss=44.4034, LR=1.8e-03, kappa=5.770, |gamma|=0.1078, time=13.0min
Epoch  480: Loss=44.3876, LR=1.4e-03, kappa=5.772, |gamma|=0.1078, time=13.3min
Epoch  490: Loss=44.3738, LR=1.1e-03, kappa=5.773, |gamma|=0.1078, time=13.6min

Training complete: 500 epochs in 13.8 min
Final loss: 44.3624

Final params: kappa=5.7743, mean|gamma|=0.1077
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2/enrollment_model_REPARAM_W0.0001_batch_70000_80000.pt

Batch 8 DONE in 14.0 min (7/39 completed)

======================================================================
BATCH 9/40: samples 80000-90000
Elapsed: 99 min | Est remaining: ~480 min (8.0 hrs)
======================================================================

============================================================
REPARAM v2 Training: samples 80000 to 90000
Epochs: 500, LR: 0.1, grad_clip: 5.0
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 17 diseases
Cluster 1: 24 diseases
Cluster 2: 5 diseases
Cluster 3: 10 diseases
Cluster 4: 16 diseases
Cluster 5: 13 diseases
Cluster 6: 12 diseases
Cluster 7: 20 diseases
Cluster 8: 32 diseases
Cluster 9: 9 diseases
Cluster 10: 100 diseases
Cluster 11: 13 diseases
Cluster 12: 9 diseases
Cluster 13: 8 diseases
Cluster 14: 17 diseases
Cluster 15: 5 diseases
Cluster 16: 4 diseases
Cluster 17: 14 diseases
Cluster 18: 11 diseases
Cluster 19: 9 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=63.3644, LR=1.0e-01, kappa=1.100, |gamma|=0.1008, time=0.0min
Epoch   10: Loss=139.0842, LR=1.0e-01, kappa=2.023, |gamma|=0.0939, time=0.3min
Epoch   20: Loss=100.2090, LR=1.0e-01, kappa=2.659, |gamma|=0.1291, time=0.6min
Epoch   30: Loss=72.2416, LR=9.9e-02, kappa=2.990, |gamma|=0.1459, time=0.8min
Epoch   40: Loss=62.5020, LR=9.8e-02, kappa=3.115, |gamma|=0.1480, time=1.1min
Epoch   50: Loss=59.5319, LR=9.7e-02, kappa=3.137, |gamma|=0.1446, time=1.4min
Epoch   60: Loss=58.2335, LR=9.6e-02, kappa=3.131, |gamma|=0.1402, time=1.7min
Epoch   70: Loss=57.4655, LR=9.5e-02, kappa=3.130, |gamma|=0.1381, time=2.0min
Epoch   80: Loss=57.0101, LR=9.4e-02, kappa=3.145, |gamma|=0.1385, time=2.2min
Epoch   90: Loss=56.6546, LR=9.2e-02, kappa=3.175, |gamma|=0.1385, time=2.5min
Epoch  100: Loss=56.2500, LR=9.0e-02, kappa=3.215, |gamma|=0.1390, time=2.8min
Epoch  110: Loss=55.8158, LR=8.8e-02, kappa=3.265, |gamma|=0.1394, time=3.1min
Epoch  120: Loss=55.3377, LR=8.6e-02, kappa=3.329, |gamma|=0.1392, time=3.3min
Epoch  130: Loss=54.7934, LR=8.4e-02, kappa=3.410, |gamma|=0.1381, time=3.6min
Epoch  140: Loss=54.2006, LR=8.2e-02, kappa=3.513, |gamma|=0.1373, time=3.9min
Epoch  150: Loss=53.5413, LR=7.9e-02, kappa=3.635, |gamma|=0.1349, time=4.2min
Epoch  160: Loss=52.8455, LR=7.7e-02, kappa=3.776, |gamma|=0.1330, time=4.4min
Epoch  170: Loss=52.1302, LR=7.4e-02, kappa=3.929, |gamma|=0.1306, time=4.7min
Epoch  180: Loss=51.4053, LR=7.1e-02, kappa=4.088, |gamma|=0.1287, time=5.0min
Epoch  190: Loss=50.7088, LR=6.8e-02, kappa=4.247, |gamma|=0.1268, time=5.3min
Epoch  200: Loss=50.0554, LR=6.6e-02, kappa=4.401, |gamma|=0.1251, time=5.5min
Epoch  210: Loss=49.4464, LR=6.3e-02, kappa=4.546, |gamma|=0.1235, time=5.8min
Epoch  220: Loss=48.8924, LR=5.9e-02, kappa=4.679, |gamma|=0.1217, time=6.1min
Epoch  230: Loss=48.3929, LR=5.6e-02, kappa=4.801, |gamma|=0.1198, time=6.4min
Epoch  240: Loss=47.9399, LR=5.3e-02, kappa=4.910, |gamma|=0.1175, time=6.6min
Epoch  250: Loss=47.5382, LR=5.0e-02, kappa=5.008, |gamma|=0.1163, time=6.9min
Epoch  260: Loss=47.1794, LR=4.7e-02, kappa=5.094, |gamma|=0.1151, time=7.2min
Epoch  270: Loss=46.8595, LR=4.4e-02, kappa=5.171, |gamma|=0.1141, time=7.5min
Epoch  280: Loss=46.5739, LR=4.1e-02, kappa=5.239, |gamma|=0.1132, time=7.7min
Epoch  290: Loss=46.3187, LR=3.8e-02, kappa=5.298, |gamma|=0.1124, time=8.0min
Epoch  300: Loss=46.0905, LR=3.5e-02, kappa=5.351, |gamma|=0.1115, time=8.3min
Epoch  310: Loss=45.8866, LR=3.2e-02, kappa=5.398, |gamma|=0.1107, time=8.6min
Epoch  320: Loss=45.7045, LR=2.9e-02, kappa=5.440, |gamma|=0.1100, time=8.9min
Epoch  330: Loss=45.5420, LR=2.6e-02, kappa=5.476, |gamma|=0.1094, time=9.1min
Epoch  340: Loss=45.3974, LR=2.4e-02, kappa=5.508, |gamma|=0.1089, time=9.4min
Epoch  350: Loss=45.2690, LR=2.1e-02, kappa=5.536, |gamma|=0.1084, time=9.7min
Epoch  360: Loss=45.1553, LR=1.9e-02, kappa=5.560, |gamma|=0.1079, time=10.0min
Epoch  370: Loss=45.0553, LR=1.6e-02, kappa=5.581, |gamma|=0.1075, time=10.3min
Epoch  380: Loss=44.9675, LR=1.4e-02, kappa=5.600, |gamma|=0.1072, time=10.6min
Epoch  390: Loss=44.8911, LR=1.2e-02, kappa=5.615, |gamma|=0.1069, time=10.8min
Epoch  400: Loss=44.8250, LR=1.0e-02, kappa=5.629, |gamma|=0.1066, time=11.1min
Epoch  410: Loss=44.7683, LR=8.5e-03, kappa=5.640, |gamma|=0.1064, time=11.4min
Epoch  420: Loss=44.7200, LR=7.0e-03, kappa=5.649, |gamma|=0.1062, time=11.6min
Epoch  430: Loss=44.6793, LR=5.6e-03, kappa=5.656, |gamma|=0.1060, time=11.9min
Epoch  440: Loss=44.6453, LR=4.4e-03, kappa=5.662, |gamma|=0.1059, time=12.2min
Epoch  450: Loss=44.6171, LR=3.3e-03, kappa=5.666, |gamma|=0.1057, time=12.5min
Epoch  460: Loss=44.5938, LR=2.5e-03, kappa=5.670, |gamma|=0.1056, time=12.7min
Epoch  470: Loss=44.5745, LR=1.8e-03, kappa=5.672, |gamma|=0.1056, time=13.0min
Epoch  480: Loss=44.5583, LR=1.4e-03, kappa=5.674, |gamma|=0.1055, time=13.3min
Epoch  490: Loss=44.5441, LR=1.1e-03, kappa=5.676, |gamma|=0.1054, time=13.6min

Training complete: 500 epochs in 13.8 min
Final loss: 44.5324

Final params: kappa=5.6766, mean|gamma|=0.1054
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2/enrollment_model_REPARAM_W0.0001_batch_80000_90000.pt

Batch 9 DONE in 14.0 min (8/39 completed)

======================================================================
BATCH 10/40: samples 90000-100000
Elapsed: 113 min | Est remaining: ~465 min (7.8 hrs)
======================================================================

============================================================
REPARAM v2 Training: samples 90000 to 100000
Epochs: 500, LR: 0.1, grad_clip: 5.0
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 7 diseases
Cluster 1: 20 diseases
Cluster 2: 8 diseases
Cluster 3: 19 diseases
Cluster 4: 7 diseases
Cluster 5: 22 diseases
Cluster 6: 12 diseases
Cluster 7: 15 diseases
Cluster 8: 9 diseases
Cluster 9: 5 diseases
Cluster 10: 9 diseases
Cluster 11: 12 diseases
Cluster 12: 26 diseases
Cluster 13: 15 diseases
Cluster 14: 5 diseases
Cluster 15: 13 diseases
Cluster 16: 8 diseases
Cluster 17: 22 diseases
Cluster 18: 20 diseases
Cluster 19: 94 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=63.9898, LR=1.0e-01, kappa=1.100, |gamma|=0.1011, time=0.0min
Epoch   10: Loss=137.6443, LR=1.0e-01, kappa=2.025, |gamma|=0.0936, time=0.3min
Epoch   20: Loss=100.1629, LR=1.0e-01, kappa=2.666, |gamma|=0.1247, time=0.6min
Epoch   30: Loss=72.8729, LR=9.9e-02, kappa=3.004, |gamma|=0.1400, time=0.8min
Epoch   40: Loss=63.1132, LR=9.8e-02, kappa=3.138, |gamma|=0.1444, time=1.1min
Epoch   50: Loss=60.0581, LR=9.7e-02, kappa=3.171, |gamma|=0.1447, time=1.4min
Epoch   60: Loss=58.7055, LR=9.6e-02, kappa=3.173, |gamma|=0.1439, time=1.7min
Epoch   70: Loss=57.9142, LR=9.5e-02, kappa=3.180, |gamma|=0.1421, time=1.9min
Epoch   80: Loss=57.4443, LR=9.4e-02, kappa=3.204, |gamma|=0.1415, time=2.2min
Epoch   90: Loss=57.0543, LR=9.2e-02, kappa=3.244, |gamma|=0.1412, time=2.5min
Epoch  100: Loss=56.6029, LR=9.0e-02, kappa=3.301, |gamma|=0.1408, time=2.8min
Epoch  110: Loss=56.1030, LR=8.8e-02, kappa=3.373, |gamma|=0.1400, time=3.1min
Epoch  120: Loss=55.5435, LR=8.6e-02, kappa=3.463, |gamma|=0.1381, time=3.3min
Epoch  130: Loss=54.9085, LR=8.4e-02, kappa=3.574, |gamma|=0.1365, time=3.6min
Epoch  140: Loss=54.2152, LR=8.2e-02, kappa=3.704, |gamma|=0.1344, time=3.9min
Epoch  150: Loss=53.4883, LR=7.9e-02, kappa=3.850, |gamma|=0.1327, time=4.2min
Epoch  160: Loss=52.7362, LR=7.7e-02, kappa=4.006, |gamma|=0.1289, time=4.4min
Epoch  170: Loss=51.9955, LR=7.4e-02, kappa=4.167, |gamma|=0.1269, time=4.7min
Epoch  180: Loss=51.2675, LR=7.1e-02, kappa=4.327, |gamma|=0.1244, time=5.0min
Epoch  190: Loss=50.5884, LR=6.8e-02, kappa=4.483, |gamma|=0.1234, time=5.3min
Epoch  200: Loss=49.9533, LR=6.6e-02, kappa=4.630, |gamma|=0.1195, time=5.5min
Epoch  210: Loss=49.3801, LR=6.3e-02, kappa=4.765, |gamma|=0.1177, time=5.8min
Epoch  220: Loss=48.8640, LR=5.9e-02, kappa=4.887, |gamma|=0.1171, time=6.1min
Epoch  230: Loss=48.4022, LR=5.6e-02, kappa=4.997, |gamma|=0.1167, time=6.4min
Epoch  240: Loss=47.9892, LR=5.3e-02, kappa=5.094, |gamma|=0.1163, time=6.6min
Epoch  250: Loss=47.6193, LR=5.0e-02, kappa=5.181, |gamma|=0.1157, time=6.9min
Epoch  260: Loss=47.2872, LR=4.7e-02, kappa=5.257, |gamma|=0.1151, time=7.2min
Epoch  270: Loss=46.9891, LR=4.4e-02, kappa=5.325, |gamma|=0.1144, time=7.5min
Epoch  280: Loss=46.7210, LR=4.1e-02, kappa=5.385, |gamma|=0.1138, time=7.7min
Epoch  290: Loss=46.4798, LR=3.8e-02, kappa=5.438, |gamma|=0.1131, time=8.0min
Epoch  300: Loss=46.2628, LR=3.5e-02, kappa=5.486, |gamma|=0.1125, time=8.3min
Epoch  310: Loss=46.0678, LR=3.2e-02, kappa=5.527, |gamma|=0.1120, time=8.6min
Epoch  320: Loss=45.8927, LR=2.9e-02, kappa=5.564, |gamma|=0.1116, time=8.8min
Epoch  330: Loss=45.7358, LR=2.6e-02, kappa=5.597, |gamma|=0.1113, time=9.1min
Epoch  340: Loss=45.5956, LR=2.4e-02, kappa=5.626, |gamma|=0.1110, time=9.4min
Epoch  350: Loss=45.4708, LR=2.1e-02, kappa=5.651, |gamma|=0.1108, time=9.7min
Epoch  360: Loss=45.3600, LR=1.9e-02, kappa=5.673, |gamma|=0.1106, time=10.0min
Epoch  370: Loss=45.2623, LR=1.6e-02, kappa=5.692, |gamma|=0.1104, time=10.3min
Epoch  380: Loss=45.1765, LR=1.4e-02, kappa=5.709, |gamma|=0.1103, time=10.5min
Epoch  390: Loss=45.1016, LR=1.2e-02, kappa=5.723, |gamma|=0.1101, time=10.8min
Epoch  400: Loss=45.0368, LR=1.0e-02, kappa=5.735, |gamma|=0.1100, time=11.1min
Epoch  410: Loss=44.9812, LR=8.5e-03, kappa=5.745, |gamma|=0.1099, time=11.4min
Epoch  420: Loss=44.9338, LR=7.0e-03, kappa=5.753, |gamma|=0.1098, time=11.7min
Epoch  430: Loss=44.8937, LR=5.6e-03, kappa=5.760, |gamma|=0.1097, time=11.9min
Epoch  440: Loss=44.8602, LR=4.4e-03, kappa=5.765, |gamma|=0.1096, time=12.2min
Epoch  450: Loss=44.8324, LR=3.3e-03, kappa=5.769, |gamma|=0.1096, time=12.5min
Epoch  460: Loss=44.8094, LR=2.5e-03, kappa=5.772, |gamma|=0.1095, time=12.8min
Epoch  470: Loss=44.7903, LR=1.8e-03, kappa=5.775, |gamma|=0.1095, time=13.1min
Epoch  480: Loss=44.7742, LR=1.4e-03, kappa=5.776, |gamma|=0.1094, time=13.3min
Epoch  490: Loss=44.7602, LR=1.1e-03, kappa=5.778, |gamma|=0.1094, time=13.6min

Training complete: 500 epochs in 13.9 min
Final loss: 44.7485

Final params: kappa=5.7785, mean|gamma|=0.1094
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2/enrollment_model_REPARAM_W0.0001_batch_90000_100000.pt

Batch 10 DONE in 14.0 min (9/39 completed)

======================================================================
BATCH 11/40: samples 100000-110000
Elapsed: 127 min | Est remaining: ~450 min (7.5 hrs)
======================================================================

============================================================
REPARAM v2 Training: samples 100000 to 110000
Epochs: 500, LR: 0.1, grad_clip: 5.0
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 42 diseases
Cluster 1: 24 diseases
Cluster 2: 8 diseases
Cluster 3: 5 diseases
Cluster 4: 8 diseases
Cluster 5: 13 diseases
Cluster 6: 60 diseases
Cluster 7: 30 diseases
Cluster 8: 7 diseases
Cluster 9: 19 diseases
Cluster 10: 10 diseases
Cluster 11: 15 diseases
Cluster 12: 16 diseases
Cluster 13: 11 diseases
Cluster 14: 7 diseases
Cluster 15: 7 diseases
Cluster 16: 12 diseases
Cluster 17: 16 diseases
Cluster 18: 22 diseases
Cluster 19: 16 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=63.1875, LR=1.0e-01, kappa=1.100, |gamma|=0.1010, time=0.0min
Epoch   10: Loss=139.2649, LR=1.0e-01, kappa=2.022, |gamma|=0.0944, time=0.3min
Epoch   20: Loss=100.1653, LR=1.0e-01, kappa=2.654, |gamma|=0.1315, time=0.6min
Epoch   30: Loss=72.0995, LR=9.9e-02, kappa=2.979, |gamma|=0.1517, time=0.8min
Epoch   40: Loss=62.3567, LR=9.8e-02, kappa=3.098, |gamma|=0.1540, time=1.1min
Epoch   50: Loss=59.3955, LR=9.7e-02, kappa=3.115, |gamma|=0.1540, time=1.4min
Epoch   60: Loss=58.1056, LR=9.6e-02, kappa=3.105, |gamma|=0.1544, time=1.7min
Epoch   70: Loss=57.3381, LR=9.5e-02, kappa=3.103, |gamma|=0.1555, time=1.9min
Epoch   80: Loss=56.8798, LR=9.4e-02, kappa=3.117, |gamma|=0.1565, time=2.2min
Epoch   90: Loss=56.5209, LR=9.2e-02, kappa=3.145, |gamma|=0.1578, time=2.5min
Epoch  100: Loss=56.1146, LR=9.0e-02, kappa=3.181, |gamma|=0.1575, time=2.8min
Epoch  110: Loss=55.6766, LR=8.8e-02, kappa=3.228, |gamma|=0.1562, time=3.0min
Epoch  120: Loss=55.1891, LR=8.6e-02, kappa=3.288, |gamma|=0.1545, time=3.3min
Epoch  130: Loss=54.6414, LR=8.4e-02, kappa=3.367, |gamma|=0.1538, time=3.6min
Epoch  140: Loss=54.0433, LR=8.2e-02, kappa=3.464, |gamma|=0.1528, time=3.9min
Epoch  150: Loss=53.3884, LR=7.9e-02, kappa=3.577, |gamma|=0.1510, time=4.1min
Epoch  160: Loss=52.7074, LR=7.7e-02, kappa=3.705, |gamma|=0.1491, time=4.4min
Epoch  170: Loss=52.0142, LR=7.4e-02, kappa=3.844, |gamma|=0.1474, time=4.7min
Epoch  180: Loss=51.3380, LR=7.1e-02, kappa=3.987, |gamma|=0.1462, time=5.0min
Epoch  190: Loss=50.6867, LR=6.8e-02, kappa=4.128, |gamma|=0.1448, time=5.2min
Epoch  200: Loss=50.0663, LR=6.6e-02, kappa=4.266, |gamma|=0.1434, time=5.5min
Epoch  210: Loss=49.4993, LR=6.3e-02, kappa=4.396, |gamma|=0.1424, time=5.8min
Epoch  220: Loss=48.9790, LR=5.9e-02, kappa=4.516, |gamma|=0.1403, time=6.1min
Epoch  230: Loss=48.5079, LR=5.6e-02, kappa=4.626, |gamma|=0.1394, time=6.3min
Epoch  240: Loss=48.0838, LR=5.3e-02, kappa=4.725, |gamma|=0.1386, time=6.6min
Epoch  250: Loss=47.7030, LR=5.0e-02, kappa=4.815, |gamma|=0.1378, time=6.9min
Epoch  260: Loss=47.3609, LR=4.7e-02, kappa=4.895, |gamma|=0.1371, time=7.2min
Epoch  270: Loss=47.0531, LR=4.4e-02, kappa=4.967, |gamma|=0.1366, time=7.4min
Epoch  280: Loss=46.7762, LR=4.1e-02, kappa=5.031, |gamma|=0.1362, time=7.7min
Epoch  290: Loss=46.5269, LR=3.8e-02, kappa=5.088, |gamma|=0.1360, time=8.0min
Epoch  300: Loss=46.3025, LR=3.5e-02, kappa=5.139, |gamma|=0.1358, time=8.3min
Epoch  310: Loss=46.1010, LR=3.2e-02, kappa=5.184, |gamma|=0.1357, time=8.6min
Epoch  320: Loss=45.9204, LR=2.9e-02, kappa=5.224, |gamma|=0.1354, time=8.8min
Epoch  330: Loss=45.7588, LR=2.6e-02, kappa=5.259, |gamma|=0.1350, time=9.1min
Epoch  340: Loss=45.6146, LR=2.4e-02, kappa=5.291, |gamma|=0.1345, time=9.4min
Epoch  350: Loss=45.4864, LR=2.1e-02, kappa=5.318, |gamma|=0.1340, time=9.7min
Epoch  360: Loss=45.3729, LR=1.9e-02, kappa=5.343, |gamma|=0.1336, time=10.0min
Epoch  370: Loss=45.2727, LR=1.6e-02, kappa=5.364, |gamma|=0.1331, time=10.3min
Epoch  380: Loss=45.1849, LR=1.4e-02, kappa=5.382, |gamma|=0.1327, time=10.5min
Epoch  390: Loss=45.1083, LR=1.2e-02, kappa=5.398, |gamma|=0.1324, time=10.8min
Epoch  400: Loss=45.0421, LR=1.0e-02, kappa=5.411, |gamma|=0.1320, time=11.1min
Epoch  410: Loss=44.9852, LR=8.5e-03, kappa=5.422, |gamma|=0.1317, time=11.4min
Epoch  420: Loss=44.9368, LR=7.0e-03, kappa=5.431, |gamma|=0.1315, time=11.6min
Epoch  430: Loss=44.8959, LR=5.6e-03, kappa=5.439, |gamma|=0.1312, time=11.9min
Epoch  440: Loss=44.8618, LR=4.4e-03, kappa=5.445, |gamma|=0.1310, time=12.2min
Epoch  450: Loss=44.8334, LR=3.3e-03, kappa=5.449, |gamma|=0.1309, time=12.5min
Epoch  460: Loss=44.8099, LR=2.5e-03, kappa=5.453, |gamma|=0.1307, time=12.7min
Epoch  470: Loss=44.7904, LR=1.8e-03, kappa=5.455, |gamma|=0.1306, time=13.0min
Epoch  480: Loss=44.7740, LR=1.4e-03, kappa=5.457, |gamma|=0.1305, time=13.3min
Epoch  490: Loss=44.7597, LR=1.1e-03, kappa=5.459, |gamma|=0.1304, time=13.6min

Training complete: 500 epochs in 13.9 min
Final loss: 44.7478

Final params: kappa=5.4597, mean|gamma|=0.1304
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2/enrollment_model_REPARAM_W0.0001_batch_100000_110000.pt

Batch 11 DONE in 14.0 min (10/39 completed)

======================================================================
BATCH 12/40: samples 110000-120000
Elapsed: 141 min | Est remaining: ~435 min (7.2 hrs)
======================================================================

============================================================
REPARAM v2 Training: samples 110000 to 120000
Epochs: 500, LR: 0.1, grad_clip: 5.0
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 11 diseases
Cluster 1: 27 diseases
Cluster 2: 9 diseases
Cluster 3: 22 diseases
Cluster 4: 13 diseases
Cluster 5: 4 diseases
Cluster 6: 13 diseases
Cluster 7: 8 diseases
Cluster 8: 15 diseases
Cluster 9: 13 diseases
Cluster 10: 27 diseases
Cluster 11: 6 diseases
Cluster 12: 12 diseases
Cluster 13: 16 diseases
Cluster 14: 9 diseases
Cluster 15: 28 diseases
Cluster 16: 8 diseases
Cluster 17: 15 diseases
Cluster 18: 87 diseases
Cluster 19: 5 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=62.7846, LR=1.0e-01, kappa=1.100, |gamma|=0.1011, time=0.0min
Epoch   10: Loss=140.7250, LR=1.0e-01, kappa=2.020, |gamma|=0.0974, time=0.3min
Epoch   20: Loss=100.3045, LR=1.0e-01, kappa=2.648, |gamma|=0.1335, time=0.6min
Epoch   30: Loss=71.6146, LR=9.9e-02, kappa=2.971, |gamma|=0.1531, time=0.9min
Epoch   40: Loss=61.9055, LR=9.8e-02, kappa=3.089, |gamma|=0.1584, time=1.2min
Epoch   50: Loss=59.0186, LR=9.7e-02, kappa=3.103, |gamma|=0.1562, time=1.4min
Epoch   60: Loss=57.7733, LR=9.6e-02, kappa=3.090, |gamma|=0.1529, time=1.7min
Epoch   70: Loss=57.0206, LR=9.5e-02, kappa=3.087, |gamma|=0.1532, time=2.0min
Epoch   80: Loss=56.5616, LR=9.4e-02, kappa=3.100, |gamma|=0.1526, time=2.3min
Epoch   90: Loss=56.2130, LR=9.2e-02, kappa=3.127, |gamma|=0.1534, time=2.6min
Epoch  100: Loss=55.8246, LR=9.0e-02, kappa=3.162, |gamma|=0.1534, time=2.9min
Epoch  110: Loss=55.4021, LR=8.8e-02, kappa=3.208, |gamma|=0.1538, time=3.2min
Epoch  120: Loss=54.9265, LR=8.6e-02, kappa=3.268, |gamma|=0.1522, time=3.5min
Epoch  130: Loss=54.3924, LR=8.4e-02, kappa=3.349, |gamma|=0.1496, time=3.7min
Epoch  140: Loss=53.8094, LR=8.2e-02, kappa=3.452, |gamma|=0.1470, time=4.0min
Epoch  150: Loss=53.1461, LR=7.9e-02, kappa=3.578, |gamma|=0.1441, time=4.3min
Epoch  160: Loss=52.4455, LR=7.7e-02, kappa=3.722, |gamma|=0.1415, time=4.6min
Epoch  170: Loss=51.7228, LR=7.4e-02, kappa=3.879, |gamma|=0.1380, time=4.9min
Epoch  180: Loss=51.0046, LR=7.1e-02, kappa=4.042, |gamma|=0.1354, time=5.2min
Epoch  190: Loss=50.2981, LR=6.8e-02, kappa=4.203, |gamma|=0.1331, time=5.5min
Epoch  200: Loss=49.6321, LR=6.6e-02, kappa=4.360, |gamma|=0.1311, time=5.8min
Epoch  210: Loss=49.0170, LR=6.3e-02, kappa=4.510, |gamma|=0.1301, time=6.1min
Epoch  220: Loss=48.4574, LR=5.9e-02, kappa=4.648, |gamma|=0.1285, time=6.4min
Epoch  230: Loss=47.9549, LR=5.6e-02, kappa=4.775, |gamma|=0.1272, time=6.6min
Epoch  240: Loss=47.4964, LR=5.3e-02, kappa=4.889, |gamma|=0.1256, time=6.9min
Epoch  250: Loss=47.0872, LR=5.0e-02, kappa=4.991, |gamma|=0.1247, time=7.2min
Epoch  260: Loss=46.7230, LR=4.7e-02, kappa=5.082, |gamma|=0.1239, time=7.5min
Epoch  270: Loss=46.3982, LR=4.4e-02, kappa=5.162, |gamma|=0.1232, time=7.8min
Epoch  280: Loss=46.1088, LR=4.1e-02, kappa=5.234, |gamma|=0.1225, time=8.1min
Epoch  290: Loss=45.8507, LR=3.8e-02, kappa=5.297, |gamma|=0.1219, time=8.4min
Epoch  300: Loss=45.6202, LR=3.5e-02, kappa=5.352, |gamma|=0.1214, time=8.7min
Epoch  310: Loss=45.4145, LR=3.2e-02, kappa=5.402, |gamma|=0.1210, time=9.0min
Epoch  320: Loss=45.2309, LR=2.9e-02, kappa=5.445, |gamma|=0.1206, time=9.3min
Epoch  330: Loss=45.0672, LR=2.6e-02, kappa=5.483, |gamma|=0.1202, time=9.5min
Epoch  340: Loss=44.9213, LR=2.4e-02, kappa=5.517, |gamma|=0.1200, time=9.8min
Epoch  350: Loss=44.7919, LR=2.1e-02, kappa=5.546, |gamma|=0.1198, time=10.1min
Epoch  360: Loss=44.6773, LR=1.9e-02, kappa=5.572, |gamma|=0.1196, time=10.4min
Epoch  370: Loss=44.5765, LR=1.6e-02, kappa=5.594, |gamma|=0.1194, time=10.6min
Epoch  380: Loss=44.4881, LR=1.4e-02, kappa=5.613, |gamma|=0.1192, time=10.9min
Epoch  390: Loss=44.4111, LR=1.2e-02, kappa=5.629, |gamma|=0.1190, time=11.2min
Epoch  400: Loss=44.3446, LR=1.0e-02, kappa=5.643, |gamma|=0.1189, time=11.5min
Epoch  410: Loss=44.2875, LR=8.5e-03, kappa=5.654, |gamma|=0.1187, time=11.8min
Epoch  420: Loss=44.2388, LR=7.0e-03, kappa=5.664, |gamma|=0.1186, time=12.0min
Epoch  430: Loss=44.1979, LR=5.6e-03, kappa=5.672, |gamma|=0.1185, time=12.3min
Epoch  440: Loss=44.1636, LR=4.4e-03, kappa=5.678, |gamma|=0.1184, time=12.6min
Epoch  450: Loss=44.1352, LR=3.3e-03, kappa=5.682, |gamma|=0.1183, time=12.9min
Epoch  460: Loss=44.1117, LR=2.5e-03, kappa=5.686, |gamma|=0.1182, time=13.1min
Epoch  470: Loss=44.0923, LR=1.8e-03, kappa=5.688, |gamma|=0.1181, time=13.4min
Epoch  480: Loss=44.0760, LR=1.4e-03, kappa=5.690, |gamma|=0.1181, time=13.7min
Epoch  490: Loss=44.0617, LR=1.1e-03, kappa=5.692, |gamma|=0.1180, time=14.0min

Training complete: 500 epochs in 14.2 min
Final loss: 44.0499

Final params: kappa=5.6929, mean|gamma|=0.1179
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2/enrollment_model_REPARAM_W0.0001_batch_110000_120000.pt

Batch 12 DONE in 14.3 min (11/39 completed)

======================================================================
BATCH 13/40: samples 120000-130000
Elapsed: 156 min | Est remaining: ~420 min (7.0 hrs)
======================================================================

============================================================
REPARAM v2 Training: samples 120000 to 130000
Epochs: 500, LR: 0.1, grad_clip: 5.0
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 11 diseases
Cluster 1: 6 diseases
Cluster 2: 5 diseases
Cluster 3: 18 diseases
Cluster 4: 8 diseases
Cluster 5: 6 diseases
Cluster 6: 26 diseases
Cluster 7: 97 diseases
Cluster 8: 27 diseases
Cluster 9: 12 diseases
Cluster 10: 14 diseases
Cluster 11: 15 diseases
Cluster 12: 14 diseases
Cluster 13: 6 diseases
Cluster 14: 5 diseases
Cluster 15: 18 diseases
Cluster 16: 7 diseases
Cluster 17: 9 diseases
Cluster 18: 15 diseases
Cluster 19: 29 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=63.3996, LR=1.0e-01, kappa=1.100, |gamma|=0.1009, time=0.0min
Epoch   10: Loss=138.5725, LR=1.0e-01, kappa=2.023, |gamma|=0.0926, time=0.3min
Epoch   20: Loss=100.0889, LR=1.0e-01, kappa=2.659, |gamma|=0.1279, time=0.6min
Epoch   30: Loss=72.3080, LR=9.9e-02, kappa=2.991, |gamma|=0.1468, time=0.8min
Epoch   40: Loss=62.5580, LR=9.8e-02, kappa=3.116, |gamma|=0.1552, time=1.1min
Epoch   50: Loss=59.5652, LR=9.7e-02, kappa=3.135, |gamma|=0.1561, time=1.4min
Epoch   60: Loss=58.2538, LR=9.6e-02, kappa=3.123, |gamma|=0.1580, time=1.7min
Epoch   70: Loss=57.4800, LR=9.5e-02, kappa=3.118, |gamma|=0.1580, time=2.0min
Epoch   80: Loss=57.0229, LR=9.4e-02, kappa=3.126, |gamma|=0.1580, time=2.2min
Epoch   90: Loss=56.6675, LR=9.2e-02, kappa=3.147, |gamma|=0.1579, time=2.5min
Epoch  100: Loss=56.2590, LR=9.0e-02, kappa=3.175, |gamma|=0.1595, time=2.8min
Epoch  110: Loss=55.8220, LR=8.8e-02, kappa=3.213, |gamma|=0.1588, time=3.1min
Epoch  120: Loss=55.3392, LR=8.6e-02, kappa=3.266, |gamma|=0.1560, time=3.4min
Epoch  130: Loss=54.8013, LR=8.4e-02, kappa=3.337, |gamma|=0.1531, time=3.6min
Epoch  140: Loss=54.2123, LR=8.2e-02, kappa=3.428, |gamma|=0.1505, time=3.9min
Epoch  150: Loss=53.5787, LR=7.9e-02, kappa=3.537, |gamma|=0.1494, time=4.2min
Epoch  160: Loss=52.9048, LR=7.7e-02, kappa=3.660, |gamma|=0.1477, time=4.5min
Epoch  170: Loss=52.2192, LR=7.4e-02, kappa=3.794, |gamma|=0.1470, time=4.7min
Epoch  180: Loss=51.5386, LR=7.1e-02, kappa=3.934, |gamma|=0.1462, time=5.0min
Epoch  190: Loss=50.8803, LR=6.8e-02, kappa=4.073, |gamma|=0.1463, time=5.3min
Epoch  200: Loss=50.2633, LR=6.6e-02, kappa=4.207, |gamma|=0.1460, time=5.6min
Epoch  210: Loss=49.6912, LR=6.3e-02, kappa=4.334, |gamma|=0.1461, time=5.9min
Epoch  220: Loss=49.1722, LR=5.9e-02, kappa=4.451, |gamma|=0.1462, time=6.1min
Epoch  230: Loss=48.6964, LR=5.6e-02, kappa=4.558, |gamma|=0.1462, time=6.4min
Epoch  240: Loss=48.2698, LR=5.3e-02, kappa=4.655, |gamma|=0.1463, time=6.7min
Epoch  250: Loss=47.8856, LR=5.0e-02, kappa=4.742, |gamma|=0.1464, time=7.0min
Epoch  260: Loss=47.5408, LR=4.7e-02, kappa=4.820, |gamma|=0.1464, time=7.2min
Epoch  270: Loss=47.2314, LR=4.4e-02, kappa=4.890, |gamma|=0.1465, time=7.5min
Epoch  280: Loss=46.9539, LR=4.1e-02, kappa=4.952, |gamma|=0.1466, time=7.8min
Epoch  290: Loss=46.7049, LR=3.8e-02, kappa=5.007, |gamma|=0.1468, time=8.1min
Epoch  300: Loss=46.4816, LR=3.5e-02, kappa=5.055, |gamma|=0.1470, time=8.4min
Epoch  310: Loss=46.2816, LR=3.2e-02, kappa=5.099, |gamma|=0.1472, time=8.6min
Epoch  320: Loss=46.1025, LR=2.9e-02, kappa=5.137, |gamma|=0.1472, time=8.9min
Epoch  330: Loss=45.9426, LR=2.6e-02, kappa=5.171, |gamma|=0.1472, time=9.2min
Epoch  340: Loss=45.7999, LR=2.4e-02, kappa=5.200, |gamma|=0.1471, time=9.5min
Epoch  350: Loss=45.6731, LR=2.1e-02, kappa=5.226, |gamma|=0.1470, time=9.7min
Epoch  360: Loss=45.5607, LR=1.9e-02, kappa=5.249, |gamma|=0.1469, time=10.0min
Epoch  370: Loss=45.4616, LR=1.6e-02, kappa=5.269, |gamma|=0.1468, time=10.3min
Epoch  380: Loss=45.3746, LR=1.4e-02, kappa=5.286, |gamma|=0.1468, time=10.6min
Epoch  390: Loss=45.2988, LR=1.2e-02, kappa=5.301, |gamma|=0.1467, time=10.8min
Epoch  400: Loss=45.2331, LR=1.0e-02, kappa=5.313, |gamma|=0.1466, time=11.1min
Epoch  410: Loss=45.1766, LR=8.5e-03, kappa=5.323, |gamma|=0.1464, time=11.4min
Epoch  420: Loss=45.1286, LR=7.0e-03, kappa=5.332, |gamma|=0.1463, time=11.7min
Epoch  430: Loss=45.0879, LR=5.6e-03, kappa=5.339, |gamma|=0.1463, time=11.9min
Epoch  440: Loss=45.0540, LR=4.4e-03, kappa=5.344, |gamma|=0.1462, time=12.2min
Epoch  450: Loss=45.0257, LR=3.3e-03, kappa=5.348, |gamma|=0.1461, time=12.5min
Epoch  460: Loss=45.0024, LR=2.5e-03, kappa=5.351, |gamma|=0.1460, time=12.8min
Epoch  470: Loss=44.9830, LR=1.8e-03, kappa=5.354, |gamma|=0.1460, time=13.1min
Epoch  480: Loss=44.9666, LR=1.4e-03, kappa=5.356, |gamma|=0.1459, time=13.3min
Epoch  490: Loss=44.9524, LR=1.1e-03, kappa=5.357, |gamma|=0.1459, time=13.6min

Training complete: 500 epochs in 13.9 min
Final loss: 44.9405

Final params: kappa=5.3579, mean|gamma|=0.1459
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2/enrollment_model_REPARAM_W0.0001_batch_120000_130000.pt

Batch 13 DONE in 14.0 min (12/39 completed)

======================================================================
BATCH 14/40: samples 130000-140000
Elapsed: 170 min | Est remaining: ~405 min (6.8 hrs)
======================================================================

============================================================
REPARAM v2 Training: samples 130000 to 140000
Epochs: 500, LR: 0.1, grad_clip: 5.0
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 13 diseases
Cluster 1: 21 diseases
Cluster 2: 16 diseases
Cluster 3: 13 diseases
Cluster 4: 13 diseases
Cluster 5: 6 diseases
Cluster 6: 9 diseases
Cluster 7: 87 diseases
Cluster 8: 8 diseases
Cluster 9: 11 diseases
Cluster 10: 12 diseases
Cluster 11: 25 diseases
Cluster 12: 11 diseases
Cluster 13: 5 diseases
Cluster 14: 8 diseases
Cluster 15: 31 diseases
Cluster 16: 18 diseases
Cluster 17: 27 diseases
Cluster 18: 7 diseases
Cluster 19: 7 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=63.6466, LR=1.0e-01, kappa=1.100, |gamma|=0.1009, time=0.0min
Epoch   10: Loss=139.3870, LR=1.0e-01, kappa=2.022, |gamma|=0.0965, time=0.3min
Epoch   20: Loss=100.5080, LR=1.0e-01, kappa=2.654, |gamma|=0.1328, time=0.6min
Epoch   30: Loss=72.5327, LR=9.9e-02, kappa=2.979, |gamma|=0.1563, time=0.8min
Epoch   40: Loss=62.7875, LR=9.8e-02, kappa=3.097, |gamma|=0.1614, time=1.1min
Epoch   50: Loss=59.8129, LR=9.7e-02, kappa=3.110, |gamma|=0.1606, time=1.4min
Epoch   60: Loss=58.5133, LR=9.6e-02, kappa=3.094, |gamma|=0.1608, time=1.7min
Epoch   70: Loss=57.7421, LR=9.5e-02, kappa=3.086, |gamma|=0.1602, time=1.9min
Epoch   80: Loss=57.2830, LR=9.4e-02, kappa=3.095, |gamma|=0.1619, time=2.2min
Epoch   90: Loss=56.9234, LR=9.2e-02, kappa=3.116, |gamma|=0.1634, time=2.5min
Epoch  100: Loss=56.5187, LR=9.0e-02, kappa=3.146, |gamma|=0.1648, time=2.8min
Epoch  110: Loss=56.0854, LR=8.8e-02, kappa=3.185, |gamma|=0.1644, time=3.0min
Epoch  120: Loss=55.5934, LR=8.6e-02, kappa=3.239, |gamma|=0.1629, time=3.3min
Epoch  130: Loss=55.0499, LR=8.4e-02, kappa=3.313, |gamma|=0.1610, time=3.6min
Epoch  140: Loss=54.4377, LR=8.2e-02, kappa=3.408, |gamma|=0.1591, time=3.8min
Epoch  150: Loss=53.7719, LR=7.9e-02, kappa=3.524, |gamma|=0.1577, time=4.1min
Epoch  160: Loss=53.0639, LR=7.7e-02, kappa=3.656, |gamma|=0.1561, time=4.4min
Epoch  170: Loss=52.3421, LR=7.4e-02, kappa=3.800, |gamma|=0.1550, time=4.7min
Epoch  180: Loss=51.6270, LR=7.1e-02, kappa=3.949, |gamma|=0.1534, time=4.9min
Epoch  190: Loss=50.9407, LR=6.8e-02, kappa=4.097, |gamma|=0.1527, time=5.2min
Epoch  200: Loss=50.2921, LR=6.6e-02, kappa=4.240, |gamma|=0.1514, time=5.5min
Epoch  210: Loss=49.6940, LR=6.3e-02, kappa=4.373, |gamma|=0.1499, time=5.8min
Epoch  220: Loss=49.1488, LR=5.9e-02, kappa=4.496, |gamma|=0.1492, time=6.0min
Epoch  230: Loss=48.6582, LR=5.6e-02, kappa=4.607, |gamma|=0.1484, time=6.3min
Epoch  240: Loss=48.2190, LR=5.3e-02, kappa=4.706, |gamma|=0.1474, time=6.6min
Epoch  250: Loss=47.8258, LR=5.0e-02, kappa=4.796, |gamma|=0.1466, time=6.9min
Epoch  260: Loss=47.4743, LR=4.7e-02, kappa=4.875, |gamma|=0.1458, time=7.1min
Epoch  270: Loss=47.1598, LR=4.4e-02, kappa=4.946, |gamma|=0.1449, time=7.4min
Epoch  280: Loss=46.8782, LR=4.1e-02, kappa=5.009, |gamma|=0.1439, time=7.7min
Epoch  290: Loss=46.6261, LR=3.8e-02, kappa=5.064, |gamma|=0.1429, time=7.9min
Epoch  300: Loss=46.4003, LR=3.5e-02, kappa=5.114, |gamma|=0.1420, time=8.2min
Epoch  310: Loss=46.1981, LR=3.2e-02, kappa=5.158, |gamma|=0.1410, time=8.5min
Epoch  320: Loss=46.0174, LR=2.9e-02, kappa=5.197, |gamma|=0.1401, time=8.8min
Epoch  330: Loss=45.8559, LR=2.6e-02, kappa=5.231, |gamma|=0.1393, time=9.0min
Epoch  340: Loss=45.7121, LR=2.4e-02, kappa=5.262, |gamma|=0.1385, time=9.3min
Epoch  350: Loss=45.5843, LR=2.1e-02, kappa=5.289, |gamma|=0.1379, time=9.6min
Epoch  360: Loss=45.4713, LR=1.9e-02, kappa=5.312, |gamma|=0.1374, time=9.9min
Epoch  370: Loss=45.3717, LR=1.6e-02, kappa=5.333, |gamma|=0.1369, time=10.1min
Epoch  380: Loss=45.2845, LR=1.4e-02, kappa=5.351, |gamma|=0.1365, time=10.4min
Epoch  390: Loss=45.2086, LR=1.2e-02, kappa=5.366, |gamma|=0.1361, time=10.7min
Epoch  400: Loss=45.1429, LR=1.0e-02, kappa=5.379, |gamma|=0.1358, time=11.0min
Epoch  410: Loss=45.0866, LR=8.5e-03, kappa=5.390, |gamma|=0.1356, time=11.2min
Epoch  420: Loss=45.0388, LR=7.0e-03, kappa=5.399, |gamma|=0.1353, time=11.5min
Epoch  430: Loss=44.9985, LR=5.6e-03, kappa=5.406, |gamma|=0.1351, time=11.8min
Epoch  440: Loss=44.9649, LR=4.4e-03, kappa=5.412, |gamma|=0.1350, time=12.0min
Epoch  450: Loss=44.9370, LR=3.3e-03, kappa=5.416, |gamma|=0.1348, time=12.3min
Epoch  460: Loss=44.9140, LR=2.5e-03, kappa=5.420, |gamma|=0.1347, time=12.6min
Epoch  470: Loss=44.8950, LR=1.8e-03, kappa=5.422, |gamma|=0.1346, time=12.9min
Epoch  480: Loss=44.8790, LR=1.4e-03, kappa=5.424, |gamma|=0.1345, time=13.1min
Epoch  490: Loss=44.8652, LR=1.1e-03, kappa=5.426, |gamma|=0.1344, time=13.4min

Training complete: 500 epochs in 13.7 min
Final loss: 44.8537

Final params: kappa=5.4267, mean|gamma|=0.1344
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2/enrollment_model_REPARAM_W0.0001_batch_130000_140000.pt

Batch 14 DONE in 13.8 min (13/39 completed)

======================================================================
BATCH 15/40: samples 140000-150000
Elapsed: 183 min | Est remaining: ~390 min (6.5 hrs)
======================================================================

============================================================
REPARAM v2 Training: samples 140000 to 150000
Epochs: 500, LR: 0.1, grad_clip: 5.0
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 18 diseases
Cluster 1: 11 diseases
Cluster 2: 20 diseases
Cluster 3: 8 diseases
Cluster 4: 8 diseases
Cluster 5: 5 diseases
Cluster 6: 26 diseases
Cluster 7: 19 diseases
Cluster 8: 5 diseases
Cluster 9: 28 diseases
Cluster 10: 11 diseases
Cluster 11: 9 diseases
Cluster 12: 16 diseases
Cluster 13: 14 diseases
Cluster 14: 14 diseases
Cluster 15: 15 diseases
Cluster 16: 8 diseases
Cluster 17: 95 diseases
Cluster 18: 11 diseases
Cluster 19: 7 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=64.2038, LR=1.0e-01, kappa=1.100, |gamma|=0.1009, time=0.0min
Epoch   10: Loss=137.2151, LR=1.0e-01, kappa=2.026, |gamma|=0.0912, time=0.3min
Epoch   20: Loss=100.1493, LR=1.0e-01, kappa=2.667, |gamma|=0.1321, time=0.6min
Epoch   30: Loss=73.0679, LR=9.9e-02, kappa=3.006, |gamma|=0.1493, time=0.8min
Epoch   40: Loss=63.3072, LR=9.8e-02, kappa=3.137, |gamma|=0.1565, time=1.1min
Epoch   50: Loss=60.2335, LR=9.7e-02, kappa=3.160, |gamma|=0.1599, time=1.4min
Epoch   60: Loss=58.8780, LR=9.6e-02, kappa=3.153, |gamma|=0.1617, time=1.7min
Epoch   70: Loss=58.0977, LR=9.5e-02, kappa=3.151, |gamma|=0.1629, time=1.9min
Epoch   80: Loss=57.6492, LR=9.4e-02, kappa=3.165, |gamma|=0.1641, time=2.2min
Epoch   90: Loss=57.2851, LR=9.2e-02, kappa=3.189, |gamma|=0.1649, time=2.5min
Epoch  100: Loss=56.8724, LR=9.0e-02, kappa=3.221, |gamma|=0.1652, time=2.8min
Epoch  110: Loss=56.4305, LR=8.8e-02, kappa=3.261, |gamma|=0.1652, time=3.0min
Epoch  120: Loss=55.9429, LR=8.6e-02, kappa=3.315, |gamma|=0.1637, time=3.3min
Epoch  130: Loss=55.3913, LR=8.4e-02, kappa=3.387, |gamma|=0.1619, time=3.6min
Epoch  140: Loss=54.7824, LR=8.2e-02, kappa=3.480, |gamma|=0.1595, time=3.9min
Epoch  150: Loss=54.1335, LR=7.9e-02, kappa=3.590, |gamma|=0.1570, time=4.1min
Epoch  160: Loss=53.4376, LR=7.7e-02, kappa=3.717, |gamma|=0.1553, time=4.4min
Epoch  170: Loss=52.7323, LR=7.4e-02, kappa=3.853, |gamma|=0.1539, time=4.7min
Epoch  180: Loss=52.0330, LR=7.1e-02, kappa=3.994, |gamma|=0.1527, time=5.0min
Epoch  190: Loss=51.3566, LR=6.8e-02, kappa=4.136, |gamma|=0.1516, time=5.2min
Epoch  200: Loss=50.7162, LR=6.6e-02, kappa=4.274, |gamma|=0.1513, time=5.5min
Epoch  210: Loss=50.1229, LR=6.3e-02, kappa=4.405, |gamma|=0.1511, time=5.8min
Epoch  220: Loss=49.5811, LR=5.9e-02, kappa=4.527, |gamma|=0.1507, time=6.1min
Epoch  230: Loss=49.0882, LR=5.6e-02, kappa=4.638, |gamma|=0.1501, time=6.3min
Epoch  240: Loss=48.6451, LR=5.3e-02, kappa=4.738, |gamma|=0.1494, time=6.6min
Epoch  250: Loss=48.2469, LR=5.0e-02, kappa=4.828, |gamma|=0.1488, time=6.9min
Epoch  260: Loss=47.8896, LR=4.7e-02, kappa=4.909, |gamma|=0.1481, time=7.2min
Epoch  270: Loss=47.5691, LR=4.4e-02, kappa=4.981, |gamma|=0.1473, time=7.4min
Epoch  280: Loss=47.2816, LR=4.1e-02, kappa=5.045, |gamma|=0.1466, time=7.7min
Epoch  290: Loss=47.0236, LR=3.8e-02, kappa=5.102, |gamma|=0.1459, time=8.0min
Epoch  300: Loss=46.7922, LR=3.5e-02, kappa=5.153, |gamma|=0.1452, time=8.3min
Epoch  310: Loss=46.5847, LR=3.2e-02, kappa=5.198, |gamma|=0.1444, time=8.6min
Epoch  320: Loss=46.3989, LR=2.9e-02, kappa=5.238, |gamma|=0.1437, time=8.8min
Epoch  330: Loss=46.2329, LR=2.6e-02, kappa=5.274, |gamma|=0.1431, time=9.1min
Epoch  340: Loss=46.0850, LR=2.4e-02, kappa=5.306, |gamma|=0.1426, time=9.4min
Epoch  350: Loss=45.9538, LR=2.1e-02, kappa=5.334, |gamma|=0.1421, time=9.7min
Epoch  360: Loss=45.8378, LR=1.9e-02, kappa=5.358, |gamma|=0.1417, time=9.9min
Epoch  370: Loss=45.7357, LR=1.6e-02, kappa=5.379, |gamma|=0.1413, time=10.2min
Epoch  380: Loss=45.6463, LR=1.4e-02, kappa=5.398, |gamma|=0.1410, time=10.5min
Epoch  390: Loss=45.5685, LR=1.2e-02, kappa=5.414, |gamma|=0.1407, time=10.8min
Epoch  400: Loss=45.5012, LR=1.0e-02, kappa=5.427, |gamma|=0.1404, time=11.1min
Epoch  410: Loss=45.4436, LR=8.5e-03, kappa=5.438, |gamma|=0.1402, time=11.4min
Epoch  420: Loss=45.3946, LR=7.0e-03, kappa=5.447, |gamma|=0.1400, time=11.6min
Epoch  430: Loss=45.3533, LR=5.6e-03, kappa=5.455, |gamma|=0.1398, time=11.9min
Epoch  440: Loss=45.3188, LR=4.4e-03, kappa=5.461, |gamma|=0.1397, time=12.2min
Epoch  450: Loss=45.2902, LR=3.3e-03, kappa=5.465, |gamma|=0.1396, time=12.5min
Epoch  460: Loss=45.2666, LR=2.5e-03, kappa=5.469, |gamma|=0.1395, time=12.8min
Epoch  470: Loss=45.2471, LR=1.8e-03, kappa=5.472, |gamma|=0.1394, time=13.1min
Epoch  480: Loss=45.2307, LR=1.4e-03, kappa=5.473, |gamma|=0.1393, time=13.4min
Epoch  490: Loss=45.2164, LR=1.1e-03, kappa=5.475, |gamma|=0.1393, time=13.7min

Training complete: 500 epochs in 14.0 min
Final loss: 45.2045

Final params: kappa=5.4760, mean|gamma|=0.1392
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2/enrollment_model_REPARAM_W0.0001_batch_140000_150000.pt

Batch 15 DONE in 14.2 min (14/39 completed)

======================================================================
BATCH 16/40: samples 150000-160000
Elapsed: 198 min | Est remaining: ~375 min (6.2 hrs)
======================================================================

============================================================
REPARAM v2 Training: samples 150000 to 160000
Epochs: 500, LR: 0.1, grad_clip: 5.0
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 15 diseases
Cluster 1: 18 diseases
Cluster 2: 82 diseases
Cluster 3: 29 diseases
Cluster 4: 9 diseases
Cluster 5: 7 diseases
Cluster 6: 24 diseases
Cluster 7: 11 diseases
Cluster 8: 12 diseases
Cluster 9: 8 diseases
Cluster 10: 31 diseases
Cluster 11: 5 diseases
Cluster 12: 15 diseases
Cluster 13: 21 diseases
Cluster 14: 8 diseases
Cluster 15: 5 diseases
Cluster 16: 10 diseases
Cluster 17: 15 diseases
Cluster 18: 17 diseases
Cluster 19: 6 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=62.6422, LR=1.0e-01, kappa=1.100, |gamma|=0.1008, time=0.0min
Epoch   10: Loss=140.7421, LR=1.0e-01, kappa=2.020, |gamma|=0.0953, time=0.3min
Epoch   20: Loss=100.2283, LR=1.0e-01, kappa=2.647, |gamma|=0.1333, time=0.6min
Epoch   30: Loss=71.5077, LR=9.9e-02, kappa=2.968, |gamma|=0.1540, time=0.9min
Epoch   40: Loss=61.7911, LR=9.8e-02, kappa=3.083, |gamma|=0.1545, time=1.2min
Epoch   50: Loss=58.9034, LR=9.7e-02, kappa=3.100, |gamma|=0.1525, time=1.5min
Epoch   60: Loss=57.6542, LR=9.6e-02, kappa=3.094, |gamma|=0.1531, time=1.8min
Epoch   70: Loss=56.8905, LR=9.5e-02, kappa=3.098, |gamma|=0.1516, time=2.1min
Epoch   80: Loss=56.4163, LR=9.4e-02, kappa=3.119, |gamma|=0.1494, time=2.4min
Epoch   90: Loss=56.0496, LR=9.2e-02, kappa=3.152, |gamma|=0.1492, time=2.7min
Epoch  100: Loss=55.6382, LR=9.0e-02, kappa=3.196, |gamma|=0.1484, time=3.0min
Epoch  110: Loss=55.1914, LR=8.8e-02, kappa=3.251, |gamma|=0.1470, time=3.3min
Epoch  120: Loss=54.6926, LR=8.6e-02, kappa=3.319, |gamma|=0.1454, time=3.6min
Epoch  130: Loss=54.1467, LR=8.4e-02, kappa=3.405, |gamma|=0.1439, time=3.9min
Epoch  140: Loss=53.5299, LR=8.2e-02, kappa=3.510, |gamma|=0.1417, time=4.1min
Epoch  150: Loss=52.8766, LR=7.9e-02, kappa=3.633, |gamma|=0.1404, time=4.4min
Epoch  160: Loss=52.1800, LR=7.7e-02, kappa=3.772, |gamma|=0.1381, time=4.7min
Epoch  170: Loss=51.4684, LR=7.4e-02, kappa=3.921, |gamma|=0.1362, time=5.0min
Epoch  180: Loss=50.7600, LR=7.1e-02, kappa=4.075, |gamma|=0.1340, time=5.3min
Epoch  190: Loss=50.0751, LR=6.8e-02, kappa=4.229, |gamma|=0.1324, time=5.6min
Epoch  200: Loss=49.4304, LR=6.6e-02, kappa=4.377, |gamma|=0.1310, time=5.9min
Epoch  210: Loss=48.8351, LR=6.3e-02, kappa=4.517, |gamma|=0.1297, time=6.2min
Epoch  220: Loss=48.2856, LR=5.9e-02, kappa=4.646, |gamma|=0.1280, time=6.5min
Epoch  230: Loss=47.7904, LR=5.6e-02, kappa=4.764, |gamma|=0.1264, time=6.8min
Epoch  240: Loss=47.3459, LR=5.3e-02, kappa=4.871, |gamma|=0.1248, time=7.1min
Epoch  250: Loss=46.9480, LR=5.0e-02, kappa=4.966, |gamma|=0.1234, time=7.4min
Epoch  260: Loss=46.5926, LR=4.7e-02, kappa=5.051, |gamma|=0.1220, time=7.7min
Epoch  270: Loss=46.2754, LR=4.4e-02, kappa=5.125, |gamma|=0.1209, time=8.0min
Epoch  280: Loss=45.9923, LR=4.1e-02, kappa=5.191, |gamma|=0.1199, time=8.3min
Epoch  290: Loss=45.7395, LR=3.8e-02, kappa=5.250, |gamma|=0.1189, time=8.6min
Epoch  300: Loss=45.5138, LR=3.5e-02, kappa=5.301, |gamma|=0.1181, time=8.9min
Epoch  310: Loss=45.3122, LR=3.2e-02, kappa=5.346, |gamma|=0.1173, time=9.2min
Epoch  320: Loss=45.1324, LR=2.9e-02, kappa=5.386, |gamma|=0.1167, time=9.5min
Epoch  330: Loss=44.9722, LR=2.6e-02, kappa=5.421, |gamma|=0.1162, time=9.8min
Epoch  340: Loss=44.8299, LR=2.4e-02, kappa=5.451, |gamma|=0.1157, time=10.1min
Epoch  350: Loss=44.7037, LR=2.1e-02, kappa=5.478, |gamma|=0.1153, time=10.4min
Epoch  360: Loss=44.5922, LR=1.9e-02, kappa=5.501, |gamma|=0.1150, time=10.7min
Epoch  370: Loss=44.4942, LR=1.6e-02, kappa=5.521, |gamma|=0.1147, time=11.0min
Epoch  380: Loss=44.4085, LR=1.4e-02, kappa=5.539, |gamma|=0.1144, time=11.3min
Epoch  390: Loss=44.3339, LR=1.2e-02, kappa=5.553, |gamma|=0.1141, time=11.6min
Epoch  400: Loss=44.2695, LR=1.0e-02, kappa=5.566, |gamma|=0.1139, time=11.9min
Epoch  410: Loss=44.2143, LR=8.5e-03, kappa=5.576, |gamma|=0.1137, time=12.2min
Epoch  420: Loss=44.1674, LR=7.0e-03, kappa=5.585, |gamma|=0.1135, time=12.5min
Epoch  430: Loss=44.1279, LR=5.6e-03, kappa=5.592, |gamma|=0.1134, time=12.8min
Epoch  440: Loss=44.0950, LR=4.4e-03, kappa=5.597, |gamma|=0.1133, time=13.1min
Epoch  450: Loss=44.0678, LR=3.3e-03, kappa=5.601, |gamma|=0.1132, time=13.4min
Epoch  460: Loss=44.0453, LR=2.5e-03, kappa=5.604, |gamma|=0.1131, time=13.7min
Epoch  470: Loss=44.0268, LR=1.8e-03, kappa=5.607, |gamma|=0.1130, time=14.0min
Epoch  480: Loss=44.0112, LR=1.4e-03, kappa=5.609, |gamma|=0.1130, time=14.4min
Epoch  490: Loss=43.9977, LR=1.1e-03, kappa=5.610, |gamma|=0.1129, time=14.7min

Training complete: 500 epochs in 14.9 min
Final loss: 43.9865

Final params: kappa=5.6109, mean|gamma|=0.1129
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2/enrollment_model_REPARAM_W0.0001_batch_150000_160000.pt

Batch 16 DONE in 15.1 min (15/39 completed)

======================================================================
BATCH 17/40: samples 160000-170000
Elapsed: 213 min | Est remaining: ~360 min (6.0 hrs)
======================================================================

============================================================
REPARAM v2 Training: samples 160000 to 170000
Epochs: 500, LR: 0.1, grad_clip: 5.0
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 16 diseases
Cluster 1: 13 diseases
Cluster 2: 6 diseases
Cluster 3: 17 diseases
Cluster 4: 9 diseases
Cluster 5: 24 diseases
Cluster 6: 11 diseases
Cluster 7: 84 diseases
Cluster 8: 6 diseases
Cluster 9: 12 diseases
Cluster 10: 7 diseases
Cluster 11: 14 diseases
Cluster 12: 11 diseases
Cluster 13: 10 diseases
Cluster 14: 39 diseases
Cluster 15: 30 diseases
Cluster 16: 8 diseases
Cluster 17: 4 diseases
Cluster 18: 19 diseases
Cluster 19: 8 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=63.1468, LR=1.0e-01, kappa=1.100, |gamma|=0.1012, time=0.0min
Epoch   10: Loss=139.5347, LR=1.0e-01, kappa=2.023, |gamma|=0.0985, time=0.3min
Epoch   20: Loss=100.1795, LR=1.0e-01, kappa=2.657, |gamma|=0.1338, time=0.6min
Epoch   30: Loss=71.9979, LR=9.9e-02, kappa=2.982, |gamma|=0.1509, time=0.9min
Epoch   40: Loss=62.2615, LR=9.8e-02, kappa=3.095, |gamma|=0.1551, time=1.2min
Epoch   50: Loss=59.3169, LR=9.7e-02, kappa=3.106, |gamma|=0.1594, time=1.5min
Epoch   60: Loss=58.0373, LR=9.6e-02, kappa=3.090, |gamma|=0.1567, time=1.8min
Epoch   70: Loss=57.2754, LR=9.5e-02, kappa=3.084, |gamma|=0.1562, time=2.1min
Epoch   80: Loss=56.8225, LR=9.4e-02, kappa=3.094, |gamma|=0.1571, time=2.4min
Epoch   90: Loss=56.4749, LR=9.2e-02, kappa=3.113, |gamma|=0.1585, time=2.7min
Epoch  100: Loss=56.0874, LR=9.0e-02, kappa=3.142, |gamma|=0.1592, time=3.0min
Epoch  110: Loss=55.6647, LR=8.8e-02, kappa=3.182, |gamma|=0.1588, time=3.3min
Epoch  120: Loss=55.2035, LR=8.6e-02, kappa=3.239, |gamma|=0.1579, time=3.7min
Epoch  130: Loss=54.6710, LR=8.4e-02, kappa=3.315, |gamma|=0.1554, time=4.0min
Epoch  140: Loss=54.0901, LR=8.2e-02, kappa=3.412, |gamma|=0.1535, time=4.4min
Epoch  150: Loss=53.4564, LR=7.9e-02, kappa=3.529, |gamma|=0.1514, time=4.7min
Epoch  160: Loss=52.7801, LR=7.7e-02, kappa=3.662, |gamma|=0.1486, time=5.0min
Epoch  170: Loss=52.0813, LR=7.4e-02, kappa=3.806, |gamma|=0.1462, time=5.3min
Epoch  180: Loss=51.3915, LR=7.1e-02, kappa=3.956, |gamma|=0.1435, time=5.7min
Epoch  190: Loss=50.7219, LR=6.8e-02, kappa=4.105, |gamma|=0.1412, time=6.0min
Epoch  200: Loss=50.0913, LR=6.6e-02, kappa=4.249, |gamma|=0.1389, time=6.3min
Epoch  210: Loss=49.5067, LR=6.3e-02, kappa=4.385, |gamma|=0.1369, time=6.6min
Epoch  220: Loss=48.9661, LR=5.9e-02, kappa=4.511, |gamma|=0.1348, time=6.9min
Epoch  230: Loss=48.4757, LR=5.6e-02, kappa=4.626, |gamma|=0.1335, time=7.2min
Epoch  240: Loss=48.0317, LR=5.3e-02, kappa=4.731, |gamma|=0.1328, time=7.6min
Epoch  250: Loss=47.6312, LR=5.0e-02, kappa=4.827, |gamma|=0.1322, time=7.9min
Epoch  260: Loss=47.2705, LR=4.7e-02, kappa=4.913, |gamma|=0.1317, time=8.2min
Epoch  270: Loss=46.9457, LR=4.4e-02, kappa=4.990, |gamma|=0.1313, time=8.5min
Epoch  280: Loss=46.6536, LR=4.1e-02, kappa=5.059, |gamma|=0.1309, time=8.8min
Epoch  290: Loss=46.3910, LR=3.8e-02, kappa=5.122, |gamma|=0.1305, time=9.1min
Epoch  300: Loss=46.1552, LR=3.5e-02, kappa=5.177, |gamma|=0.1301, time=9.4min
Epoch  310: Loss=45.9436, LR=3.2e-02, kappa=5.227, |gamma|=0.1297, time=9.7min
Epoch  320: Loss=45.7541, LR=2.9e-02, kappa=5.271, |gamma|=0.1293, time=10.1min
Epoch  330: Loss=45.5846, LR=2.6e-02, kappa=5.311, |gamma|=0.1288, time=10.4min
Epoch  340: Loss=45.4334, LR=2.4e-02, kappa=5.346, |gamma|=0.1284, time=10.7min
Epoch  350: Loss=45.2989, LR=2.1e-02, kappa=5.376, |gamma|=0.1279, time=11.0min
Epoch  360: Loss=45.1798, LR=1.9e-02, kappa=5.403, |gamma|=0.1275, time=11.3min
Epoch  370: Loss=45.0748, LR=1.6e-02, kappa=5.427, |gamma|=0.1272, time=11.6min
Epoch  380: Loss=44.9826, LR=1.4e-02, kappa=5.447, |gamma|=0.1268, time=11.9min
Epoch  390: Loss=44.9023, LR=1.2e-02, kappa=5.465, |gamma|=0.1265, time=12.3min
Epoch  400: Loss=44.8329, LR=1.0e-02, kappa=5.480, |gamma|=0.1262, time=12.6min
Epoch  410: Loss=44.7733, LR=8.5e-03, kappa=5.492, |gamma|=0.1260, time=12.9min
Epoch  420: Loss=44.7226, LR=7.0e-03, kappa=5.502, |gamma|=0.1258, time=13.2min
Epoch  430: Loss=44.6799, LR=5.6e-03, kappa=5.511, |gamma|=0.1256, time=13.5min
Epoch  440: Loss=44.6442, LR=4.4e-03, kappa=5.517, |gamma|=0.1254, time=13.8min
Epoch  450: Loss=44.6148, LR=3.3e-03, kappa=5.523, |gamma|=0.1253, time=14.1min
Epoch  460: Loss=44.5905, LR=2.5e-03, kappa=5.526, |gamma|=0.1251, time=14.4min
Epoch  470: Loss=44.5705, LR=1.8e-03, kappa=5.529, |gamma|=0.1250, time=14.7min
Epoch  480: Loss=44.5538, LR=1.4e-03, kappa=5.531, |gamma|=0.1249, time=15.0min
Epoch  490: Loss=44.5393, LR=1.1e-03, kappa=5.533, |gamma|=0.1248, time=15.3min

Training complete: 500 epochs in 15.6 min
Final loss: 44.5273

Final params: kappa=5.5343, mean|gamma|=0.1248
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2/enrollment_model_REPARAM_W0.0001_batch_160000_170000.pt

Batch 17 DONE in 15.8 min (16/39 completed)

======================================================================
BATCH 18/40: samples 170000-180000
Elapsed: 228 min | Est remaining: ~345 min (5.8 hrs)
======================================================================

============================================================
REPARAM v2 Training: samples 170000 to 180000
Epochs: 500, LR: 0.1, grad_clip: 5.0
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 11 diseases
Cluster 1: 27 diseases
Cluster 2: 18 diseases
Cluster 3: 9 diseases
Cluster 4: 29 diseases
Cluster 5: 6 diseases
Cluster 6: 31 diseases
Cluster 7: 5 diseases
Cluster 8: 30 diseases
Cluster 9: 8 diseases
Cluster 10: 12 diseases
Cluster 11: 15 diseases
Cluster 12: 5 diseases
Cluster 13: 11 diseases
Cluster 14: 9 diseases
Cluster 15: 8 diseases
Cluster 16: 6 diseases
Cluster 17: 15 diseases
Cluster 18: 11 diseases
Cluster 19: 82 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=63.4167, LR=1.0e-01, kappa=1.100, |gamma|=0.1005, time=0.0min
Epoch   10: Loss=139.4934, LR=1.0e-01, kappa=2.022, |gamma|=0.0971, time=0.3min
Epoch   20: Loss=100.3787, LR=1.0e-01, kappa=2.657, |gamma|=0.1333, time=0.7min
Epoch   30: Loss=72.3014, LR=9.9e-02, kappa=2.984, |gamma|=0.1500, time=1.0min
Epoch   40: Loss=62.5619, LR=9.8e-02, kappa=3.105, |gamma|=0.1529, time=1.3min
Epoch   50: Loss=59.6013, LR=9.7e-02, kappa=3.121, |gamma|=0.1515, time=1.6min
Epoch   60: Loss=58.3124, LR=9.6e-02, kappa=3.109, |gamma|=0.1505, time=1.9min
Epoch   70: Loss=57.5467, LR=9.5e-02, kappa=3.103, |gamma|=0.1503, time=2.2min
Epoch   80: Loss=57.0912, LR=9.4e-02, kappa=3.113, |gamma|=0.1508, time=2.6min
Epoch   90: Loss=56.7383, LR=9.2e-02, kappa=3.136, |gamma|=0.1518, time=2.9min
Epoch  100: Loss=56.3377, LR=9.0e-02, kappa=3.167, |gamma|=0.1522, time=3.2min
Epoch  110: Loss=55.9026, LR=8.8e-02, kappa=3.211, |gamma|=0.1505, time=3.5min
Epoch  120: Loss=55.4157, LR=8.6e-02, kappa=3.273, |gamma|=0.1493, time=3.8min
Epoch  130: Loss=54.8700, LR=8.4e-02, kappa=3.355, |gamma|=0.1474, time=4.1min
Epoch  140: Loss=54.2684, LR=8.2e-02, kappa=3.456, |gamma|=0.1452, time=4.4min
Epoch  150: Loss=53.6101, LR=7.9e-02, kappa=3.578, |gamma|=0.1430, time=4.7min
Epoch  160: Loss=52.9212, LR=7.7e-02, kappa=3.715, |gamma|=0.1414, time=5.0min
Epoch  170: Loss=52.1978, LR=7.4e-02, kappa=3.865, |gamma|=0.1387, time=5.3min
Epoch  180: Loss=51.4886, LR=7.1e-02, kappa=4.022, |gamma|=0.1366, time=5.6min
Epoch  190: Loss=50.7773, LR=6.8e-02, kappa=4.179, |gamma|=0.1351, time=6.0min
Epoch  200: Loss=50.1036, LR=6.6e-02, kappa=4.333, |gamma|=0.1334, time=6.3min
Epoch  210: Loss=49.4794, LR=6.3e-02, kappa=4.479, |gamma|=0.1324, time=6.6min
Epoch  220: Loss=48.9090, LR=5.9e-02, kappa=4.614, |gamma|=0.1312, time=6.9min
Epoch  230: Loss=48.3934, LR=5.6e-02, kappa=4.737, |gamma|=0.1303, time=7.2min
Epoch  240: Loss=47.9354, LR=5.3e-02, kappa=4.848, |gamma|=0.1296, time=7.5min
Epoch  250: Loss=47.5103, LR=5.0e-02, kappa=4.949, |gamma|=0.1288, time=7.9min
Epoch  260: Loss=47.1330, LR=4.7e-02, kappa=5.040, |gamma|=0.1279, time=8.2min
Epoch  270: Loss=46.7952, LR=4.4e-02, kappa=5.122, |gamma|=0.1271, time=8.5min
Epoch  280: Loss=46.4931, LR=4.1e-02, kappa=5.195, |gamma|=0.1266, time=8.8min
Epoch  290: Loss=46.2234, LR=3.8e-02, kappa=5.261, |gamma|=0.1260, time=9.1min
Epoch  300: Loss=45.9828, LR=3.5e-02, kappa=5.319, |gamma|=0.1255, time=9.4min
Epoch  310: Loss=45.7685, LR=3.2e-02, kappa=5.370, |gamma|=0.1251, time=9.7min
Epoch  320: Loss=45.5777, LR=2.9e-02, kappa=5.415, |gamma|=0.1247, time=10.0min
Epoch  330: Loss=45.4081, LR=2.6e-02, kappa=5.455, |gamma|=0.1243, time=10.4min
Epoch  340: Loss=45.2576, LR=2.4e-02, kappa=5.490, |gamma|=0.1239, time=10.7min
Epoch  350: Loss=45.1245, LR=2.1e-02, kappa=5.521, |gamma|=0.1235, time=11.0min
Epoch  360: Loss=45.0071, LR=1.9e-02, kappa=5.548, |gamma|=0.1232, time=11.4min
Epoch  370: Loss=44.9039, LR=1.6e-02, kappa=5.571, |gamma|=0.1229, time=11.7min
Epoch  380: Loss=44.8138, LR=1.4e-02, kappa=5.591, |gamma|=0.1226, time=12.0min
Epoch  390: Loss=44.7355, LR=1.2e-02, kappa=5.608, |gamma|=0.1223, time=12.3min
Epoch  400: Loss=44.6679, LR=1.0e-02, kappa=5.622, |gamma|=0.1220, time=12.6min
Epoch  410: Loss=44.6101, LR=8.5e-03, kappa=5.634, |gamma|=0.1218, time=12.9min
Epoch  420: Loss=44.5609, LR=7.0e-03, kappa=5.644, |gamma|=0.1216, time=13.2min
Epoch  430: Loss=44.5196, LR=5.6e-03, kappa=5.652, |gamma|=0.1215, time=13.5min
Epoch  440: Loss=44.4851, LR=4.4e-03, kappa=5.658, |gamma|=0.1213, time=13.9min
Epoch  450: Loss=44.4566, LR=3.3e-03, kappa=5.663, |gamma|=0.1212, time=14.2min
Epoch  460: Loss=44.4331, LR=2.5e-03, kappa=5.667, |gamma|=0.1211, time=14.5min
Epoch  470: Loss=44.4137, LR=1.8e-03, kappa=5.669, |gamma|=0.1210, time=14.9min
Epoch  480: Loss=44.3974, LR=1.4e-03, kappa=5.671, |gamma|=0.1209, time=15.2min
Epoch  490: Loss=44.3833, LR=1.1e-03, kappa=5.673, |gamma|=0.1209, time=15.5min

Training complete: 500 epochs in 15.8 min
Final loss: 44.3715

Final params: kappa=5.6740, mean|gamma|=0.1208
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2/enrollment_model_REPARAM_W0.0001_batch_170000_180000.pt

Batch 18 DONE in 15.9 min (17/39 completed)

======================================================================
BATCH 19/40: samples 180000-190000
Elapsed: 244 min | Est remaining: ~330 min (5.5 hrs)
======================================================================

============================================================
REPARAM v2 Training: samples 180000 to 190000
Epochs: 500, LR: 0.1, grad_clip: 5.0
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 89 diseases
Cluster 1: 6 diseases
Cluster 2: 10 diseases
Cluster 3: 15 diseases
Cluster 4: 8 diseases
Cluster 5: 28 diseases
Cluster 6: 8 diseases
Cluster 7: 10 diseases
Cluster 8: 21 diseases
Cluster 9: 5 diseases
Cluster 10: 37 diseases
Cluster 11: 9 diseases
Cluster 12: 13 diseases
Cluster 13: 15 diseases
Cluster 14: 13 diseases
Cluster 15: 17 diseases
Cluster 16: 8 diseases
Cluster 17: 13 diseases
Cluster 18: 10 diseases
Cluster 19: 13 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=63.8359, LR=1.0e-01, kappa=1.100, |gamma|=0.1005, time=0.0min
Epoch   10: Loss=137.8017, LR=1.0e-01, kappa=2.024, |gamma|=0.0939, time=0.3min
Epoch   20: Loss=100.1227, LR=1.0e-01, kappa=2.662, |gamma|=0.1308, time=0.7min
Epoch   30: Loss=72.7326, LR=9.9e-02, kappa=2.993, |gamma|=0.1538, time=1.0min
Epoch   40: Loss=62.9848, LR=9.8e-02, kappa=3.118, |gamma|=0.1624, time=1.3min
Epoch   50: Loss=59.9531, LR=9.7e-02, kappa=3.139, |gamma|=0.1648, time=1.6min
Epoch   60: Loss=58.6221, LR=9.6e-02, kappa=3.129, |gamma|=0.1652, time=2.0min
Epoch   70: Loss=57.8500, LR=9.5e-02, kappa=3.124, |gamma|=0.1655, time=2.3min
Epoch   80: Loss=57.4046, LR=9.4e-02, kappa=3.133, |gamma|=0.1652, time=2.6min
Epoch   90: Loss=57.0511, LR=9.2e-02, kappa=3.154, |gamma|=0.1665, time=3.0min
Epoch  100: Loss=56.6518, LR=9.0e-02, kappa=3.180, |gamma|=0.1674, time=3.3min
Epoch  110: Loss=56.2302, LR=8.8e-02, kappa=3.213, |gamma|=0.1685, time=3.6min
Epoch  120: Loss=55.7651, LR=8.6e-02, kappa=3.256, |gamma|=0.1685, time=3.9min
Epoch  130: Loss=55.2461, LR=8.4e-02, kappa=3.315, |gamma|=0.1670, time=4.2min
Epoch  140: Loss=54.6809, LR=8.2e-02, kappa=3.393, |gamma|=0.1656, time=4.6min
Epoch  150: Loss=54.0777, LR=7.9e-02, kappa=3.489, |gamma|=0.1645, time=4.9min
Epoch  160: Loss=53.4178, LR=7.7e-02, kappa=3.602, |gamma|=0.1629, time=5.2min
Epoch  170: Loss=52.7303, LR=7.4e-02, kappa=3.728, |gamma|=0.1613, time=5.5min
Epoch  180: Loss=52.0440, LR=7.1e-02, kappa=3.864, |gamma|=0.1601, time=5.9min
Epoch  190: Loss=51.3720, LR=6.8e-02, kappa=4.003, |gamma|=0.1595, time=6.2min
Epoch  200: Loss=50.7313, LR=6.6e-02, kappa=4.141, |gamma|=0.1585, time=6.5min
Epoch  210: Loss=50.1328, LR=6.3e-02, kappa=4.274, |gamma|=0.1574, time=6.8min
Epoch  220: Loss=49.5732, LR=5.9e-02, kappa=4.398, |gamma|=0.1567, time=7.1min
Epoch  230: Loss=49.0667, LR=5.6e-02, kappa=4.514, |gamma|=0.1556, time=7.5min
Epoch  240: Loss=48.6085, LR=5.3e-02, kappa=4.619, |gamma|=0.1545, time=7.8min
Epoch  250: Loss=48.1963, LR=5.0e-02, kappa=4.715, |gamma|=0.1534, time=8.1min
Epoch  260: Loss=47.8266, LR=4.7e-02, kappa=4.801, |gamma|=0.1521, time=8.4min
Epoch  270: Loss=47.4955, LR=4.4e-02, kappa=4.877, |gamma|=0.1510, time=8.7min
Epoch  280: Loss=47.1990, LR=4.1e-02, kappa=4.945, |gamma|=0.1499, time=9.0min
Epoch  290: Loss=46.9338, LR=3.8e-02, kappa=5.006, |gamma|=0.1488, time=9.4min
Epoch  300: Loss=46.6965, LR=3.5e-02, kappa=5.060, |gamma|=0.1478, time=9.7min
Epoch  310: Loss=46.4845, LR=3.2e-02, kappa=5.107, |gamma|=0.1468, time=10.0min
Epoch  320: Loss=46.2952, LR=2.9e-02, kappa=5.150, |gamma|=0.1459, time=10.3min
Epoch  330: Loss=46.1264, LR=2.6e-02, kappa=5.187, |gamma|=0.1450, time=10.6min
Epoch  340: Loss=45.9763, LR=2.4e-02, kappa=5.220, |gamma|=0.1443, time=11.0min
Epoch  350: Loss=45.8432, LR=2.1e-02, kappa=5.249, |gamma|=0.1436, time=11.3min
Epoch  360: Loss=45.7255, LR=1.9e-02, kappa=5.274, |gamma|=0.1430, time=11.6min
Epoch  370: Loss=45.6219, LR=1.6e-02, kappa=5.296, |gamma|=0.1425, time=12.0min
Epoch  380: Loss=45.5312, LR=1.4e-02, kappa=5.315, |gamma|=0.1420, time=12.3min
Epoch  390: Loss=45.4523, LR=1.2e-02, kappa=5.331, |gamma|=0.1417, time=12.7min
Epoch  400: Loss=45.3842, LR=1.0e-02, kappa=5.345, |gamma|=0.1414, time=13.0min
Epoch  410: Loss=45.3259, LR=8.5e-03, kappa=5.356, |gamma|=0.1411, time=13.3min
Epoch  420: Loss=45.2763, LR=7.0e-03, kappa=5.366, |gamma|=0.1409, time=13.6min
Epoch  430: Loss=45.2345, LR=5.6e-03, kappa=5.373, |gamma|=0.1407, time=14.0min
Epoch  440: Loss=45.1996, LR=4.4e-03, kappa=5.379, |gamma|=0.1406, time=14.3min
Epoch  450: Loss=45.1707, LR=3.3e-03, kappa=5.384, |gamma|=0.1405, time=14.6min
Epoch  460: Loss=45.1469, LR=2.5e-03, kappa=5.388, |gamma|=0.1404, time=14.9min
Epoch  470: Loss=45.1271, LR=1.8e-03, kappa=5.390, |gamma|=0.1403, time=15.3min
Epoch  480: Loss=45.1105, LR=1.4e-03, kappa=5.392, |gamma|=0.1402, time=15.6min
Epoch  490: Loss=45.0960, LR=1.1e-03, kappa=5.394, |gamma|=0.1402, time=15.9min

Training complete: 500 epochs in 16.2 min
Final loss: 45.0840

Final params: kappa=5.3948, mean|gamma|=0.1402
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2/enrollment_model_REPARAM_W0.0001_batch_180000_190000.pt

Batch 19 DONE in 16.4 min (18/39 completed)

======================================================================
BATCH 20/40: samples 190000-200000
Elapsed: 261 min | Est remaining: ~315 min (5.2 hrs)
======================================================================

============================================================
REPARAM v2 Training: samples 190000 to 200000
Epochs: 500, LR: 0.1, grad_clip: 5.0
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 10 diseases
Cluster 1: 9 diseases
Cluster 2: 26 diseases
Cluster 3: 19 diseases
Cluster 4: 27 diseases
Cluster 5: 96 diseases
Cluster 6: 14 diseases
Cluster 7: 5 diseases
Cluster 8: 15 diseases
Cluster 9: 9 diseases
Cluster 10: 16 diseases
Cluster 11: 13 diseases
Cluster 12: 17 diseases
Cluster 13: 10 diseases
Cluster 14: 7 diseases
Cluster 15: 13 diseases
Cluster 16: 9 diseases
Cluster 17: 19 diseases
Cluster 18: 9 diseases
Cluster 19: 5 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=63.6597, LR=1.0e-01, kappa=1.100, |gamma|=0.1007, time=0.0min
Epoch   10: Loss=138.9766, LR=1.0e-01, kappa=2.023, |gamma|=0.0943, time=0.4min
Epoch   20: Loss=100.3865, LR=1.0e-01, kappa=2.658, |gamma|=0.1308, time=0.7min
Epoch   30: Loss=72.5477, LR=9.9e-02, kappa=2.990, |gamma|=0.1476, time=1.0min
Epoch   40: Loss=62.7936, LR=9.8e-02, kappa=3.115, |gamma|=0.1472, time=1.3min
Epoch   50: Loss=59.8010, LR=9.7e-02, kappa=3.133, |gamma|=0.1467, time=1.6min
Epoch   60: Loss=58.4910, LR=9.6e-02, kappa=3.124, |gamma|=0.1461, time=1.9min
Epoch   70: Loss=57.7184, LR=9.5e-02, kappa=3.122, |gamma|=0.1474, time=2.3min
Epoch   80: Loss=57.2626, LR=9.4e-02, kappa=3.137, |gamma|=0.1490, time=2.6min
Epoch   90: Loss=56.9064, LR=9.2e-02, kappa=3.165, |gamma|=0.1491, time=2.9min
Epoch  100: Loss=56.5024, LR=9.0e-02, kappa=3.202, |gamma|=0.1495, time=3.2min
Epoch  110: Loss=56.0706, LR=8.8e-02, kappa=3.248, |gamma|=0.1501, time=3.5min
Epoch  120: Loss=55.5918, LR=8.6e-02, kappa=3.308, |gamma|=0.1511, time=3.9min
Epoch  130: Loss=55.0556, LR=8.4e-02, kappa=3.385, |gamma|=0.1507, time=4.2min
Epoch  140: Loss=54.4659, LR=8.2e-02, kappa=3.483, |gamma|=0.1500, time=4.5min
Epoch  150: Loss=53.8357, LR=7.9e-02, kappa=3.599, |gamma|=0.1487, time=4.8min
Epoch  160: Loss=53.1534, LR=7.7e-02, kappa=3.730, |gamma|=0.1467, time=5.1min
Epoch  170: Loss=52.4650, LR=7.4e-02, kappa=3.872, |gamma|=0.1445, time=5.4min
Epoch  180: Loss=51.7905, LR=7.1e-02, kappa=4.016, |gamma|=0.1430, time=5.7min
Epoch  190: Loss=51.1251, LR=6.8e-02, kappa=4.160, |gamma|=0.1414, time=6.0min
Epoch  200: Loss=50.4973, LR=6.6e-02, kappa=4.299, |gamma|=0.1401, time=6.4min
Epoch  210: Loss=49.9162, LR=6.3e-02, kappa=4.431, |gamma|=0.1389, time=6.7min
Epoch  220: Loss=49.3823, LR=5.9e-02, kappa=4.554, |gamma|=0.1376, time=7.0min
Epoch  230: Loss=48.8959, LR=5.6e-02, kappa=4.666, |gamma|=0.1365, time=7.3min
Epoch  240: Loss=48.4549, LR=5.3e-02, kappa=4.768, |gamma|=0.1355, time=7.6min
Epoch  250: Loss=48.0565, LR=5.0e-02, kappa=4.860, |gamma|=0.1346, time=7.9min
Epoch  260: Loss=47.6974, LR=4.7e-02, kappa=4.942, |gamma|=0.1338, time=8.2min
Epoch  270: Loss=47.3742, LR=4.4e-02, kappa=5.016, |gamma|=0.1331, time=8.5min
Epoch  280: Loss=47.0837, LR=4.1e-02, kappa=5.081, |gamma|=0.1325, time=8.8min
Epoch  290: Loss=46.8229, LR=3.8e-02, kappa=5.139, |gamma|=0.1320, time=9.1min
Epoch  300: Loss=46.5889, LR=3.5e-02, kappa=5.191, |gamma|=0.1316, time=9.4min
Epoch  310: Loss=46.3794, LR=3.2e-02, kappa=5.236, |gamma|=0.1312, time=9.8min
Epoch  320: Loss=46.1919, LR=2.9e-02, kappa=5.276, |gamma|=0.1309, time=10.1min
Epoch  330: Loss=46.0246, LR=2.6e-02, kappa=5.312, |gamma|=0.1306, time=10.4min
Epoch  340: Loss=45.8757, LR=2.4e-02, kappa=5.343, |gamma|=0.1304, time=10.7min
Epoch  350: Loss=45.7436, LR=2.1e-02, kappa=5.370, |gamma|=0.1302, time=11.0min
Epoch  360: Loss=45.6267, LR=1.9e-02, kappa=5.393, |gamma|=0.1299, time=11.3min
Epoch  370: Loss=45.5239, LR=1.6e-02, kappa=5.414, |gamma|=0.1297, time=11.6min
Epoch  380: Loss=45.4338, LR=1.4e-02, kappa=5.431, |gamma|=0.1295, time=11.9min
Epoch  390: Loss=45.3554, LR=1.2e-02, kappa=5.446, |gamma|=0.1294, time=12.2min
Epoch  400: Loss=45.2877, LR=1.0e-02, kappa=5.458, |gamma|=0.1292, time=12.5min
Epoch  410: Loss=45.2297, LR=8.5e-03, kappa=5.469, |gamma|=0.1290, time=12.8min
Epoch  420: Loss=45.1803, LR=7.0e-03, kappa=5.477, |gamma|=0.1289, time=13.2min
Epoch  430: Loss=45.1388, LR=5.6e-03, kappa=5.484, |gamma|=0.1288, time=13.5min
Epoch  440: Loss=45.1041, LR=4.4e-03, kappa=5.490, |gamma|=0.1287, time=13.8min
Epoch  450: Loss=45.0754, LR=3.3e-03, kappa=5.494, |gamma|=0.1286, time=14.1min
Epoch  460: Loss=45.0518, LR=2.5e-03, kappa=5.497, |gamma|=0.1286, time=14.4min
Epoch  470: Loss=45.0322, LR=1.8e-03, kappa=5.500, |gamma|=0.1285, time=14.7min
Epoch  480: Loss=45.0158, LR=1.4e-03, kappa=5.501, |gamma|=0.1285, time=15.0min
Epoch  490: Loss=45.0015, LR=1.1e-03, kappa=5.503, |gamma|=0.1284, time=15.3min

Training complete: 500 epochs in 15.6 min
Final loss: 44.9897

Final params: kappa=5.5038, mean|gamma|=0.1284
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2/enrollment_model_REPARAM_W0.0001_batch_190000_200000.pt

Batch 20 DONE in 15.7 min (19/39 completed)

======================================================================
BATCH 21/40: samples 200000-210000
Elapsed: 276 min | Est remaining: ~300 min (5.0 hrs)
======================================================================

============================================================
REPARAM v2 Training: samples 200000 to 210000
Epochs: 500, LR: 0.1, grad_clip: 5.0
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 11 diseases
Cluster 1: 28 diseases
Cluster 2: 9 diseases
Cluster 3: 24 diseases
Cluster 4: 16 diseases
Cluster 5: 9 diseases
Cluster 6: 19 diseases
Cluster 7: 16 diseases
Cluster 8: 78 diseases
Cluster 9: 12 diseases
Cluster 10: 22 diseases
Cluster 11: 18 diseases
Cluster 12: 5 diseases
Cluster 13: 5 diseases
Cluster 14: 13 diseases
Cluster 15: 22 diseases
Cluster 16: 11 diseases
Cluster 17: 8 diseases
Cluster 18: 15 diseases
Cluster 19: 7 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=63.3513, LR=1.0e-01, kappa=1.100, |gamma|=0.1009, time=0.0min
Epoch   10: Loss=139.7557, LR=1.0e-01, kappa=2.022, |gamma|=0.0959, time=0.3min
Epoch   20: Loss=100.3841, LR=1.0e-01, kappa=2.656, |gamma|=0.1339, time=0.6min
Epoch   30: Loss=72.1924, LR=9.9e-02, kappa=2.982, |gamma|=0.1555, time=0.9min
Epoch   40: Loss=62.4571, LR=9.8e-02, kappa=3.102, |gamma|=0.1601, time=1.2min
Epoch   50: Loss=59.5114, LR=9.7e-02, kappa=3.118, |gamma|=0.1624, time=1.5min
Epoch   60: Loss=58.2307, LR=9.6e-02, kappa=3.103, |gamma|=0.1643, time=1.8min
Epoch   70: Loss=57.4673, LR=9.5e-02, kappa=3.095, |gamma|=0.1661, time=2.1min
Epoch   80: Loss=57.0116, LR=9.4e-02, kappa=3.102, |gamma|=0.1684, time=2.4min
Epoch   90: Loss=56.6660, LR=9.2e-02, kappa=3.119, |gamma|=0.1721, time=2.8min
Epoch  100: Loss=56.2750, LR=9.0e-02, kappa=3.142, |gamma|=0.1757, time=3.1min
Epoch  110: Loss=55.8582, LR=8.8e-02, kappa=3.173, |gamma|=0.1784, time=3.4min
Epoch  120: Loss=55.4063, LR=8.6e-02, kappa=3.215, |gamma|=0.1802, time=3.7min
Epoch  130: Loss=54.8959, LR=8.4e-02, kappa=3.274, |gamma|=0.1798, time=4.0min
Epoch  140: Loss=54.3268, LR=8.2e-02, kappa=3.354, |gamma|=0.1783, time=4.3min
Epoch  150: Loss=53.7137, LR=7.9e-02, kappa=3.455, |gamma|=0.1765, time=4.6min
Epoch  160: Loss=53.0568, LR=7.7e-02, kappa=3.573, |gamma|=0.1742, time=4.9min
Epoch  170: Loss=52.3763, LR=7.4e-02, kappa=3.705, |gamma|=0.1721, time=5.2min
Epoch  180: Loss=51.6856, LR=7.1e-02, kappa=3.845, |gamma|=0.1696, time=5.5min
Epoch  190: Loss=51.0127, LR=6.8e-02, kappa=3.987, |gamma|=0.1665, time=5.8min
Epoch  200: Loss=50.3703, LR=6.6e-02, kappa=4.126, |gamma|=0.1632, time=6.1min
Epoch  210: Loss=49.7613, LR=6.3e-02, kappa=4.261, |gamma|=0.1590, time=6.4min
Epoch  220: Loss=49.1925, LR=5.9e-02, kappa=4.392, |gamma|=0.1597, time=6.7min
Epoch  230: Loss=48.6793, LR=5.6e-02, kappa=4.513, |gamma|=0.1578, time=7.0min
Epoch  240: Loss=48.2156, LR=5.3e-02, kappa=4.623, |gamma|=0.1567, time=7.3min
Epoch  250: Loss=47.7993, LR=5.0e-02, kappa=4.721, |gamma|=0.1558, time=7.6min
Epoch  260: Loss=47.4262, LR=4.7e-02, kappa=4.809, |gamma|=0.1546, time=7.9min
Epoch  270: Loss=47.0920, LR=4.4e-02, kappa=4.888, |gamma|=0.1531, time=8.2min
Epoch  280: Loss=46.7925, LR=4.1e-02, kappa=4.958, |gamma|=0.1519, time=8.5min
Epoch  290: Loss=46.5242, LR=3.8e-02, kappa=5.020, |gamma|=0.1507, time=8.8min
Epoch  300: Loss=46.2840, LR=3.5e-02, kappa=5.075, |gamma|=0.1497, time=9.1min
Epoch  310: Loss=46.0691, LR=3.2e-02, kappa=5.124, |gamma|=0.1487, time=9.4min
Epoch  320: Loss=45.8771, LR=2.9e-02, kappa=5.167, |gamma|=0.1477, time=9.7min
Epoch  330: Loss=45.7059, LR=2.6e-02, kappa=5.206, |gamma|=0.1469, time=9.9min
Epoch  340: Loss=45.5534, LR=2.4e-02, kappa=5.240, |gamma|=0.1462, time=10.3min
Epoch  350: Loss=45.4182, LR=2.1e-02, kappa=5.270, |gamma|=0.1455, time=10.5min
Epoch  360: Loss=45.2986, LR=1.9e-02, kappa=5.296, |gamma|=0.1448, time=10.8min
Epoch  370: Loss=45.1933, LR=1.6e-02, kappa=5.319, |gamma|=0.1442, time=11.1min
Epoch  380: Loss=45.1012, LR=1.4e-02, kappa=5.339, |gamma|=0.1436, time=11.4min
Epoch  390: Loss=45.0210, LR=1.2e-02, kappa=5.356, |gamma|=0.1432, time=11.7min
Epoch  400: Loss=44.9518, LR=1.0e-02, kappa=5.370, |gamma|=0.1428, time=12.0min
Epoch  410: Loss=44.8923, LR=8.5e-03, kappa=5.382, |gamma|=0.1424, time=12.3min
Epoch  420: Loss=44.8418, LR=7.0e-03, kappa=5.392, |gamma|=0.1421, time=12.6min
Epoch  430: Loss=44.7992, LR=5.6e-03, kappa=5.400, |gamma|=0.1419, time=12.9min
Epoch  440: Loss=44.7637, LR=4.4e-03, kappa=5.407, |gamma|=0.1417, time=13.2min
Epoch  450: Loss=44.7341, LR=3.3e-03, kappa=5.412, |gamma|=0.1415, time=13.5min
Epoch  460: Loss=44.7098, LR=2.5e-03, kappa=5.415, |gamma|=0.1414, time=13.8min
Epoch  470: Loss=44.6895, LR=1.8e-03, kappa=5.418, |gamma|=0.1413, time=14.1min
Epoch  480: Loss=44.6725, LR=1.4e-03, kappa=5.420, |gamma|=0.1412, time=14.4min
Epoch  490: Loss=44.6577, LR=1.1e-03, kappa=5.422, |gamma|=0.1411, time=14.7min

Training complete: 500 epochs in 15.0 min
Final loss: 44.6454

Final params: kappa=5.4230, mean|gamma|=0.1410
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2/enrollment_model_REPARAM_W0.0001_batch_200000_210000.pt

Batch 21 DONE in 15.1 min (20/39 completed)

======================================================================
BATCH 22/40: samples 210000-220000
Elapsed: 292 min | Est remaining: ~285 min (4.8 hrs)
======================================================================

============================================================
REPARAM v2 Training: samples 210000 to 220000
Epochs: 500, LR: 0.1, grad_clip: 5.0
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 16 diseases
Cluster 1: 7 diseases
Cluster 2: 14 diseases
Cluster 3: 102 diseases
Cluster 4: 9 diseases
Cluster 5: 18 diseases
Cluster 6: 5 diseases
Cluster 7: 26 diseases
Cluster 8: 9 diseases
Cluster 9: 8 diseases
Cluster 10: 12 diseases
Cluster 11: 14 diseases
Cluster 12: 14 diseases
Cluster 13: 10 diseases
Cluster 14: 5 diseases
Cluster 15: 12 diseases
Cluster 16: 25 diseases
Cluster 17: 5 diseases
Cluster 18: 24 diseases
Cluster 19: 13 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=63.6113, LR=1.0e-01, kappa=1.100, |gamma|=0.1013, time=0.0min
Epoch   10: Loss=138.8470, LR=1.0e-01, kappa=2.023, |gamma|=0.0999, time=0.3min
Epoch   20: Loss=100.2890, LR=1.0e-01, kappa=2.659, |gamma|=0.1368, time=0.6min
Epoch   30: Loss=72.4813, LR=9.9e-02, kappa=2.987, |gamma|=0.1553, time=0.9min
Epoch   40: Loss=62.7319, LR=9.8e-02, kappa=3.108, |gamma|=0.1620, time=1.2min
Epoch   50: Loss=59.7355, LR=9.7e-02, kappa=3.125, |gamma|=0.1638, time=1.5min
Epoch   60: Loss=58.4220, LR=9.6e-02, kappa=3.113, |gamma|=0.1596, time=1.8min
Epoch   70: Loss=57.6467, LR=9.5e-02, kappa=3.110, |gamma|=0.1590, time=2.1min
Epoch   80: Loss=57.1867, LR=9.4e-02, kappa=3.128, |gamma|=0.1577, time=2.4min
Epoch   90: Loss=56.8209, LR=9.2e-02, kappa=3.164, |gamma|=0.1574, time=2.7min
Epoch  100: Loss=56.4080, LR=9.0e-02, kappa=3.211, |gamma|=0.1567, time=3.0min
Epoch  110: Loss=55.9577, LR=8.8e-02, kappa=3.272, |gamma|=0.1547, time=3.3min
Epoch  120: Loss=55.4589, LR=8.6e-02, kappa=3.348, |gamma|=0.1521, time=3.6min
Epoch  130: Loss=54.8964, LR=8.4e-02, kappa=3.442, |gamma|=0.1499, time=3.9min
Epoch  140: Loss=54.2804, LR=8.2e-02, kappa=3.558, |gamma|=0.1475, time=4.2min
Epoch  150: Loss=53.6100, LR=7.9e-02, kappa=3.692, |gamma|=0.1449, time=4.5min
Epoch  160: Loss=52.9157, LR=7.7e-02, kappa=3.841, |gamma|=0.1422, time=4.8min
Epoch  170: Loss=52.2023, LR=7.4e-02, kappa=3.997, |gamma|=0.1397, time=5.1min
Epoch  180: Loss=51.5073, LR=7.1e-02, kappa=4.155, |gamma|=0.1375, time=5.4min
Epoch  190: Loss=50.8370, LR=6.8e-02, kappa=4.310, |gamma|=0.1356, time=5.7min
Epoch  200: Loss=50.2157, LR=6.6e-02, kappa=4.458, |gamma|=0.1336, time=6.0min
Epoch  210: Loss=49.6330, LR=6.3e-02, kappa=4.594, |gamma|=0.1317, time=6.3min
Epoch  220: Loss=49.1089, LR=5.9e-02, kappa=4.719, |gamma|=0.1300, time=6.6min
Epoch  230: Loss=48.6261, LR=5.6e-02, kappa=4.833, |gamma|=0.1279, time=6.9min
Epoch  240: Loss=48.1912, LR=5.3e-02, kappa=4.934, |gamma|=0.1261, time=7.2min
Epoch  250: Loss=47.7983, LR=5.0e-02, kappa=5.025, |gamma|=0.1245, time=7.5min
Epoch  260: Loss=47.4436, LR=4.7e-02, kappa=5.106, |gamma|=0.1231, time=7.8min
Epoch  270: Loss=47.1263, LR=4.4e-02, kappa=5.178, |gamma|=0.1230, time=8.1min
Epoch  280: Loss=46.8425, LR=4.1e-02, kappa=5.242, |gamma|=0.1226, time=8.4min
Epoch  290: Loss=46.5877, LR=3.8e-02, kappa=5.298, |gamma|=0.1223, time=8.7min
Epoch  300: Loss=46.3590, LR=3.5e-02, kappa=5.349, |gamma|=0.1222, time=9.0min
Epoch  310: Loss=46.1536, LR=3.2e-02, kappa=5.393, |gamma|=0.1220, time=9.3min
Epoch  320: Loss=45.9694, LR=2.9e-02, kappa=5.432, |gamma|=0.1219, time=9.6min
Epoch  330: Loss=45.8044, LR=2.6e-02, kappa=5.467, |gamma|=0.1218, time=9.9min
Epoch  340: Loss=45.6568, LR=2.4e-02, kappa=5.497, |gamma|=0.1220, time=10.2min
Epoch  350: Loss=45.5256, LR=2.1e-02, kappa=5.523, |gamma|=0.1221, time=10.5min
Epoch  360: Loss=45.4091, LR=1.9e-02, kappa=5.547, |gamma|=0.1220, time=10.7min
Epoch  370: Loss=45.3064, LR=1.6e-02, kappa=5.567, |gamma|=0.1218, time=11.0min
Epoch  380: Loss=45.2163, LR=1.4e-02, kappa=5.584, |gamma|=0.1216, time=11.3min
Epoch  390: Loss=45.1377, LR=1.2e-02, kappa=5.599, |gamma|=0.1214, time=11.6min
Epoch  400: Loss=45.0697, LR=1.0e-02, kappa=5.611, |gamma|=0.1213, time=11.9min
Epoch  410: Loss=45.0113, LR=8.5e-03, kappa=5.622, |gamma|=0.1211, time=12.2min
Epoch  420: Loss=44.9616, LR=7.0e-03, kappa=5.630, |gamma|=0.1210, time=12.5min
Epoch  430: Loss=44.9197, LR=5.6e-03, kappa=5.637, |gamma|=0.1209, time=12.8min
Epoch  440: Loss=44.8847, LR=4.4e-03, kappa=5.643, |gamma|=0.1208, time=13.1min
Epoch  450: Loss=44.8557, LR=3.3e-03, kappa=5.647, |gamma|=0.1207, time=13.4min
Epoch  460: Loss=44.8317, LR=2.5e-03, kappa=5.650, |gamma|=0.1206, time=13.7min
Epoch  470: Loss=44.8119, LR=1.8e-03, kappa=5.653, |gamma|=0.1205, time=14.0min
Epoch  480: Loss=44.7952, LR=1.4e-03, kappa=5.655, |gamma|=0.1204, time=14.3min
Epoch  490: Loss=44.7807, LR=1.1e-03, kappa=5.656, |gamma|=0.1204, time=14.6min

Training complete: 500 epochs in 14.9 min
Final loss: 44.7686

Final params: kappa=5.6569, mean|gamma|=0.1203
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2/enrollment_model_REPARAM_W0.0001_batch_210000_220000.pt

Batch 22 DONE in 15.0 min (21/39 completed)

======================================================================
BATCH 23/40: samples 220000-230000
Elapsed: 307 min | Est remaining: ~270 min (4.5 hrs)
======================================================================

============================================================
REPARAM v2 Training: samples 220000 to 230000
Epochs: 500, LR: 0.1, grad_clip: 5.0
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 16 diseases
Cluster 1: 10 diseases
Cluster 2: 16 diseases
Cluster 3: 27 diseases
Cluster 4: 35 diseases
Cluster 5: 9 diseases
Cluster 6: 9 diseases
Cluster 7: 8 diseases
Cluster 8: 5 diseases
Cluster 9: 81 diseases
Cluster 10: 11 diseases
Cluster 11: 12 diseases
Cluster 12: 16 diseases
Cluster 13: 15 diseases
Cluster 14: 8 diseases
Cluster 15: 5 diseases
Cluster 16: 24 diseases
Cluster 17: 10 diseases
Cluster 18: 15 diseases
Cluster 19: 16 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=63.1564, LR=1.0e-01, kappa=1.100, |gamma|=0.1008, time=0.0min
Epoch   10: Loss=138.9618, LR=1.0e-01, kappa=2.022, |gamma|=0.0945, time=0.3min
Epoch   20: Loss=100.0365, LR=1.0e-01, kappa=2.655, |gamma|=0.1330, time=0.6min
Epoch   30: Loss=72.0544, LR=9.9e-02, kappa=2.980, |gamma|=0.1530, time=0.9min
Epoch   40: Loss=62.3120, LR=9.8e-02, kappa=3.097, |gamma|=0.1561, time=1.2min
Epoch   50: Loss=59.3451, LR=9.7e-02, kappa=3.109, |gamma|=0.1569, time=1.5min
Epoch   60: Loss=58.0531, LR=9.6e-02, kappa=3.094, |gamma|=0.1558, time=1.8min
Epoch   70: Loss=57.2902, LR=9.5e-02, kappa=3.086, |gamma|=0.1558, time=2.1min
Epoch   80: Loss=56.8420, LR=9.4e-02, kappa=3.094, |gamma|=0.1561, time=2.4min
Epoch   90: Loss=56.4966, LR=9.2e-02, kappa=3.115, |gamma|=0.1566, time=2.7min
Epoch  100: Loss=56.1139, LR=9.0e-02, kappa=3.144, |gamma|=0.1576, time=3.0min
Epoch  110: Loss=55.7006, LR=8.8e-02, kappa=3.180, |gamma|=0.1579, time=3.3min
Epoch  120: Loss=55.2508, LR=8.6e-02, kappa=3.226, |gamma|=0.1575, time=3.6min
Epoch  130: Loss=54.7650, LR=8.4e-02, kappa=3.284, |gamma|=0.1560, time=3.8min
Epoch  140: Loss=54.2219, LR=8.2e-02, kappa=3.358, |gamma|=0.1555, time=4.1min
Epoch  150: Loss=53.6415, LR=7.9e-02, kappa=3.449, |gamma|=0.1549, time=4.4min
Epoch  160: Loss=53.0280, LR=7.7e-02, kappa=3.557, |gamma|=0.1549, time=4.7min
Epoch  170: Loss=52.3838, LR=7.4e-02, kappa=3.678, |gamma|=0.1548, time=5.0min
Epoch  180: Loss=51.7304, LR=7.1e-02, kappa=3.809, |gamma|=0.1551, time=5.3min
Epoch  190: Loss=51.0911, LR=6.8e-02, kappa=3.944, |gamma|=0.1551, time=5.6min
Epoch  200: Loss=50.4695, LR=6.6e-02, kappa=4.078, |gamma|=0.1549, time=5.9min
Epoch  210: Loss=49.8830, LR=6.3e-02, kappa=4.208, |gamma|=0.1544, time=6.2min
Epoch  220: Loss=49.3397, LR=5.9e-02, kappa=4.329, |gamma|=0.1537, time=6.5min
Epoch  230: Loss=48.8416, LR=5.6e-02, kappa=4.442, |gamma|=0.1529, time=6.8min
Epoch  240: Loss=48.3886, LR=5.3e-02, kappa=4.545, |gamma|=0.1519, time=7.1min
Epoch  250: Loss=47.9780, LR=5.0e-02, kappa=4.638, |gamma|=0.1507, time=7.4min
Epoch  260: Loss=47.6078, LR=4.7e-02, kappa=4.722, |gamma|=0.1494, time=7.6min
Epoch  270: Loss=47.2746, LR=4.4e-02, kappa=4.797, |gamma|=0.1480, time=7.9min
Epoch  280: Loss=46.9751, LR=4.1e-02, kappa=4.864, |gamma|=0.1468, time=8.2min
Epoch  290: Loss=46.7061, LR=3.8e-02, kappa=4.924, |gamma|=0.1455, time=8.5min
Epoch  300: Loss=46.4648, LR=3.5e-02, kappa=4.978, |gamma|=0.1442, time=8.8min
Epoch  310: Loss=46.2486, LR=3.2e-02, kappa=5.025, |gamma|=0.1431, time=9.1min
Epoch  320: Loss=46.0554, LR=2.9e-02, kappa=5.067, |gamma|=0.1420, time=9.4min
Epoch  330: Loss=45.8829, LR=2.6e-02, kappa=5.104, |gamma|=0.1410, time=9.7min
Epoch  340: Loss=45.7294, LR=2.4e-02, kappa=5.137, |gamma|=0.1400, time=10.0min
Epoch  350: Loss=45.5931, LR=2.1e-02, kappa=5.166, |gamma|=0.1392, time=10.3min
Epoch  360: Loss=45.4728, LR=1.9e-02, kappa=5.191, |gamma|=0.1385, time=10.6min
Epoch  370: Loss=45.3668, LR=1.6e-02, kappa=5.213, |gamma|=0.1379, time=10.9min
Epoch  380: Loss=45.2741, LR=1.4e-02, kappa=5.232, |gamma|=0.1373, time=11.2min
Epoch  390: Loss=45.1934, LR=1.2e-02, kappa=5.248, |gamma|=0.1368, time=11.5min
Epoch  400: Loss=45.1238, LR=1.0e-02, kappa=5.262, |gamma|=0.1364, time=11.8min
Epoch  410: Loss=45.0641, LR=8.5e-03, kappa=5.273, |gamma|=0.1360, time=12.1min
Epoch  420: Loss=45.0133, LR=7.0e-03, kappa=5.283, |gamma|=0.1357, time=12.4min
Epoch  430: Loss=44.9705, LR=5.6e-03, kappa=5.290, |gamma|=0.1355, time=12.7min
Epoch  440: Loss=44.9348, LR=4.4e-03, kappa=5.296, |gamma|=0.1352, time=13.0min
Epoch  450: Loss=44.9053, LR=3.3e-03, kappa=5.301, |gamma|=0.1351, time=13.3min
Epoch  460: Loss=44.8809, LR=2.5e-03, kappa=5.305, |gamma|=0.1349, time=13.6min
Epoch  470: Loss=44.8607, LR=1.8e-03, kappa=5.307, |gamma|=0.1348, time=13.9min
Epoch  480: Loss=44.8437, LR=1.4e-03, kappa=5.309, |gamma|=0.1347, time=14.2min
Epoch  490: Loss=44.8289, LR=1.1e-03, kappa=5.311, |gamma|=0.1346, time=14.5min

Training complete: 500 epochs in 14.7 min
Final loss: 44.8166

Final params: kappa=5.3119, mean|gamma|=0.1345
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2/enrollment_model_REPARAM_W0.0001_batch_220000_230000.pt

Batch 23 DONE in 14.9 min (22/39 completed)

======================================================================
BATCH 24/40: samples 230000-240000
Elapsed: 321 min | Est remaining: ~255 min (4.2 hrs)
======================================================================

============================================================
REPARAM v2 Training: samples 230000 to 240000
Epochs: 500, LR: 0.1, grad_clip: 5.0
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 17 diseases
Cluster 1: 14 diseases
Cluster 2: 5 diseases
Cluster 3: 37 diseases
Cluster 4: 20 diseases
Cluster 5: 10 diseases
Cluster 6: 86 diseases
Cluster 7: 24 diseases
Cluster 8: 6 diseases
Cluster 9: 13 diseases
Cluster 10: 8 diseases
Cluster 11: 17 diseases
Cluster 12: 5 diseases
Cluster 13: 11 diseases
Cluster 14: 8 diseases
Cluster 15: 10 diseases
Cluster 16: 4 diseases
Cluster 17: 8 diseases
Cluster 18: 10 diseases
Cluster 19: 35 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=62.9686, LR=1.0e-01, kappa=1.100, |gamma|=0.1012, time=0.0min
Epoch   10: Loss=140.0068, LR=1.0e-01, kappa=2.021, |gamma|=0.0995, time=0.3min
Epoch   20: Loss=100.2021, LR=1.0e-01, kappa=2.654, |gamma|=0.1355, time=0.6min
Epoch   30: Loss=71.8029, LR=9.9e-02, kappa=2.982, |gamma|=0.1554, time=0.9min
Epoch   40: Loss=62.0762, LR=9.8e-02, kappa=3.105, |gamma|=0.1636, time=1.2min
Epoch   50: Loss=59.1529, LR=9.7e-02, kappa=3.123, |gamma|=0.1671, time=1.5min
Epoch   60: Loss=57.8831, LR=9.6e-02, kappa=3.113, |gamma|=0.1670, time=1.8min
Epoch   70: Loss=57.1200, LR=9.5e-02, kappa=3.109, |gamma|=0.1674, time=2.1min
Epoch   80: Loss=56.6593, LR=9.4e-02, kappa=3.121, |gamma|=0.1689, time=2.4min
Epoch   90: Loss=56.3052, LR=9.2e-02, kappa=3.144, |gamma|=0.1698, time=2.7min
Epoch  100: Loss=55.9097, LR=9.0e-02, kappa=3.177, |gamma|=0.1699, time=3.0min
Epoch  110: Loss=55.4793, LR=8.8e-02, kappa=3.219, |gamma|=0.1697, time=3.3min
Epoch  120: Loss=55.0029, LR=8.6e-02, kappa=3.275, |gamma|=0.1681, time=3.6min
Epoch  130: Loss=54.4822, LR=8.4e-02, kappa=3.347, |gamma|=0.1658, time=3.9min
Epoch  140: Loss=53.8935, LR=8.2e-02, kappa=3.437, |gamma|=0.1635, time=4.2min
Epoch  150: Loss=53.2745, LR=7.9e-02, kappa=3.546, |gamma|=0.1611, time=4.5min
Epoch  160: Loss=52.6162, LR=7.7e-02, kappa=3.670, |gamma|=0.1592, time=4.8min
Epoch  170: Loss=51.9397, LR=7.4e-02, kappa=3.804, |gamma|=0.1576, time=5.0min
Epoch  180: Loss=51.2654, LR=7.1e-02, kappa=3.944, |gamma|=0.1565, time=5.3min
Epoch  190: Loss=50.6207, LR=6.8e-02, kappa=4.084, |gamma|=0.1554, time=5.6min
Epoch  200: Loss=49.9930, LR=6.6e-02, kappa=4.219, |gamma|=0.1548, time=5.9min
Epoch  210: Loss=49.4186, LR=6.3e-02, kappa=4.348, |gamma|=0.1552, time=6.2min
Epoch  220: Loss=48.8930, LR=5.9e-02, kappa=4.467, |gamma|=0.1557, time=6.5min
Epoch  230: Loss=48.4164, LR=5.6e-02, kappa=4.577, |gamma|=0.1559, time=6.8min
Epoch  240: Loss=47.9861, LR=5.3e-02, kappa=4.676, |gamma|=0.1557, time=7.1min
Epoch  250: Loss=47.5988, LR=5.0e-02, kappa=4.764, |gamma|=0.1552, time=7.4min
Epoch  260: Loss=47.2507, LR=4.7e-02, kappa=4.844, |gamma|=0.1546, time=7.7min
Epoch  270: Loss=46.9379, LR=4.4e-02, kappa=4.915, |gamma|=0.1539, time=8.0min
Epoch  280: Loss=46.6569, LR=4.1e-02, kappa=4.977, |gamma|=0.1530, time=8.2min
Epoch  290: Loss=46.4045, LR=3.8e-02, kappa=5.033, |gamma|=0.1520, time=8.5min
Epoch  300: Loss=46.1779, LR=3.5e-02, kappa=5.083, |gamma|=0.1510, time=8.8min
Epoch  310: Loss=45.9746, LR=3.2e-02, kappa=5.128, |gamma|=0.1500, time=9.1min
Epoch  320: Loss=45.7925, LR=2.9e-02, kappa=5.167, |gamma|=0.1490, time=9.4min
Epoch  330: Loss=45.6297, LR=2.6e-02, kappa=5.201, |gamma|=0.1482, time=9.7min
Epoch  340: Loss=45.4845, LR=2.4e-02, kappa=5.232, |gamma|=0.1473, time=10.0min
Epoch  350: Loss=45.3554, LR=2.1e-02, kappa=5.259, |gamma|=0.1465, time=10.3min
Epoch  360: Loss=45.2411, LR=1.9e-02, kappa=5.283, |gamma|=0.1458, time=10.6min
Epoch  370: Loss=45.1404, LR=1.6e-02, kappa=5.303, |gamma|=0.1451, time=10.9min
Epoch  380: Loss=45.0521, LR=1.4e-02, kappa=5.321, |gamma|=0.1445, time=11.1min
Epoch  390: Loss=44.9751, LR=1.2e-02, kappa=5.336, |gamma|=0.1439, time=11.4min
Epoch  400: Loss=44.9086, LR=1.0e-02, kappa=5.349, |gamma|=0.1434, time=11.7min
Epoch  410: Loss=44.8515, LR=8.5e-03, kappa=5.360, |gamma|=0.1430, time=12.0min
Epoch  420: Loss=44.8029, LR=7.0e-03, kappa=5.369, |gamma|=0.1426, time=12.3min
Epoch  430: Loss=44.7620, LR=5.6e-03, kappa=5.376, |gamma|=0.1423, time=12.6min
Epoch  440: Loss=44.7278, LR=4.4e-03, kappa=5.382, |gamma|=0.1421, time=12.9min
Epoch  450: Loss=44.6995, LR=3.3e-03, kappa=5.386, |gamma|=0.1419, time=13.2min
Epoch  460: Loss=44.6762, LR=2.5e-03, kappa=5.390, |gamma|=0.1417, time=13.5min
Epoch  470: Loss=44.6569, LR=1.8e-03, kappa=5.392, |gamma|=0.1416, time=13.7min
Epoch  480: Loss=44.6407, LR=1.4e-03, kappa=5.394, |gamma|=0.1414, time=14.0min
Epoch  490: Loss=44.6266, LR=1.1e-03, kappa=5.396, |gamma|=0.1414, time=14.3min

Training complete: 500 epochs in 14.6 min
Final loss: 44.6149

Final params: kappa=5.3967, mean|gamma|=0.1413
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2/enrollment_model_REPARAM_W0.0001_batch_230000_240000.pt

Batch 24 DONE in 14.7 min (23/39 completed)

======================================================================
BATCH 25/40: samples 240000-250000
Elapsed: 336 min | Est remaining: ~240 min (4.0 hrs)
======================================================================

============================================================
REPARAM v2 Training: samples 240000 to 250000
Epochs: 500, LR: 0.1, grad_clip: 5.0
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 28 diseases
Cluster 1: 29 diseases
Cluster 2: 14 diseases
Cluster 3: 8 diseases
Cluster 4: 102 diseases
Cluster 5: 13 diseases
Cluster 6: 9 diseases
Cluster 7: 16 diseases
Cluster 8: 19 diseases
Cluster 9: 7 diseases
Cluster 10: 16 diseases
Cluster 11: 13 diseases
Cluster 12: 6 diseases
Cluster 13: 6 diseases
Cluster 14: 5 diseases
Cluster 15: 13 diseases
Cluster 16: 15 diseases
Cluster 17: 6 diseases
Cluster 18: 12 diseases
Cluster 19: 11 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=64.0704, LR=1.0e-01, kappa=1.100, |gamma|=0.1006, time=0.0min
Epoch   10: Loss=137.9983, LR=1.0e-01, kappa=2.024, |gamma|=0.0972, time=0.3min
Epoch   20: Loss=100.3024, LR=1.0e-01, kappa=2.666, |gamma|=0.1327, time=0.6min
Epoch   30: Loss=72.9240, LR=9.9e-02, kappa=3.005, |gamma|=0.1523, time=0.9min
Epoch   40: Loss=63.1659, LR=9.8e-02, kappa=3.136, |gamma|=0.1590, time=1.2min
Epoch   50: Loss=60.1223, LR=9.7e-02, kappa=3.159, |gamma|=0.1604, time=1.5min
Epoch   60: Loss=58.7793, LR=9.6e-02, kappa=3.150, |gamma|=0.1598, time=1.7min
Epoch   70: Loss=57.9968, LR=9.5e-02, kappa=3.148, |gamma|=0.1595, time=2.0min
Epoch   80: Loss=57.5418, LR=9.4e-02, kappa=3.161, |gamma|=0.1603, time=2.3min
Epoch   90: Loss=57.1791, LR=9.2e-02, kappa=3.188, |gamma|=0.1617, time=2.6min
Epoch  100: Loss=56.7672, LR=9.0e-02, kappa=3.224, |gamma|=0.1615, time=2.9min
Epoch  110: Loss=56.3332, LR=8.8e-02, kappa=3.269, |gamma|=0.1608, time=3.2min
Epoch  120: Loss=55.8449, LR=8.6e-02, kappa=3.329, |gamma|=0.1601, time=3.5min
Epoch  130: Loss=55.3007, LR=8.4e-02, kappa=3.407, |gamma|=0.1586, time=3.8min
Epoch  140: Loss=54.7128, LR=8.2e-02, kappa=3.503, |gamma|=0.1562, time=4.1min
Epoch  150: Loss=54.0784, LR=7.9e-02, kappa=3.615, |gamma|=0.1538, time=4.3min
Epoch  160: Loss=53.4196, LR=7.7e-02, kappa=3.739, |gamma|=0.1517, time=4.6min
Epoch  170: Loss=52.7568, LR=7.4e-02, kappa=3.871, |gamma|=0.1501, time=4.9min
Epoch  180: Loss=52.0971, LR=7.1e-02, kappa=4.005, |gamma|=0.1484, time=5.2min
Epoch  190: Loss=51.4679, LR=6.8e-02, kappa=4.137, |gamma|=0.1471, time=5.5min
Epoch  200: Loss=50.8696, LR=6.6e-02, kappa=4.263, |gamma|=0.1454, time=5.8min
Epoch  210: Loss=50.3088, LR=6.3e-02, kappa=4.381, |gamma|=0.1452, time=6.1min
Epoch  220: Loss=49.7978, LR=5.9e-02, kappa=4.490, |gamma|=0.1454, time=6.3min
Epoch  230: Loss=49.3306, LR=5.6e-02, kappa=4.590, |gamma|=0.1457, time=6.6min
Epoch  240: Loss=48.9065, LR=5.3e-02, kappa=4.681, |gamma|=0.1459, time=6.9min
Epoch  250: Loss=48.5220, LR=5.0e-02, kappa=4.763, |gamma|=0.1461, time=7.2min
Epoch  260: Loss=48.1741, LR=4.7e-02, kappa=4.836, |gamma|=0.1462, time=7.5min
Epoch  270: Loss=47.8599, LR=4.4e-02, kappa=4.902, |gamma|=0.1463, time=7.8min
Epoch  280: Loss=47.5763, LR=4.1e-02, kappa=4.961, |gamma|=0.1463, time=8.1min
Epoch  290: Loss=47.3208, LR=3.8e-02, kappa=5.013, |gamma|=0.1462, time=8.3min
Epoch  300: Loss=47.0907, LR=3.5e-02, kappa=5.060, |gamma|=0.1462, time=8.6min
Epoch  310: Loss=46.8839, LR=3.2e-02, kappa=5.102, |gamma|=0.1461, time=8.9min
Epoch  320: Loss=46.6984, LR=2.9e-02, kappa=5.139, |gamma|=0.1460, time=9.2min
Epoch  330: Loss=46.5323, LR=2.6e-02, kappa=5.171, |gamma|=0.1459, time=9.5min
Epoch  340: Loss=46.3841, LR=2.4e-02, kappa=5.200, |gamma|=0.1457, time=9.8min
Epoch  350: Loss=46.2523, LR=2.1e-02, kappa=5.226, |gamma|=0.1456, time=10.1min
Epoch  360: Loss=46.1356, LR=1.9e-02, kappa=5.248, |gamma|=0.1455, time=10.3min
Epoch  370: Loss=46.0326, LR=1.6e-02, kappa=5.267, |gamma|=0.1454, time=10.6min
Epoch  380: Loss=45.9422, LR=1.4e-02, kappa=5.284, |gamma|=0.1453, time=10.9min
Epoch  390: Loss=45.8635, LR=1.2e-02, kappa=5.299, |gamma|=0.1452, time=11.2min
Epoch  400: Loss=45.7953, LR=1.0e-02, kappa=5.311, |gamma|=0.1451, time=11.5min
Epoch  410: Loss=45.7368, LR=8.5e-03, kappa=5.321, |gamma|=0.1451, time=11.8min
Epoch  420: Loss=45.6869, LR=7.0e-03, kappa=5.329, |gamma|=0.1450, time=12.1min
Epoch  430: Loss=45.6448, LR=5.6e-03, kappa=5.336, |gamma|=0.1449, time=12.3min
Epoch  440: Loss=45.6096, LR=4.4e-03, kappa=5.341, |gamma|=0.1448, time=12.6min
Epoch  450: Loss=45.5804, LR=3.3e-03, kappa=5.346, |gamma|=0.1448, time=12.9min
Epoch  460: Loss=45.5562, LR=2.5e-03, kappa=5.349, |gamma|=0.1447, time=13.2min
Epoch  470: Loss=45.5361, LR=1.8e-03, kappa=5.351, |gamma|=0.1447, time=13.5min
Epoch  480: Loss=45.5191, LR=1.4e-03, kappa=5.353, |gamma|=0.1447, time=13.8min
Epoch  490: Loss=45.5043, LR=1.1e-03, kappa=5.354, |gamma|=0.1446, time=14.1min

Training complete: 500 epochs in 14.3 min
Final loss: 45.4920

Final params: kappa=5.3553, mean|gamma|=0.1446
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2/enrollment_model_REPARAM_W0.0001_batch_240000_250000.pt

Batch 25 DONE in 14.5 min (24/39 completed)

======================================================================
BATCH 26/40: samples 250000-260000
Elapsed: 351 min | Est remaining: ~225 min (3.8 hrs)
======================================================================

============================================================
REPARAM v2 Training: samples 250000 to 260000
Epochs: 500, LR: 0.1, grad_clip: 5.0
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 88 diseases
Cluster 1: 17 diseases
Cluster 2: 13 diseases
Cluster 3: 17 diseases
Cluster 4: 17 diseases
Cluster 5: 9 diseases
Cluster 6: 10 diseases
Cluster 7: 21 diseases
Cluster 8: 23 diseases
Cluster 9: 4 diseases
Cluster 10: 8 diseases
Cluster 11: 9 diseases
Cluster 12: 4 diseases
Cluster 13: 22 diseases
Cluster 14: 21 diseases
Cluster 15: 11 diseases
Cluster 16: 26 diseases
Cluster 17: 6 diseases
Cluster 18: 8 diseases
Cluster 19: 14 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=63.2965, LR=1.0e-01, kappa=1.100, |gamma|=0.1009, time=0.0min
Epoch   10: Loss=138.9762, LR=1.0e-01, kappa=2.023, |gamma|=0.0943, time=0.3min
Epoch   20: Loss=100.1282, LR=1.0e-01, kappa=2.659, |gamma|=0.1321, time=0.6min
Epoch   30: Loss=72.1725, LR=9.9e-02, kappa=2.989, |gamma|=0.1499, time=0.9min
Epoch   40: Loss=62.4226, LR=9.8e-02, kappa=3.112, |gamma|=0.1560, time=1.2min
Epoch   50: Loss=59.4418, LR=9.7e-02, kappa=3.132, |gamma|=0.1586, time=1.5min
Epoch   60: Loss=58.1406, LR=9.6e-02, kappa=3.123, |gamma|=0.1582, time=1.7min
Epoch   70: Loss=57.3689, LR=9.5e-02, kappa=3.122, |gamma|=0.1587, time=2.0min
Epoch   80: Loss=56.9117, LR=9.4e-02, kappa=3.135, |gamma|=0.1599, time=2.3min
Epoch   90: Loss=56.5593, LR=9.2e-02, kappa=3.157, |gamma|=0.1610, time=2.6min
Epoch  100: Loss=56.1604, LR=9.0e-02, kappa=3.184, |gamma|=0.1618, time=2.9min
Epoch  110: Loss=55.7372, LR=8.8e-02, kappa=3.220, |gamma|=0.1620, time=3.2min
Epoch  120: Loss=55.2712, LR=8.6e-02, kappa=3.272, |gamma|=0.1610, time=3.5min
Epoch  130: Loss=54.7555, LR=8.4e-02, kappa=3.340, |gamma|=0.1591, time=3.7min
Epoch  140: Loss=54.1869, LR=8.2e-02, kappa=3.428, |gamma|=0.1567, time=4.0min
Epoch  150: Loss=53.5769, LR=7.9e-02, kappa=3.536, |gamma|=0.1547, time=4.3min
Epoch  160: Loss=52.9208, LR=7.7e-02, kappa=3.659, |gamma|=0.1521, time=4.6min
Epoch  170: Loss=52.2579, LR=7.4e-02, kappa=3.794, |gamma|=0.1498, time=4.9min
Epoch  180: Loss=51.5722, LR=7.1e-02, kappa=3.937, |gamma|=0.1471, time=5.2min
Epoch  190: Loss=50.9069, LR=6.8e-02, kappa=4.082, |gamma|=0.1447, time=5.5min
Epoch  200: Loss=50.2721, LR=6.6e-02, kappa=4.227, |gamma|=0.1440, time=5.8min
Epoch  210: Loss=49.6573, LR=6.3e-02, kappa=4.369, |gamma|=0.1416, time=6.1min
Epoch  220: Loss=49.0900, LR=5.9e-02, kappa=4.503, |gamma|=0.1399, time=6.4min
Epoch  230: Loss=48.5681, LR=5.6e-02, kappa=4.629, |gamma|=0.1382, time=6.6min
Epoch  240: Loss=48.0953, LR=5.3e-02, kappa=4.744, |gamma|=0.1368, time=6.9min
Epoch  250: Loss=47.6674, LR=5.0e-02, kappa=4.849, |gamma|=0.1359, time=7.2min
Epoch  260: Loss=47.2850, LR=4.7e-02, kappa=4.942, |gamma|=0.1348, time=7.5min
Epoch  270: Loss=46.9442, LR=4.4e-02, kappa=5.025, |gamma|=0.1341, time=7.8min
Epoch  280: Loss=46.6405, LR=4.1e-02, kappa=5.099, |gamma|=0.1335, time=8.1min
Epoch  290: Loss=46.3695, LR=3.8e-02, kappa=5.165, |gamma|=0.1329, time=8.4min
Epoch  300: Loss=46.1261, LR=3.5e-02, kappa=5.223, |gamma|=0.1323, time=8.7min
Epoch  310: Loss=45.9072, LR=3.2e-02, kappa=5.275, |gamma|=0.1318, time=8.9min
Epoch  320: Loss=45.7113, LR=2.9e-02, kappa=5.322, |gamma|=0.1315, time=9.2min
Epoch  330: Loss=45.5370, LR=2.6e-02, kappa=5.364, |gamma|=0.1312, time=9.5min
Epoch  340: Loss=45.3825, LR=2.4e-02, kappa=5.400, |gamma|=0.1310, time=9.8min
Epoch  350: Loss=45.2459, LR=2.1e-02, kappa=5.432, |gamma|=0.1308, time=10.1min
Epoch  360: Loss=45.1257, LR=1.9e-02, kappa=5.460, |gamma|=0.1306, time=10.4min
Epoch  370: Loss=45.0203, LR=1.6e-02, kappa=5.484, |gamma|=0.1304, time=10.7min
Epoch  380: Loss=44.9282, LR=1.4e-02, kappa=5.505, |gamma|=0.1302, time=10.9min
Epoch  390: Loss=44.8484, LR=1.2e-02, kappa=5.522, |gamma|=0.1300, time=11.2min
Epoch  400: Loss=44.7796, LR=1.0e-02, kappa=5.537, |gamma|=0.1298, time=11.5min
Epoch  410: Loss=44.7207, LR=8.5e-03, kappa=5.549, |gamma|=0.1297, time=11.8min
Epoch  420: Loss=44.6707, LR=7.0e-03, kappa=5.560, |gamma|=0.1296, time=12.1min
Epoch  430: Loss=44.6287, LR=5.6e-03, kappa=5.568, |gamma|=0.1295, time=12.4min
Epoch  440: Loss=44.5938, LR=4.4e-03, kappa=5.574, |gamma|=0.1294, time=12.7min
Epoch  450: Loss=44.5648, LR=3.3e-03, kappa=5.579, |gamma|=0.1293, time=12.9min
Epoch  460: Loss=44.5410, LR=2.5e-03, kappa=5.583, |gamma|=0.1292, time=13.2min
Epoch  470: Loss=44.5214, LR=1.8e-03, kappa=5.586, |gamma|=0.1292, time=13.5min
Epoch  480: Loss=44.5049, LR=1.4e-03, kappa=5.588, |gamma|=0.1291, time=13.8min
Epoch  490: Loss=44.4906, LR=1.1e-03, kappa=5.590, |gamma|=0.1291, time=14.1min

Training complete: 500 epochs in 14.4 min
Final loss: 44.4788

Final params: kappa=5.5909, mean|gamma|=0.1291
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2/enrollment_model_REPARAM_W0.0001_batch_250000_260000.pt

Batch 26 DONE in 14.5 min (25/39 completed)

======================================================================
BATCH 27/40: samples 260000-270000
Elapsed: 365 min | Est remaining: ~210 min (3.5 hrs)
======================================================================

============================================================
REPARAM v2 Training: samples 260000 to 270000
Epochs: 500, LR: 0.1, grad_clip: 5.0
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 28 diseases
Cluster 1: 16 diseases
Cluster 2: 17 diseases
Cluster 3: 14 diseases
Cluster 4: 6 diseases
Cluster 5: 23 diseases
Cluster 6: 28 diseases
Cluster 7: 8 diseases
Cluster 8: 10 diseases
Cluster 9: 9 diseases
Cluster 10: 10 diseases
Cluster 11: 65 diseases
Cluster 12: 14 diseases
Cluster 13: 13 diseases
Cluster 14: 2 diseases
Cluster 15: 6 diseases
Cluster 16: 20 diseases
Cluster 17: 5 diseases
Cluster 18: 47 diseases
Cluster 19: 7 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=63.6040, LR=1.0e-01, kappa=1.100, |gamma|=0.1009, time=0.0min
Epoch   10: Loss=138.8063, LR=1.0e-01, kappa=2.023, |gamma|=0.0938, time=0.3min
Epoch   20: Loss=100.2565, LR=1.0e-01, kappa=2.662, |gamma|=0.1323, time=0.6min
Epoch   30: Loss=72.4461, LR=9.9e-02, kappa=2.995, |gamma|=0.1533, time=0.9min
Epoch   40: Loss=62.6921, LR=9.8e-02, kappa=3.121, |gamma|=0.1591, time=1.2min
Epoch   50: Loss=59.6923, LR=9.7e-02, kappa=3.141, |gamma|=0.1583, time=1.4min
Epoch   60: Loss=58.3751, LR=9.6e-02, kappa=3.131, |gamma|=0.1542, time=1.7min
Epoch   70: Loss=57.5950, LR=9.5e-02, kappa=3.132, |gamma|=0.1553, time=2.0min
Epoch   80: Loss=57.1322, LR=9.4e-02, kappa=3.148, |gamma|=0.1555, time=2.3min
Epoch   90: Loss=56.7693, LR=9.2e-02, kappa=3.176, |gamma|=0.1566, time=2.6min
Epoch  100: Loss=56.3557, LR=9.0e-02, kappa=3.214, |gamma|=0.1578, time=2.9min
Epoch  110: Loss=55.9123, LR=8.8e-02, kappa=3.262, |gamma|=0.1586, time=3.2min
Epoch  120: Loss=55.4248, LR=8.6e-02, kappa=3.324, |gamma|=0.1586, time=3.4min
Epoch  130: Loss=54.8784, LR=8.4e-02, kappa=3.402, |gamma|=0.1570, time=3.7min
Epoch  140: Loss=54.2854, LR=8.2e-02, kappa=3.501, |gamma|=0.1558, time=4.0min
Epoch  150: Loss=53.6334, LR=7.9e-02, kappa=3.617, |gamma|=0.1537, time=4.3min
Epoch  160: Loss=52.9546, LR=7.7e-02, kappa=3.746, |gamma|=0.1517, time=4.6min
Epoch  170: Loss=52.2778, LR=7.4e-02, kappa=3.884, |gamma|=0.1496, time=4.9min
Epoch  180: Loss=51.5912, LR=7.1e-02, kappa=4.025, |gamma|=0.1485, time=5.2min
Epoch  190: Loss=50.9389, LR=6.8e-02, kappa=4.164, |gamma|=0.1473, time=5.4min
Epoch  200: Loss=50.3262, LR=6.6e-02, kappa=4.297, |gamma|=0.1463, time=5.7min
Epoch  210: Loss=49.7587, LR=6.3e-02, kappa=4.422, |gamma|=0.1451, time=6.0min
Epoch  220: Loss=49.2494, LR=5.9e-02, kappa=4.537, |gamma|=0.1435, time=6.3min
Epoch  230: Loss=48.7656, LR=5.6e-02, kappa=4.641, |gamma|=0.1425, time=6.6min
Epoch  240: Loss=48.3333, LR=5.3e-02, kappa=4.735, |gamma|=0.1421, time=6.9min
Epoch  250: Loss=47.9457, LR=5.0e-02, kappa=4.820, |gamma|=0.1413, time=7.1min
Epoch  260: Loss=47.5957, LR=4.7e-02, kappa=4.896, |gamma|=0.1406, time=7.4min
Epoch  270: Loss=47.2805, LR=4.4e-02, kappa=4.964, |gamma|=0.1398, time=7.7min
Epoch  280: Loss=46.9966, LR=4.1e-02, kappa=5.025, |gamma|=0.1391, time=8.0min
Epoch  290: Loss=46.7410, LR=3.8e-02, kappa=5.079, |gamma|=0.1383, time=8.3min
Epoch  300: Loss=46.5111, LR=3.5e-02, kappa=5.127, |gamma|=0.1376, time=8.6min
Epoch  310: Loss=46.3045, LR=3.2e-02, kappa=5.170, |gamma|=0.1370, time=8.8min
Epoch  320: Loss=46.1191, LR=2.9e-02, kappa=5.208, |gamma|=0.1364, time=9.1min
Epoch  330: Loss=45.9532, LR=2.6e-02, kappa=5.242, |gamma|=0.1359, time=9.4min
Epoch  340: Loss=45.8050, LR=2.4e-02, kappa=5.272, |gamma|=0.1354, time=9.7min
Epoch  350: Loss=45.6731, LR=2.1e-02, kappa=5.298, |gamma|=0.1350, time=10.0min
Epoch  360: Loss=45.5563, LR=1.9e-02, kappa=5.321, |gamma|=0.1347, time=10.3min
Epoch  370: Loss=45.4532, LR=1.6e-02, kappa=5.341, |gamma|=0.1343, time=10.6min
Epoch  380: Loss=45.3628, LR=1.4e-02, kappa=5.359, |gamma|=0.1340, time=10.9min
Epoch  390: Loss=45.2840, LR=1.2e-02, kappa=5.374, |gamma|=0.1337, time=11.2min
Epoch  400: Loss=45.2159, LR=1.0e-02, kappa=5.386, |gamma|=0.1335, time=11.5min
Epoch  410: Loss=45.1573, LR=8.5e-03, kappa=5.397, |gamma|=0.1333, time=11.8min
Epoch  420: Loss=45.1075, LR=7.0e-03, kappa=5.405, |gamma|=0.1331, time=12.0min
Epoch  430: Loss=45.0656, LR=5.6e-03, kappa=5.412, |gamma|=0.1330, time=12.3min
Epoch  440: Loss=45.0305, LR=4.4e-03, kappa=5.418, |gamma|=0.1329, time=12.6min
Epoch  450: Loss=45.0014, LR=3.3e-03, kappa=5.422, |gamma|=0.1328, time=12.9min
Epoch  460: Loss=44.9774, LR=2.5e-03, kappa=5.425, |gamma|=0.1327, time=13.2min
Epoch  470: Loss=44.9575, LR=1.8e-03, kappa=5.428, |gamma|=0.1326, time=13.5min
Epoch  480: Loss=44.9408, LR=1.4e-03, kappa=5.429, |gamma|=0.1326, time=13.8min
Epoch  490: Loss=44.9263, LR=1.1e-03, kappa=5.431, |gamma|=0.1325, time=14.1min

Training complete: 500 epochs in 14.3 min
Final loss: 44.9142

Final params: kappa=5.4318, mean|gamma|=0.1325
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2/enrollment_model_REPARAM_W0.0001_batch_260000_270000.pt

Batch 27 DONE in 14.5 min (26/39 completed)

======================================================================
BATCH 28/40: samples 270000-280000
Elapsed: 380 min | Est remaining: ~195 min (3.2 hrs)
======================================================================

============================================================
REPARAM v2 Training: samples 270000 to 280000
Epochs: 500, LR: 0.1, grad_clip: 5.0
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 15 diseases
Cluster 1: 95 diseases
Cluster 2: 6 diseases
Cluster 3: 25 diseases
Cluster 4: 8 diseases
Cluster 5: 14 diseases
Cluster 6: 7 diseases
Cluster 7: 22 diseases
Cluster 8: 24 diseases
Cluster 9: 8 diseases
Cluster 10: 13 diseases
Cluster 11: 12 diseases
Cluster 12: 9 diseases
Cluster 13: 15 diseases
Cluster 14: 13 diseases
Cluster 15: 18 diseases
Cluster 16: 20 diseases
Cluster 17: 5 diseases
Cluster 18: 9 diseases
Cluster 19: 10 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=63.9523, LR=1.0e-01, kappa=1.100, |gamma|=0.1011, time=0.0min
Epoch   10: Loss=137.6595, LR=1.0e-01, kappa=2.025, |gamma|=0.0939, time=0.3min
Epoch   20: Loss=100.1227, LR=1.0e-01, kappa=2.666, |gamma|=0.1307, time=0.6min
Epoch   30: Loss=72.8219, LR=9.9e-02, kappa=3.000, |gamma|=0.1505, time=0.9min
Epoch   40: Loss=63.0721, LR=9.8e-02, kappa=3.126, |gamma|=0.1562, time=1.2min
Epoch   50: Loss=60.0243, LR=9.7e-02, kappa=3.146, |gamma|=0.1554, time=1.4min
Epoch   60: Loss=58.6828, LR=9.6e-02, kappa=3.139, |gamma|=0.1535, time=1.7min
Epoch   70: Loss=57.9053, LR=9.5e-02, kappa=3.140, |gamma|=0.1535, time=2.0min
Epoch   80: Loss=57.4547, LR=9.4e-02, kappa=3.159, |gamma|=0.1537, time=2.3min
Epoch   90: Loss=57.0911, LR=9.2e-02, kappa=3.194, |gamma|=0.1527, time=2.6min
Epoch  100: Loss=56.6767, LR=9.0e-02, kappa=3.240, |gamma|=0.1510, time=2.9min
Epoch  110: Loss=56.2263, LR=8.8e-02, kappa=3.298, |gamma|=0.1508, time=3.1min
Epoch  120: Loss=55.7164, LR=8.6e-02, kappa=3.373, |gamma|=0.1497, time=3.4min
Epoch  130: Loss=55.1563, LR=8.4e-02, kappa=3.469, |gamma|=0.1481, time=3.7min
Epoch  140: Loss=54.4995, LR=8.2e-02, kappa=3.587, |gamma|=0.1452, time=4.0min
Epoch  150: Loss=53.7994, LR=7.9e-02, kappa=3.726, |gamma|=0.1427, time=4.3min
Epoch  160: Loss=53.0807, LR=7.7e-02, kappa=3.882, |gamma|=0.1403, time=4.6min
Epoch  170: Loss=52.3325, LR=7.4e-02, kappa=4.045, |gamma|=0.1374, time=4.9min
Epoch  180: Loss=51.5938, LR=7.1e-02, kappa=4.209, |gamma|=0.1337, time=5.2min
Epoch  190: Loss=50.8958, LR=6.8e-02, kappa=4.370, |gamma|=0.1316, time=5.5min
Epoch  200: Loss=50.2478, LR=6.6e-02, kappa=4.525, |gamma|=0.1290, time=5.8min
Epoch  210: Loss=49.6531, LR=6.3e-02, kappa=4.668, |gamma|=0.1271, time=6.1min
Epoch  220: Loss=49.1123, LR=5.9e-02, kappa=4.799, |gamma|=0.1251, time=6.4min
Epoch  230: Loss=48.6260, LR=5.6e-02, kappa=4.916, |gamma|=0.1238, time=6.7min
Epoch  240: Loss=48.1904, LR=5.3e-02, kappa=5.020, |gamma|=0.1225, time=6.9min
Epoch  250: Loss=47.8008, LR=5.0e-02, kappa=5.113, |gamma|=0.1217, time=7.2min
Epoch  260: Loss=47.4519, LR=4.7e-02, kappa=5.195, |gamma|=0.1212, time=7.5min
Epoch  270: Loss=47.1391, LR=4.4e-02, kappa=5.268, |gamma|=0.1206, time=7.8min
Epoch  280: Loss=46.8586, LR=4.1e-02, kappa=5.332, |gamma|=0.1198, time=8.1min
Epoch  290: Loss=46.6069, LR=3.8e-02, kappa=5.388, |gamma|=0.1189, time=8.4min
Epoch  300: Loss=46.3811, LR=3.5e-02, kappa=5.438, |gamma|=0.1180, time=8.7min
Epoch  310: Loss=46.1787, LR=3.2e-02, kappa=5.482, |gamma|=0.1173, time=8.9min
Epoch  320: Loss=45.9974, LR=2.9e-02, kappa=5.521, |gamma|=0.1166, time=9.2min
Epoch  330: Loss=45.8353, LR=2.6e-02, kappa=5.555, |gamma|=0.1161, time=9.5min
Epoch  340: Loss=45.6908, LR=2.4e-02, kappa=5.585, |gamma|=0.1156, time=9.8min
Epoch  350: Loss=45.5623, LR=2.1e-02, kappa=5.612, |gamma|=0.1153, time=10.1min
Epoch  360: Loss=45.4485, LR=1.9e-02, kappa=5.635, |gamma|=0.1151, time=10.4min
Epoch  370: Loss=45.3482, LR=1.6e-02, kappa=5.655, |gamma|=0.1150, time=10.7min
Epoch  380: Loss=45.2603, LR=1.4e-02, kappa=5.672, |gamma|=0.1148, time=10.9min
Epoch  390: Loss=45.1837, LR=1.2e-02, kappa=5.687, |gamma|=0.1146, time=11.2min
Epoch  400: Loss=45.1174, LR=1.0e-02, kappa=5.699, |gamma|=0.1144, time=11.5min
Epoch  410: Loss=45.0605, LR=8.5e-03, kappa=5.709, |gamma|=0.1142, time=11.8min
Epoch  420: Loss=45.0122, LR=7.0e-03, kappa=5.718, |gamma|=0.1140, time=12.1min
Epoch  430: Loss=44.9714, LR=5.6e-03, kappa=5.725, |gamma|=0.1139, time=12.4min
Epoch  440: Loss=44.9373, LR=4.4e-03, kappa=5.730, |gamma|=0.1137, time=12.6min
Epoch  450: Loss=44.9090, LR=3.3e-03, kappa=5.734, |gamma|=0.1136, time=12.9min
Epoch  460: Loss=44.8857, LR=2.5e-03, kappa=5.738, |gamma|=0.1136, time=13.2min
Epoch  470: Loss=44.8664, LR=1.8e-03, kappa=5.740, |gamma|=0.1135, time=13.5min
Epoch  480: Loss=44.8501, LR=1.4e-03, kappa=5.742, |gamma|=0.1134, time=13.8min
Epoch  490: Loss=44.8360, LR=1.1e-03, kappa=5.743, |gamma|=0.1134, time=14.1min

Training complete: 500 epochs in 14.3 min
Final loss: 44.8243

Final params: kappa=5.7441, mean|gamma|=0.1133
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2/enrollment_model_REPARAM_W0.0001_batch_270000_280000.pt

Batch 28 DONE in 14.5 min (27/39 completed)

======================================================================
BATCH 29/40: samples 280000-290000
Elapsed: 394 min | Est remaining: ~180 min (3.0 hrs)
======================================================================

============================================================
REPARAM v2 Training: samples 280000 to 290000
Epochs: 500, LR: 0.1, grad_clip: 5.0
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 13 diseases
Cluster 1: 15 diseases
Cluster 2: 11 diseases
Cluster 3: 102 diseases
Cluster 4: 17 diseases
Cluster 5: 26 diseases
Cluster 6: 9 diseases
Cluster 7: 14 diseases
Cluster 8: 7 diseases
Cluster 9: 10 diseases
Cluster 10: 28 diseases
Cluster 11: 5 diseases
Cluster 12: 16 diseases
Cluster 13: 24 diseases
Cluster 14: 8 diseases
Cluster 15: 5 diseases
Cluster 16: 10 diseases
Cluster 17: 12 diseases
Cluster 18: 9 diseases
Cluster 19: 7 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=63.6860, LR=1.0e-01, kappa=1.100, |gamma|=0.1012, time=0.0min
Epoch   10: Loss=138.7325, LR=1.0e-01, kappa=2.023, |gamma|=0.0950, time=0.3min
Epoch   20: Loss=100.2953, LR=1.0e-01, kappa=2.659, |gamma|=0.1296, time=0.6min
Epoch   30: Loss=72.5441, LR=9.9e-02, kappa=2.991, |gamma|=0.1466, time=0.9min
Epoch   40: Loss=62.7858, LR=9.8e-02, kappa=3.115, |gamma|=0.1551, time=1.2min
Epoch   50: Loss=59.7799, LR=9.7e-02, kappa=3.136, |gamma|=0.1566, time=1.4min
Epoch   60: Loss=58.4592, LR=9.6e-02, kappa=3.130, |gamma|=0.1577, time=1.7min
Epoch   70: Loss=57.6767, LR=9.5e-02, kappa=3.132, |gamma|=0.1583, time=2.0min
Epoch   80: Loss=57.2119, LR=9.4e-02, kappa=3.150, |gamma|=0.1573, time=2.3min
Epoch   90: Loss=56.8412, LR=9.2e-02, kappa=3.182, |gamma|=0.1551, time=2.6min
Epoch  100: Loss=56.4295, LR=9.0e-02, kappa=3.224, |gamma|=0.1545, time=2.8min
Epoch  110: Loss=55.9853, LR=8.8e-02, kappa=3.273, |gamma|=0.1534, time=3.1min
Epoch  120: Loss=55.5028, LR=8.6e-02, kappa=3.332, |gamma|=0.1526, time=3.4min
Epoch  130: Loss=54.9849, LR=8.4e-02, kappa=3.404, |gamma|=0.1514, time=3.7min
Epoch  140: Loss=54.4099, LR=8.2e-02, kappa=3.491, |gamma|=0.1497, time=4.0min
Epoch  150: Loss=53.8021, LR=7.9e-02, kappa=3.594, |gamma|=0.1481, time=4.3min
Epoch  160: Loss=53.1801, LR=7.7e-02, kappa=3.709, |gamma|=0.1469, time=4.6min
Epoch  170: Loss=52.5187, LR=7.4e-02, kappa=3.834, |gamma|=0.1458, time=4.8min
Epoch  180: Loss=51.8661, LR=7.1e-02, kappa=3.965, |gamma|=0.1449, time=5.1min
Epoch  190: Loss=51.2258, LR=6.8e-02, kappa=4.098, |gamma|=0.1440, time=5.4min
Epoch  200: Loss=50.6255, LR=6.6e-02, kappa=4.228, |gamma|=0.1437, time=5.7min
Epoch  210: Loss=50.0406, LR=6.3e-02, kappa=4.353, |gamma|=0.1431, time=6.0min
Epoch  220: Loss=49.5008, LR=5.9e-02, kappa=4.471, |gamma|=0.1419, time=6.3min
Epoch  230: Loss=49.0087, LR=5.6e-02, kappa=4.581, |gamma|=0.1410, time=6.5min
Epoch  240: Loss=48.5598, LR=5.3e-02, kappa=4.681, |gamma|=0.1403, time=6.8min
Epoch  250: Loss=48.1523, LR=5.0e-02, kappa=4.772, |gamma|=0.1397, time=7.1min
Epoch  260: Loss=47.7834, LR=4.7e-02, kappa=4.854, |gamma|=0.1390, time=7.4min
Epoch  270: Loss=47.4497, LR=4.4e-02, kappa=4.928, |gamma|=0.1383, time=7.7min
Epoch  280: Loss=47.1485, LR=4.1e-02, kappa=4.994, |gamma|=0.1377, time=7.9min
Epoch  290: Loss=46.8768, LR=3.8e-02, kappa=5.054, |gamma|=0.1370, time=8.2min
Epoch  300: Loss=46.6321, LR=3.5e-02, kappa=5.107, |gamma|=0.1363, time=8.5min
Epoch  310: Loss=46.4120, LR=3.2e-02, kappa=5.154, |gamma|=0.1356, time=8.8min
Epoch  320: Loss=46.2145, LR=2.9e-02, kappa=5.196, |gamma|=0.1350, time=9.1min
Epoch  330: Loss=46.0377, LR=2.6e-02, kappa=5.233, |gamma|=0.1344, time=9.3min
Epoch  340: Loss=45.8798, LR=2.4e-02, kappa=5.266, |gamma|=0.1339, time=9.6min
Epoch  350: Loss=45.7394, LR=2.1e-02, kappa=5.295, |gamma|=0.1334, time=9.9min
Epoch  360: Loss=45.6150, LR=1.9e-02, kappa=5.320, |gamma|=0.1330, time=10.2min
Epoch  370: Loss=45.5053, LR=1.6e-02, kappa=5.342, |gamma|=0.1326, time=10.5min
Epoch  380: Loss=45.4092, LR=1.4e-02, kappa=5.361, |gamma|=0.1322, time=10.7min
Epoch  390: Loss=45.3255, LR=1.2e-02, kappa=5.377, |gamma|=0.1318, time=11.0min
Epoch  400: Loss=45.2531, LR=1.0e-02, kappa=5.391, |gamma|=0.1315, time=11.3min
Epoch  410: Loss=45.1910, LR=8.5e-03, kappa=5.402, |gamma|=0.1313, time=11.6min
Epoch  420: Loss=45.1382, LR=7.0e-03, kappa=5.412, |gamma|=0.1310, time=11.9min
Epoch  430: Loss=45.0938, LR=5.6e-03, kappa=5.419, |gamma|=0.1308, time=12.2min
Epoch  440: Loss=45.0567, LR=4.4e-03, kappa=5.425, |gamma|=0.1307, time=12.4min
Epoch  450: Loss=45.0260, LR=3.3e-03, kappa=5.430, |gamma|=0.1305, time=12.7min
Epoch  460: Loss=45.0007, LR=2.5e-03, kappa=5.433, |gamma|=0.1304, time=13.0min
Epoch  470: Loss=44.9799, LR=1.8e-03, kappa=5.436, |gamma|=0.1303, time=13.3min
Epoch  480: Loss=44.9624, LR=1.4e-03, kappa=5.438, |gamma|=0.1303, time=13.6min
Epoch  490: Loss=44.9472, LR=1.1e-03, kappa=5.439, |gamma|=0.1302, time=13.8min

Training complete: 500 epochs in 14.1 min
Final loss: 44.9346

Final params: kappa=5.4406, mean|gamma|=0.1302
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2/enrollment_model_REPARAM_W0.0001_batch_280000_290000.pt

Batch 29 DONE in 14.2 min (28/39 completed)

======================================================================
BATCH 30/40: samples 290000-300000
Elapsed: 408 min | Est remaining: ~165 min (2.8 hrs)
======================================================================

============================================================
REPARAM v2 Training: samples 290000 to 300000
Epochs: 500, LR: 0.1, grad_clip: 5.0
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 16 diseases
Cluster 1: 11 diseases
Cluster 2: 7 diseases
Cluster 3: 13 diseases
Cluster 4: 16 diseases
Cluster 5: 8 diseases
Cluster 6: 5 diseases
Cluster 7: 31 diseases
Cluster 8: 28 diseases
Cluster 9: 9 diseases
Cluster 10: 13 diseases
Cluster 11: 9 diseases
Cluster 12: 8 diseases
Cluster 13: 5 diseases
Cluster 14: 18 diseases
Cluster 15: 80 diseases
Cluster 16: 16 diseases
Cluster 17: 9 diseases
Cluster 18: 14 diseases
Cluster 19: 32 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=63.6402, LR=1.0e-01, kappa=1.100, |gamma|=0.1008, time=0.0min
Epoch   10: Loss=138.4459, LR=1.0e-01, kappa=2.023, |gamma|=0.0932, time=0.3min
Epoch   20: Loss=100.2157, LR=1.0e-01, kappa=2.659, |gamma|=0.1286, time=0.6min
Epoch   30: Loss=72.5580, LR=9.9e-02, kappa=2.990, |gamma|=0.1484, time=0.9min
Epoch   40: Loss=62.8089, LR=9.8e-02, kappa=3.110, |gamma|=0.1553, time=1.2min
Epoch   50: Loss=59.8049, LR=9.7e-02, kappa=3.124, |gamma|=0.1531, time=1.5min
Epoch   60: Loss=58.4915, LR=9.6e-02, kappa=3.110, |gamma|=0.1523, time=1.8min
Epoch   70: Loss=57.7239, LR=9.5e-02, kappa=3.104, |gamma|=0.1543, time=2.0min
Epoch   80: Loss=57.2765, LR=9.4e-02, kappa=3.114, |gamma|=0.1575, time=2.3min
Epoch   90: Loss=56.9237, LR=9.2e-02, kappa=3.136, |gamma|=0.1610, time=2.6min
Epoch  100: Loss=56.5308, LR=9.0e-02, kappa=3.167, |gamma|=0.1639, time=2.9min
Epoch  110: Loss=56.0992, LR=8.8e-02, kappa=3.206, |gamma|=0.1655, time=3.2min
Epoch  120: Loss=55.6264, LR=8.6e-02, kappa=3.258, |gamma|=0.1659, time=3.5min
Epoch  130: Loss=55.1002, LR=8.4e-02, kappa=3.328, |gamma|=0.1661, time=3.7min
Epoch  140: Loss=54.5115, LR=8.2e-02, kappa=3.417, |gamma|=0.1660, time=4.0min
Epoch  150: Loss=53.8707, LR=7.9e-02, kappa=3.526, |gamma|=0.1650, time=4.3min
Epoch  160: Loss=53.2051, LR=7.7e-02, kappa=3.651, |gamma|=0.1639, time=4.6min
Epoch  170: Loss=52.4881, LR=7.4e-02, kappa=3.788, |gamma|=0.1621, time=4.9min
Epoch  180: Loss=51.7808, LR=7.1e-02, kappa=3.932, |gamma|=0.1602, time=5.2min
Epoch  190: Loss=51.0856, LR=6.8e-02, kappa=4.082, |gamma|=0.1594, time=5.4min
Epoch  200: Loss=50.4315, LR=6.6e-02, kappa=4.232, |gamma|=0.1587, time=5.7min
Epoch  210: Loss=49.8168, LR=6.3e-02, kappa=4.375, |gamma|=0.1579, time=6.0min
Epoch  220: Loss=49.2564, LR=5.9e-02, kappa=4.507, |gamma|=0.1568, time=6.3min
Epoch  230: Loss=48.7507, LR=5.6e-02, kappa=4.628, |gamma|=0.1553, time=6.6min
Epoch  240: Loss=48.2956, LR=5.3e-02, kappa=4.737, |gamma|=0.1538, time=6.9min
Epoch  250: Loss=47.8875, LR=5.0e-02, kappa=4.835, |gamma|=0.1520, time=7.1min
Epoch  260: Loss=47.5218, LR=4.7e-02, kappa=4.922, |gamma|=0.1501, time=7.4min
Epoch  270: Loss=47.1942, LR=4.4e-02, kappa=5.000, |gamma|=0.1482, time=7.7min
Epoch  280: Loss=46.9007, LR=4.1e-02, kappa=5.069, |gamma|=0.1464, time=8.0min
Epoch  290: Loss=46.6378, LR=3.8e-02, kappa=5.130, |gamma|=0.1447, time=8.3min
Epoch  300: Loss=46.4023, LR=3.5e-02, kappa=5.185, |gamma|=0.1433, time=8.6min
Epoch  310: Loss=46.1916, LR=3.2e-02, kappa=5.234, |gamma|=0.1420, time=8.8min
Epoch  320: Loss=46.0034, LR=2.9e-02, kappa=5.277, |gamma|=0.1409, time=9.1min
Epoch  330: Loss=45.8356, LR=2.6e-02, kappa=5.315, |gamma|=0.1400, time=9.4min
Epoch  340: Loss=45.6864, LR=2.4e-02, kappa=5.349, |gamma|=0.1393, time=9.7min
Epoch  350: Loss=45.5540, LR=2.1e-02, kappa=5.379, |gamma|=0.1386, time=10.0min
Epoch  360: Loss=45.4371, LR=1.9e-02, kappa=5.405, |gamma|=0.1380, time=10.3min
Epoch  370: Loss=45.3342, LR=1.6e-02, kappa=5.427, |gamma|=0.1375, time=10.5min
Epoch  380: Loss=45.2442, LR=1.4e-02, kappa=5.447, |gamma|=0.1371, time=10.8min
Epoch  390: Loss=45.1658, LR=1.2e-02, kappa=5.463, |gamma|=0.1368, time=11.1min
Epoch  400: Loss=45.0981, LR=1.0e-02, kappa=5.478, |gamma|=0.1365, time=11.4min
Epoch  410: Loss=45.0401, LR=8.5e-03, kappa=5.489, |gamma|=0.1362, time=11.7min
Epoch  420: Loss=44.9907, LR=7.0e-03, kappa=5.499, |gamma|=0.1360, time=12.0min
Epoch  430: Loss=44.9491, LR=5.6e-03, kappa=5.507, |gamma|=0.1358, time=12.2min
Epoch  440: Loss=44.9143, LR=4.4e-03, kappa=5.513, |gamma|=0.1356, time=12.5min
Epoch  450: Loss=44.8854, LR=3.3e-03, kappa=5.518, |gamma|=0.1355, time=12.8min
Epoch  460: Loss=44.8615, LR=2.5e-03, kappa=5.522, |gamma|=0.1354, time=13.1min
Epoch  470: Loss=44.8417, LR=1.8e-03, kappa=5.525, |gamma|=0.1353, time=13.4min
Epoch  480: Loss=44.8250, LR=1.4e-03, kappa=5.527, |gamma|=0.1352, time=13.6min
Epoch  490: Loss=44.8104, LR=1.1e-03, kappa=5.528, |gamma|=0.1352, time=13.9min

Training complete: 500 epochs in 14.2 min
Final loss: 44.7983

Final params: kappa=5.5293, mean|gamma|=0.1351
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2/enrollment_model_REPARAM_W0.0001_batch_290000_300000.pt

Batch 30 DONE in 14.3 min (29/39 completed)

======================================================================
BATCH 31/40: samples 300000-310000
Elapsed: 423 min | Est remaining: ~150 min (2.5 hrs)
======================================================================

============================================================
REPARAM v2 Training: samples 300000 to 310000
Epochs: 500, LR: 0.1, grad_clip: 5.0
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 14 diseases
Cluster 1: 27 diseases
Cluster 2: 12 diseases
Cluster 3: 11 diseases
Cluster 4: 27 diseases
Cluster 5: 8 diseases
Cluster 6: 4 diseases
Cluster 7: 13 diseases
Cluster 8: 8 diseases
Cluster 9: 8 diseases
Cluster 10: 7 diseases
Cluster 11: 7 diseases
Cluster 12: 32 diseases
Cluster 13: 12 diseases
Cluster 14: 20 diseases
Cluster 15: 22 diseases
Cluster 16: 10 diseases
Cluster 17: 15 diseases
Cluster 18: 85 diseases
Cluster 19: 6 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=62.7682, LR=1.0e-01, kappa=1.100, |gamma|=0.1011, time=0.0min
Epoch   10: Loss=140.6807, LR=1.0e-01, kappa=2.020, |gamma|=0.0987, time=0.3min
Epoch   20: Loss=100.2806, LR=1.0e-01, kappa=2.652, |gamma|=0.1319, time=0.6min
Epoch   30: Loss=71.6039, LR=9.9e-02, kappa=2.979, |gamma|=0.1513, time=0.9min
Epoch   40: Loss=61.8887, LR=9.8e-02, kappa=3.101, |gamma|=0.1582, time=1.1min
Epoch   50: Loss=59.0030, LR=9.7e-02, kappa=3.118, |gamma|=0.1605, time=1.4min
Epoch   60: Loss=57.7584, LR=9.6e-02, kappa=3.107, |gamma|=0.1608, time=1.7min
Epoch   70: Loss=57.0066, LR=9.5e-02, kappa=3.102, |gamma|=0.1611, time=2.0min
Epoch   80: Loss=56.5505, LR=9.4e-02, kappa=3.111, |gamma|=0.1621, time=2.2min
Epoch   90: Loss=56.2071, LR=9.2e-02, kappa=3.131, |gamma|=0.1633, time=2.5min
Epoch  100: Loss=55.8284, LR=9.0e-02, kappa=3.158, |gamma|=0.1643, time=2.8min
Epoch  110: Loss=55.4155, LR=8.8e-02, kappa=3.194, |gamma|=0.1636, time=3.1min
Epoch  120: Loss=54.9646, LR=8.6e-02, kappa=3.244, |gamma|=0.1611, time=3.4min
Epoch  130: Loss=54.4543, LR=8.4e-02, kappa=3.312, |gamma|=0.1589, time=3.6min
Epoch  140: Loss=53.8968, LR=8.2e-02, kappa=3.398, |gamma|=0.1566, time=3.9min
Epoch  150: Loss=53.2932, LR=7.9e-02, kappa=3.501, |gamma|=0.1533, time=4.2min
Epoch  160: Loss=52.6520, LR=7.7e-02, kappa=3.618, |gamma|=0.1505, time=4.5min
Epoch  170: Loss=51.9841, LR=7.4e-02, kappa=3.745, |gamma|=0.1483, time=4.7min
Epoch  180: Loss=51.3233, LR=7.1e-02, kappa=3.878, |gamma|=0.1473, time=5.0min
Epoch  190: Loss=50.6742, LR=6.8e-02, kappa=4.012, |gamma|=0.1467, time=5.3min
Epoch  200: Loss=50.0550, LR=6.6e-02, kappa=4.143, |gamma|=0.1461, time=5.6min
Epoch  210: Loss=49.4752, LR=6.3e-02, kappa=4.268, |gamma|=0.1456, time=5.8min
Epoch  220: Loss=48.9428, LR=5.9e-02, kappa=4.385, |gamma|=0.1450, time=6.1min
Epoch  230: Loss=48.4570, LR=5.6e-02, kappa=4.492, |gamma|=0.1443, time=6.4min
Epoch  240: Loss=48.0164, LR=5.3e-02, kappa=4.589, |gamma|=0.1434, time=6.7min
Epoch  250: Loss=47.6182, LR=5.0e-02, kappa=4.677, |gamma|=0.1425, time=6.9min
Epoch  260: Loss=47.2593, LR=4.7e-02, kappa=4.756, |gamma|=0.1416, time=7.2min
Epoch  270: Loss=46.9364, LR=4.4e-02, kappa=4.827, |gamma|=0.1408, time=7.5min
Epoch  280: Loss=46.6462, LR=4.1e-02, kappa=4.890, |gamma|=0.1400, time=7.8min
Epoch  290: Loss=46.3856, LR=3.8e-02, kappa=4.946, |gamma|=0.1392, time=8.0min
Epoch  300: Loss=46.1519, LR=3.5e-02, kappa=4.996, |gamma|=0.1385, time=8.3min
Epoch  310: Loss=45.9426, LR=3.2e-02, kappa=5.041, |gamma|=0.1378, time=8.6min
Epoch  320: Loss=45.7554, LR=2.9e-02, kappa=5.080, |gamma|=0.1372, time=8.9min
Epoch  330: Loss=45.5884, LR=2.6e-02, kappa=5.115, |gamma|=0.1366, time=9.1min
Epoch  340: Loss=45.4398, LR=2.4e-02, kappa=5.146, |gamma|=0.1361, time=9.4min
Epoch  350: Loss=45.3079, LR=2.1e-02, kappa=5.173, |gamma|=0.1356, time=9.7min
Epoch  360: Loss=45.1914, LR=1.9e-02, kappa=5.196, |gamma|=0.1351, time=10.0min
Epoch  370: Loss=45.0888, LR=1.6e-02, kappa=5.217, |gamma|=0.1348, time=10.2min
Epoch  380: Loss=44.9990, LR=1.4e-02, kappa=5.235, |gamma|=0.1345, time=10.5min
Epoch  390: Loss=44.9209, LR=1.2e-02, kappa=5.250, |gamma|=0.1342, time=10.8min
Epoch  400: Loss=44.8535, LR=1.0e-02, kappa=5.263, |gamma|=0.1339, time=11.1min
Epoch  410: Loss=44.7957, LR=8.5e-03, kappa=5.273, |gamma|=0.1337, time=11.3min
Epoch  420: Loss=44.7465, LR=7.0e-03, kappa=5.282, |gamma|=0.1335, time=11.6min
Epoch  430: Loss=44.7051, LR=5.6e-03, kappa=5.289, |gamma|=0.1334, time=11.9min
Epoch  440: Loss=44.6705, LR=4.4e-03, kappa=5.295, |gamma|=0.1333, time=12.2min
Epoch  450: Loss=44.6418, LR=3.3e-03, kappa=5.300, |gamma|=0.1332, time=12.4min
Epoch  460: Loss=44.6182, LR=2.5e-03, kappa=5.303, |gamma|=0.1331, time=12.7min
Epoch  470: Loss=44.5986, LR=1.8e-03, kappa=5.305, |gamma|=0.1330, time=13.0min
Epoch  480: Loss=44.5821, LR=1.4e-03, kappa=5.307, |gamma|=0.1330, time=13.3min
Epoch  490: Loss=44.5678, LR=1.1e-03, kappa=5.309, |gamma|=0.1329, time=13.6min

Training complete: 500 epochs in 13.8 min
Final loss: 44.5559

Final params: kappa=5.3097, mean|gamma|=0.1329
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2/enrollment_model_REPARAM_W0.0001_batch_300000_310000.pt

Batch 31 DONE in 14.0 min (30/39 completed)

======================================================================
BATCH 32/40: samples 310000-320000
Elapsed: 437 min | Est remaining: ~135 min (2.2 hrs)
======================================================================

============================================================
REPARAM v2 Training: samples 310000 to 320000
Epochs: 500, LR: 0.1, grad_clip: 5.0
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 26 diseases
Cluster 1: 24 diseases
Cluster 2: 21 diseases
Cluster 3: 7 diseases
Cluster 4: 8 diseases
Cluster 5: 5 diseases
Cluster 6: 86 diseases
Cluster 7: 9 diseases
Cluster 8: 8 diseases
Cluster 9: 7 diseases
Cluster 10: 25 diseases
Cluster 11: 21 diseases
Cluster 12: 13 diseases
Cluster 13: 10 diseases
Cluster 14: 5 diseases
Cluster 15: 13 diseases
Cluster 16: 11 diseases
Cluster 17: 8 diseases
Cluster 18: 29 diseases
Cluster 19: 12 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=63.3621, LR=1.0e-01, kappa=1.100, |gamma|=0.1014, time=0.0min
Epoch   10: Loss=139.2803, LR=1.0e-01, kappa=2.022, |gamma|=0.0990, time=0.3min
Epoch   20: Loss=100.2395, LR=1.0e-01, kappa=2.656, |gamma|=0.1336, time=0.6min
Epoch   30: Loss=72.1973, LR=9.9e-02, kappa=2.984, |gamma|=0.1460, time=0.9min
Epoch   40: Loss=62.4562, LR=9.8e-02, kappa=3.109, |gamma|=0.1458, time=1.1min
Epoch   50: Loss=59.4868, LR=9.7e-02, kappa=3.135, |gamma|=0.1480, time=1.4min
Epoch   60: Loss=58.1817, LR=9.6e-02, kappa=3.132, |gamma|=0.1444, time=1.7min
Epoch   70: Loss=57.3983, LR=9.5e-02, kappa=3.138, |gamma|=0.1456, time=2.0min
Epoch   80: Loss=56.9228, LR=9.4e-02, kappa=3.159, |gamma|=0.1473, time=2.2min
Epoch   90: Loss=56.5428, LR=9.2e-02, kappa=3.196, |gamma|=0.1484, time=2.5min
Epoch  100: Loss=56.1141, LR=9.0e-02, kappa=3.244, |gamma|=0.1493, time=2.8min
Epoch  110: Loss=55.6488, LR=8.8e-02, kappa=3.304, |gamma|=0.1493, time=3.1min
Epoch  120: Loss=55.1461, LR=8.6e-02, kappa=3.378, |gamma|=0.1492, time=3.3min
Epoch  130: Loss=54.5606, LR=8.4e-02, kappa=3.471, |gamma|=0.1483, time=3.6min
Epoch  140: Loss=53.9259, LR=8.2e-02, kappa=3.583, |gamma|=0.1467, time=3.9min
Epoch  150: Loss=53.2473, LR=7.9e-02, kappa=3.714, |gamma|=0.1441, time=4.2min
Epoch  160: Loss=52.5422, LR=7.7e-02, kappa=3.859, |gamma|=0.1412, time=4.4min
Epoch  170: Loss=51.8195, LR=7.4e-02, kappa=4.013, |gamma|=0.1390, time=4.7min
Epoch  180: Loss=51.1109, LR=7.1e-02, kappa=4.169, |gamma|=0.1367, time=5.0min
Epoch  190: Loss=50.4343, LR=6.8e-02, kappa=4.323, |gamma|=0.1348, time=5.3min
Epoch  200: Loss=49.8057, LR=6.6e-02, kappa=4.469, |gamma|=0.1329, time=5.5min
Epoch  210: Loss=49.2354, LR=6.3e-02, kappa=4.604, |gamma|=0.1316, time=5.8min
Epoch  220: Loss=48.7054, LR=5.9e-02, kappa=4.727, |gamma|=0.1300, time=6.1min
Epoch  230: Loss=48.2283, LR=5.6e-02, kappa=4.839, |gamma|=0.1287, time=6.3min
Epoch  240: Loss=47.7975, LR=5.3e-02, kappa=4.939, |gamma|=0.1268, time=6.6min
Epoch  250: Loss=47.4128, LR=5.0e-02, kappa=5.027, |gamma|=0.1262, time=6.9min
Epoch  260: Loss=47.0695, LR=4.7e-02, kappa=5.106, |gamma|=0.1258, time=7.2min
Epoch  270: Loss=46.7621, LR=4.4e-02, kappa=5.175, |gamma|=0.1257, time=7.5min
Epoch  280: Loss=46.4863, LR=4.1e-02, kappa=5.237, |gamma|=0.1257, time=7.8min
Epoch  290: Loss=46.2389, LR=3.8e-02, kappa=5.291, |gamma|=0.1257, time=8.0min
Epoch  300: Loss=46.0171, LR=3.5e-02, kappa=5.339, |gamma|=0.1255, time=8.3min
Epoch  310: Loss=45.8181, LR=3.2e-02, kappa=5.382, |gamma|=0.1254, time=8.6min
Epoch  320: Loss=45.6400, LR=2.9e-02, kappa=5.419, |gamma|=0.1251, time=8.9min
Epoch  330: Loss=45.4808, LR=2.6e-02, kappa=5.452, |gamma|=0.1249, time=9.2min
Epoch  340: Loss=45.3388, LR=2.4e-02, kappa=5.481, |gamma|=0.1246, time=9.4min
Epoch  350: Loss=45.2126, LR=2.1e-02, kappa=5.507, |gamma|=0.1245, time=9.7min
Epoch  360: Loss=45.1008, LR=1.9e-02, kappa=5.529, |gamma|=0.1243, time=10.0min
Epoch  370: Loss=45.0022, LR=1.6e-02, kappa=5.548, |gamma|=0.1242, time=10.2min
Epoch  380: Loss=44.9155, LR=1.4e-02, kappa=5.565, |gamma|=0.1241, time=10.5min
Epoch  390: Loss=44.8398, LR=1.2e-02, kappa=5.579, |gamma|=0.1241, time=10.8min
Epoch  400: Loss=44.7744, LR=1.0e-02, kappa=5.591, |gamma|=0.1241, time=11.1min
Epoch  410: Loss=44.7184, LR=8.5e-03, kappa=5.601, |gamma|=0.1241, time=11.3min
Epoch  420: Loss=44.6708, LR=7.0e-03, kappa=5.609, |gamma|=0.1240, time=11.6min
Epoch  430: Loss=44.6307, LR=5.6e-03, kappa=5.616, |gamma|=0.1239, time=11.9min
Epoch  440: Loss=44.5972, LR=4.4e-03, kappa=5.621, |gamma|=0.1238, time=12.1min
Epoch  450: Loss=44.5694, LR=3.3e-03, kappa=5.625, |gamma|=0.1238, time=12.4min
Epoch  460: Loss=44.5465, LR=2.5e-03, kappa=5.628, |gamma|=0.1237, time=12.7min
Epoch  470: Loss=44.5275, LR=1.8e-03, kappa=5.630, |gamma|=0.1237, time=13.0min
Epoch  480: Loss=44.5116, LR=1.4e-03, kappa=5.632, |gamma|=0.1236, time=13.2min
Epoch  490: Loss=44.4977, LR=1.1e-03, kappa=5.633, |gamma|=0.1236, time=13.5min

Training complete: 500 epochs in 13.8 min
Final loss: 44.4862

Final params: kappa=5.6344, mean|gamma|=0.1235
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2/enrollment_model_REPARAM_W0.0001_batch_310000_320000.pt

Batch 32 DONE in 13.9 min (31/39 completed)

======================================================================
BATCH 33/40: samples 320000-330000
Elapsed: 450 min | Est remaining: ~120 min (2.0 hrs)
======================================================================

============================================================
REPARAM v2 Training: samples 320000 to 330000
Epochs: 500, LR: 0.1, grad_clip: 5.0
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 14 diseases
Cluster 1: 14 diseases
Cluster 2: 26 diseases
Cluster 3: 16 diseases
Cluster 4: 5 diseases
Cluster 5: 21 diseases
Cluster 6: 8 diseases
Cluster 7: 7 diseases
Cluster 8: 8 diseases
Cluster 9: 10 diseases
Cluster 10: 26 diseases
Cluster 11: 104 diseases
Cluster 12: 13 diseases
Cluster 13: 6 diseases
Cluster 14: 5 diseases
Cluster 15: 16 diseases
Cluster 16: 11 diseases
Cluster 17: 15 diseases
Cluster 18: 13 diseases
Cluster 19: 10 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=64.3223, LR=1.0e-01, kappa=1.100, |gamma|=0.1006, time=0.0min
Epoch   10: Loss=137.0549, LR=1.0e-01, kappa=2.025, |gamma|=0.0954, time=0.3min
Epoch   20: Loss=100.1877, LR=1.0e-01, kappa=2.667, |gamma|=0.1308, time=0.6min
Epoch   30: Loss=73.2268, LR=9.9e-02, kappa=3.004, |gamma|=0.1536, time=0.8min
Epoch   40: Loss=63.4678, LR=9.8e-02, kappa=3.133, |gamma|=0.1600, time=1.1min
Epoch   50: Loss=60.3851, LR=9.7e-02, kappa=3.157, |gamma|=0.1634, time=1.4min
Epoch   60: Loss=59.0202, LR=9.6e-02, kappa=3.150, |gamma|=0.1621, time=1.6min
Epoch   70: Loss=58.2369, LR=9.5e-02, kappa=3.149, |gamma|=0.1631, time=1.9min
Epoch   80: Loss=57.7883, LR=9.4e-02, kappa=3.163, |gamma|=0.1647, time=2.2min
Epoch   90: Loss=57.4225, LR=9.2e-02, kappa=3.191, |gamma|=0.1667, time=2.5min
Epoch  100: Loss=57.0064, LR=9.0e-02, kappa=3.229, |gamma|=0.1667, time=2.7min
Epoch  110: Loss=56.5667, LR=8.8e-02, kappa=3.279, |gamma|=0.1653, time=3.0min
Epoch  120: Loss=56.0664, LR=8.6e-02, kappa=3.344, |gamma|=0.1638, time=3.3min
Epoch  130: Loss=55.5160, LR=8.4e-02, kappa=3.425, |gamma|=0.1625, time=3.5min
Epoch  140: Loss=54.9183, LR=8.2e-02, kappa=3.524, |gamma|=0.1615, time=3.8min
Epoch  150: Loss=54.2679, LR=7.9e-02, kappa=3.639, |gamma|=0.1602, time=4.1min
Epoch  160: Loss=53.6032, LR=7.7e-02, kappa=3.767, |gamma|=0.1588, time=4.3min
Epoch  170: Loss=52.9066, LR=7.4e-02, kappa=3.904, |gamma|=0.1577, time=4.6min
Epoch  180: Loss=52.2132, LR=7.1e-02, kappa=4.045, |gamma|=0.1562, time=4.9min
Epoch  190: Loss=51.5520, LR=6.8e-02, kappa=4.184, |gamma|=0.1546, time=5.2min
Epoch  200: Loss=50.9249, LR=6.6e-02, kappa=4.319, |gamma|=0.1535, time=5.4min
Epoch  210: Loss=50.3444, LR=6.3e-02, kappa=4.445, |gamma|=0.1525, time=5.7min
Epoch  220: Loss=49.8098, LR=5.9e-02, kappa=4.563, |gamma|=0.1509, time=6.0min
Epoch  230: Loss=49.3217, LR=5.6e-02, kappa=4.670, |gamma|=0.1498, time=6.3min
Epoch  240: Loss=48.8835, LR=5.3e-02, kappa=4.767, |gamma|=0.1486, time=6.5min
Epoch  250: Loss=48.4889, LR=5.0e-02, kappa=4.855, |gamma|=0.1475, time=6.8min
Epoch  260: Loss=48.1346, LR=4.7e-02, kappa=4.934, |gamma|=0.1464, time=7.1min
Epoch  270: Loss=47.8167, LR=4.4e-02, kappa=5.005, |gamma|=0.1452, time=7.3min
Epoch  280: Loss=47.5316, LR=4.1e-02, kappa=5.068, |gamma|=0.1440, time=7.6min
Epoch  290: Loss=47.2759, LR=3.8e-02, kappa=5.124, |gamma|=0.1429, time=7.9min
Epoch  300: Loss=47.0469, LR=3.5e-02, kappa=5.174, |gamma|=0.1419, time=8.2min
Epoch  310: Loss=46.8419, LR=3.2e-02, kappa=5.218, |gamma|=0.1410, time=8.4min
Epoch  320: Loss=46.6587, LR=2.9e-02, kappa=5.258, |gamma|=0.1401, time=8.7min
Epoch  330: Loss=46.4953, LR=2.6e-02, kappa=5.293, |gamma|=0.1394, time=9.0min
Epoch  340: Loss=46.3498, LR=2.4e-02, kappa=5.324, |gamma|=0.1387, time=9.2min
Epoch  350: Loss=46.2207, LR=2.1e-02, kappa=5.352, |gamma|=0.1381, time=9.5min
Epoch  360: Loss=46.1066, LR=1.9e-02, kappa=5.376, |gamma|=0.1376, time=9.8min
Epoch  370: Loss=46.0062, LR=1.6e-02, kappa=5.397, |gamma|=0.1372, time=10.1min
Epoch  380: Loss=45.9183, LR=1.4e-02, kappa=5.415, |gamma|=0.1368, time=10.3min
Epoch  390: Loss=45.8418, LR=1.2e-02, kappa=5.430, |gamma|=0.1365, time=10.6min
Epoch  400: Loss=45.7757, LR=1.0e-02, kappa=5.443, |gamma|=0.1362, time=10.9min
Epoch  410: Loss=45.7189, LR=8.5e-03, kappa=5.454, |gamma|=0.1359, time=11.1min
Epoch  420: Loss=45.6707, LR=7.0e-03, kappa=5.463, |gamma|=0.1357, time=11.4min
Epoch  430: Loss=45.6301, LR=5.6e-03, kappa=5.471, |gamma|=0.1355, time=11.7min
Epoch  440: Loss=45.5961, LR=4.4e-03, kappa=5.476, |gamma|=0.1353, time=12.0min
Epoch  450: Loss=45.5680, LR=3.3e-03, kappa=5.481, |gamma|=0.1352, time=12.2min
Epoch  460: Loss=45.5447, LR=2.5e-03, kappa=5.484, |gamma|=0.1351, time=12.5min
Epoch  470: Loss=45.5255, LR=1.8e-03, kappa=5.487, |gamma|=0.1350, time=12.8min
Epoch  480: Loss=45.5093, LR=1.4e-03, kappa=5.489, |gamma|=0.1349, time=13.0min
Epoch  490: Loss=45.4952, LR=1.1e-03, kappa=5.490, |gamma|=0.1349, time=13.3min

Training complete: 500 epochs in 13.6 min
Final loss: 45.4834

Final params: kappa=5.4914, mean|gamma|=0.1348
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2/enrollment_model_REPARAM_W0.0001_batch_320000_330000.pt

Batch 33 DONE in 13.7 min (32/39 completed)

======================================================================
BATCH 34/40: samples 330000-340000
Elapsed: 464 min | Est remaining: ~105 min (1.8 hrs)
======================================================================

============================================================
REPARAM v2 Training: samples 330000 to 340000
Epochs: 500, LR: 0.1, grad_clip: 5.0
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 11 diseases
Cluster 1: 12 diseases
Cluster 2: 7 diseases
Cluster 3: 11 diseases
Cluster 4: 9 diseases
Cluster 5: 14 diseases
Cluster 6: 98 diseases
Cluster 7: 14 diseases
Cluster 8: 13 diseases
Cluster 9: 27 diseases
Cluster 10: 17 diseases
Cluster 11: 17 diseases
Cluster 12: 5 diseases
Cluster 13: 7 diseases
Cluster 14: 9 diseases
Cluster 15: 24 diseases
Cluster 16: 22 diseases
Cluster 17: 5 diseases
Cluster 18: 12 diseases
Cluster 19: 14 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=63.9938, LR=1.0e-01, kappa=1.100, |gamma|=0.1009, time=0.0min
Epoch   10: Loss=137.9458, LR=1.0e-01, kappa=2.025, |gamma|=0.0945, time=0.3min
Epoch   20: Loss=100.2593, LR=1.0e-01, kappa=2.663, |gamma|=0.1304, time=0.6min
Epoch   30: Loss=72.8767, LR=9.9e-02, kappa=2.996, |gamma|=0.1506, time=0.8min
Epoch   40: Loss=63.1205, LR=9.8e-02, kappa=3.120, |gamma|=0.1546, time=1.1min
Epoch   50: Loss=60.0805, LR=9.7e-02, kappa=3.139, |gamma|=0.1554, time=1.4min
Epoch   60: Loss=58.7426, LR=9.6e-02, kappa=3.129, |gamma|=0.1562, time=1.6min
Epoch   70: Loss=57.9631, LR=9.5e-02, kappa=3.128, |gamma|=0.1560, time=1.9min
Epoch   80: Loss=57.5085, LR=9.4e-02, kappa=3.145, |gamma|=0.1560, time=2.2min
Epoch   90: Loss=57.1432, LR=9.2e-02, kappa=3.175, |gamma|=0.1564, time=2.5min
Epoch  100: Loss=56.7268, LR=9.0e-02, kappa=3.215, |gamma|=0.1570, time=2.7min
Epoch  110: Loss=56.2776, LR=8.8e-02, kappa=3.264, |gamma|=0.1572, time=3.0min
Epoch  120: Loss=55.7890, LR=8.6e-02, kappa=3.327, |gamma|=0.1569, time=3.3min
Epoch  130: Loss=55.2480, LR=8.4e-02, kappa=3.405, |gamma|=0.1562, time=3.5min
Epoch  140: Loss=54.6541, LR=8.2e-02, kappa=3.501, |gamma|=0.1548, time=3.8min
Epoch  150: Loss=54.0169, LR=7.9e-02, kappa=3.614, |gamma|=0.1530, time=4.1min
Epoch  160: Loss=53.3468, LR=7.7e-02, kappa=3.742, |gamma|=0.1509, time=4.4min
Epoch  170: Loss=52.6565, LR=7.4e-02, kappa=3.879, |gamma|=0.1491, time=4.6min
Epoch  180: Loss=51.9757, LR=7.1e-02, kappa=4.021, |gamma|=0.1475, time=4.9min
Epoch  190: Loss=51.3202, LR=6.8e-02, kappa=4.163, |gamma|=0.1460, time=5.2min
Epoch  200: Loss=50.6935, LR=6.6e-02, kappa=4.300, |gamma|=0.1450, time=5.5min
Epoch  210: Loss=50.1101, LR=6.3e-02, kappa=4.429, |gamma|=0.1440, time=5.7min
Epoch  220: Loss=49.5722, LR=5.9e-02, kappa=4.549, |gamma|=0.1433, time=6.0min
Epoch  230: Loss=49.0809, LR=5.6e-02, kappa=4.659, |gamma|=0.1426, time=6.3min
Epoch  240: Loss=48.6349, LR=5.3e-02, kappa=4.759, |gamma|=0.1421, time=6.6min
Epoch  250: Loss=48.2316, LR=5.0e-02, kappa=4.849, |gamma|=0.1415, time=6.8min
Epoch  260: Loss=47.8677, LR=4.7e-02, kappa=4.929, |gamma|=0.1409, time=7.1min
Epoch  270: Loss=47.5397, LR=4.4e-02, kappa=5.000, |gamma|=0.1403, time=7.4min
Epoch  280: Loss=47.2445, LR=4.1e-02, kappa=5.064, |gamma|=0.1397, time=7.7min
Epoch  290: Loss=46.9790, LR=3.8e-02, kappa=5.120, |gamma|=0.1393, time=7.9min
Epoch  300: Loss=46.7405, LR=3.5e-02, kappa=5.170, |gamma|=0.1389, time=8.2min
Epoch  310: Loss=46.5265, LR=3.2e-02, kappa=5.215, |gamma|=0.1385, time=8.5min
Epoch  320: Loss=46.3347, LR=2.9e-02, kappa=5.254, |gamma|=0.1381, time=8.7min
Epoch  330: Loss=46.1633, LR=2.6e-02, kappa=5.289, |gamma|=0.1376, time=9.0min
Epoch  340: Loss=46.0103, LR=2.4e-02, kappa=5.319, |gamma|=0.1372, time=9.3min
Epoch  350: Loss=45.8742, LR=2.1e-02, kappa=5.347, |gamma|=0.1368, time=9.6min
Epoch  360: Loss=45.7537, LR=1.9e-02, kappa=5.370, |gamma|=0.1364, time=9.8min
Epoch  370: Loss=45.6473, LR=1.6e-02, kappa=5.391, |gamma|=0.1360, time=10.1min
Epoch  380: Loss=45.5540, LR=1.4e-02, kappa=5.409, |gamma|=0.1355, time=10.4min
Epoch  390: Loss=45.4722, LR=1.2e-02, kappa=5.424, |gamma|=0.1349, time=10.7min
Epoch  400: Loss=45.4010, LR=1.0e-02, kappa=5.437, |gamma|=0.1343, time=10.9min
Epoch  410: Loss=45.3403, LR=8.5e-03, kappa=5.448, |gamma|=0.1339, time=11.2min
Epoch  420: Loss=45.2886, LR=7.0e-03, kappa=5.457, |gamma|=0.1336, time=11.5min
Epoch  430: Loss=45.2451, LR=5.6e-03, kappa=5.464, |gamma|=0.1335, time=11.8min
Epoch  440: Loss=45.2088, LR=4.4e-03, kappa=5.470, |gamma|=0.1333, time=12.0min
Epoch  450: Loss=45.1788, LR=3.3e-03, kappa=5.474, |gamma|=0.1332, time=12.3min
Epoch  460: Loss=45.1540, LR=2.5e-03, kappa=5.478, |gamma|=0.1332, time=12.6min
Epoch  470: Loss=45.1336, LR=1.8e-03, kappa=5.480, |gamma|=0.1331, time=12.9min
Epoch  480: Loss=45.1165, LR=1.4e-03, kappa=5.482, |gamma|=0.1331, time=13.1min
Epoch  490: Loss=45.1017, LR=1.1e-03, kappa=5.484, |gamma|=0.1330, time=13.4min

Training complete: 500 epochs in 13.7 min
Final loss: 45.0893

Final params: kappa=5.4846, mean|gamma|=0.1330
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2/enrollment_model_REPARAM_W0.0001_batch_330000_340000.pt

Batch 34 DONE in 13.8 min (33/39 completed)

======================================================================
BATCH 35/40: samples 340000-350000
Elapsed: 478 min | Est remaining: ~90 min (1.5 hrs)
======================================================================

============================================================
REPARAM v2 Training: samples 340000 to 350000
Epochs: 500, LR: 0.1, grad_clip: 5.0
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 10 diseases
Cluster 1: 15 diseases
Cluster 2: 11 diseases
Cluster 3: 18 diseases
Cluster 4: 9 diseases
Cluster 5: 5 diseases
Cluster 6: 5 diseases
Cluster 7: 17 diseases
Cluster 8: 7 diseases
Cluster 9: 8 diseases
Cluster 10: 28 diseases
Cluster 11: 30 diseases
Cluster 12: 12 diseases
Cluster 13: 99 diseases
Cluster 14: 24 diseases
Cluster 15: 18 diseases
Cluster 16: 9 diseases
Cluster 17: 13 diseases
Cluster 18: 4 diseases
Cluster 19: 6 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=63.3389, LR=1.0e-01, kappa=1.100, |gamma|=0.1007, time=0.0min
Epoch   10: Loss=139.3463, LR=1.0e-01, kappa=2.022, |gamma|=0.0893, time=0.3min
Epoch   20: Loss=100.3125, LR=1.0e-01, kappa=2.655, |gamma|=0.1267, time=0.6min
Epoch   30: Loss=72.2589, LR=9.9e-02, kappa=2.980, |gamma|=0.1465, time=0.8min
Epoch   40: Loss=62.5166, LR=9.8e-02, kappa=3.098, |gamma|=0.1506, time=1.1min
Epoch   50: Loss=59.5544, LR=9.7e-02, kappa=3.113, |gamma|=0.1499, time=1.4min
Epoch   60: Loss=58.2631, LR=9.6e-02, kappa=3.102, |gamma|=0.1499, time=1.7min
Epoch   70: Loss=57.4962, LR=9.5e-02, kappa=3.099, |gamma|=0.1512, time=1.9min
Epoch   80: Loss=57.0393, LR=9.4e-02, kappa=3.114, |gamma|=0.1523, time=2.2min
Epoch   90: Loss=56.6912, LR=9.2e-02, kappa=3.142, |gamma|=0.1542, time=2.5min
Epoch  100: Loss=56.2832, LR=9.0e-02, kappa=3.179, |gamma|=0.1545, time=2.8min
Epoch  110: Loss=55.8462, LR=8.8e-02, kappa=3.231, |gamma|=0.1533, time=3.0min
Epoch  120: Loss=55.3636, LR=8.6e-02, kappa=3.300, |gamma|=0.1518, time=3.3min
Epoch  130: Loss=54.8168, LR=8.4e-02, kappa=3.387, |gamma|=0.1499, time=3.6min
Epoch  140: Loss=54.2186, LR=8.2e-02, kappa=3.494, |gamma|=0.1475, time=3.9min
Epoch  150: Loss=53.5712, LR=7.9e-02, kappa=3.620, |gamma|=0.1452, time=4.1min
Epoch  160: Loss=52.8767, LR=7.7e-02, kappa=3.762, |gamma|=0.1428, time=4.4min
Epoch  170: Loss=52.1591, LR=7.4e-02, kappa=3.917, |gamma|=0.1404, time=4.7min
Epoch  180: Loss=51.4430, LR=7.1e-02, kappa=4.077, |gamma|=0.1385, time=5.0min
Epoch  190: Loss=50.7409, LR=6.8e-02, kappa=4.237, |gamma|=0.1369, time=5.2min
Epoch  200: Loss=50.0775, LR=6.6e-02, kappa=4.391, |gamma|=0.1351, time=5.5min
Epoch  210: Loss=49.4601, LR=6.3e-02, kappa=4.537, |gamma|=0.1338, time=5.8min
Epoch  220: Loss=48.8969, LR=5.9e-02, kappa=4.671, |gamma|=0.1320, time=6.1min
Epoch  230: Loss=48.3873, LR=5.6e-02, kappa=4.793, |gamma|=0.1307, time=6.3min
Epoch  240: Loss=47.9275, LR=5.3e-02, kappa=4.903, |gamma|=0.1292, time=6.6min
Epoch  250: Loss=47.5146, LR=5.0e-02, kappa=5.003, |gamma|=0.1280, time=6.9min
Epoch  260: Loss=47.1334, LR=4.7e-02, kappa=5.095, |gamma|=0.1268, time=7.2min
Epoch  270: Loss=46.7933, LR=4.4e-02, kappa=5.177, |gamma|=0.1259, time=7.4min
Epoch  280: Loss=46.4890, LR=4.1e-02, kappa=5.250, |gamma|=0.1251, time=7.7min
Epoch  290: Loss=46.2172, LR=3.8e-02, kappa=5.315, |gamma|=0.1243, time=8.0min
Epoch  300: Loss=45.9747, LR=3.5e-02, kappa=5.373, |gamma|=0.1235, time=8.3min
Epoch  310: Loss=45.7585, LR=3.2e-02, kappa=5.424, |gamma|=0.1227, time=8.5min
Epoch  320: Loss=45.5659, LR=2.9e-02, kappa=5.469, |gamma|=0.1220, time=8.8min
Epoch  330: Loss=45.3945, LR=2.6e-02, kappa=5.509, |gamma|=0.1214, time=9.1min
Epoch  340: Loss=45.2424, LR=2.4e-02, kappa=5.544, |gamma|=0.1208, time=9.4min
Epoch  350: Loss=45.1077, LR=2.1e-02, kappa=5.574, |gamma|=0.1203, time=9.6min
Epoch  360: Loss=44.9889, LR=1.9e-02, kappa=5.601, |gamma|=0.1198, time=9.9min
Epoch  370: Loss=44.8845, LR=1.6e-02, kappa=5.624, |gamma|=0.1194, time=10.2min
Epoch  380: Loss=44.7932, LR=1.4e-02, kappa=5.644, |gamma|=0.1191, time=10.4min
Epoch  390: Loss=44.7140, LR=1.2e-02, kappa=5.661, |gamma|=0.1188, time=10.7min
Epoch  400: Loss=44.6455, LR=1.0e-02, kappa=5.675, |gamma|=0.1185, time=11.0min
Epoch  410: Loss=44.5869, LR=8.5e-03, kappa=5.687, |gamma|=0.1183, time=11.3min
Epoch  420: Loss=44.5372, LR=7.0e-03, kappa=5.697, |gamma|=0.1181, time=11.5min
Epoch  430: Loss=44.4953, LR=5.6e-03, kappa=5.705, |gamma|=0.1179, time=11.8min
Epoch  440: Loss=44.4604, LR=4.4e-03, kappa=5.711, |gamma|=0.1178, time=12.1min
Epoch  450: Loss=44.4315, LR=3.3e-03, kappa=5.716, |gamma|=0.1177, time=12.4min
Epoch  460: Loss=44.4076, LR=2.5e-03, kappa=5.720, |gamma|=0.1176, time=12.6min
Epoch  470: Loss=44.3879, LR=1.8e-03, kappa=5.722, |gamma|=0.1175, time=12.9min
Epoch  480: Loss=44.3714, LR=1.4e-03, kappa=5.724, |gamma|=0.1174, time=13.2min
Epoch  490: Loss=44.3571, LR=1.1e-03, kappa=5.726, |gamma|=0.1173, time=13.5min

Training complete: 500 epochs in 13.7 min
Final loss: 44.3451

Final params: kappa=5.7271, mean|gamma|=0.1173
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2/enrollment_model_REPARAM_W0.0001_batch_340000_350000.pt

Batch 35 DONE in 13.8 min (34/39 completed)

======================================================================
BATCH 36/40: samples 350000-360000
Elapsed: 492 min | Est remaining: ~75 min (1.2 hrs)
======================================================================

============================================================
REPARAM v2 Training: samples 350000 to 360000
Epochs: 500, LR: 0.1, grad_clip: 5.0
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 5 diseases
Cluster 1: 9 diseases
Cluster 2: 14 diseases
Cluster 3: 14 diseases
Cluster 4: 17 diseases
Cluster 5: 113 diseases
Cluster 6: 9 diseases
Cluster 7: 13 diseases
Cluster 8: 8 diseases
Cluster 9: 13 diseases
Cluster 10: 12 diseases
Cluster 11: 20 diseases
Cluster 12: 29 diseases
Cluster 13: 9 diseases
Cluster 14: 18 diseases
Cluster 15: 5 diseases
Cluster 16: 15 diseases
Cluster 17: 12 diseases
Cluster 18: 7 diseases
Cluster 19: 6 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=63.6311, LR=1.0e-01, kappa=1.100, |gamma|=0.1007, time=0.0min
Epoch   10: Loss=139.4097, LR=1.0e-01, kappa=2.023, |gamma|=0.0996, time=0.3min
Epoch   20: Loss=100.4860, LR=1.0e-01, kappa=2.661, |gamma|=0.1370, time=0.6min
Epoch   30: Loss=72.4962, LR=9.9e-02, kappa=2.997, |gamma|=0.1574, time=0.8min
Epoch   40: Loss=62.7491, LR=9.8e-02, kappa=3.125, |gamma|=0.1607, time=1.1min
Epoch   50: Loss=59.7709, LR=9.7e-02, kappa=3.148, |gamma|=0.1597, time=1.4min
Epoch   60: Loss=58.4671, LR=9.6e-02, kappa=3.143, |gamma|=0.1575, time=1.6min
Epoch   70: Loss=57.6922, LR=9.5e-02, kappa=3.143, |gamma|=0.1578, time=1.9min
Epoch   80: Loss=57.2302, LR=9.4e-02, kappa=3.159, |gamma|=0.1580, time=2.2min
Epoch   90: Loss=56.8669, LR=9.2e-02, kappa=3.188, |gamma|=0.1581, time=2.5min
Epoch  100: Loss=56.4567, LR=9.0e-02, kappa=3.228, |gamma|=0.1576, time=2.7min
Epoch  110: Loss=56.0109, LR=8.8e-02, kappa=3.279, |gamma|=0.1573, time=3.0min
Epoch  120: Loss=55.5204, LR=8.6e-02, kappa=3.342, |gamma|=0.1560, time=3.3min
Epoch  130: Loss=54.9817, LR=8.4e-02, kappa=3.422, |gamma|=0.1544, time=3.5min
Epoch  140: Loss=54.3839, LR=8.2e-02, kappa=3.518, |gamma|=0.1520, time=3.8min
Epoch  150: Loss=53.7473, LR=7.9e-02, kappa=3.630, |gamma|=0.1499, time=4.1min
Epoch  160: Loss=53.1020, LR=7.7e-02, kappa=3.754, |gamma|=0.1478, time=4.4min
Epoch  170: Loss=52.4193, LR=7.4e-02, kappa=3.886, |gamma|=0.1460, time=4.7min
Epoch  180: Loss=51.7555, LR=7.1e-02, kappa=4.020, |gamma|=0.1443, time=4.9min
Epoch  190: Loss=51.1175, LR=6.8e-02, kappa=4.154, |gamma|=0.1424, time=5.2min
Epoch  200: Loss=50.5175, LR=6.6e-02, kappa=4.283, |gamma|=0.1405, time=5.5min
Epoch  210: Loss=49.9481, LR=6.3e-02, kappa=4.405, |gamma|=0.1381, time=5.8min
Epoch  220: Loss=49.4211, LR=5.9e-02, kappa=4.520, |gamma|=0.1386, time=6.1min
Epoch  230: Loss=48.9404, LR=5.6e-02, kappa=4.627, |gamma|=0.1374, time=6.3min
Epoch  240: Loss=48.5033, LR=5.3e-02, kappa=4.724, |gamma|=0.1364, time=6.6min
Epoch  250: Loss=48.1071, LR=5.0e-02, kappa=4.812, |gamma|=0.1354, time=6.9min
Epoch  260: Loss=47.7488, LR=4.7e-02, kappa=4.891, |gamma|=0.1344, time=7.1min
Epoch  270: Loss=47.4251, LR=4.4e-02, kappa=4.962, |gamma|=0.1334, time=7.4min
Epoch  280: Loss=47.1332, LR=4.1e-02, kappa=5.026, |gamma|=0.1325, time=7.7min
Epoch  290: Loss=46.8701, LR=3.8e-02, kappa=5.082, |gamma|=0.1316, time=8.0min
Epoch  300: Loss=46.6332, LR=3.5e-02, kappa=5.133, |gamma|=0.1308, time=8.2min
Epoch  310: Loss=46.4203, LR=3.2e-02, kappa=5.177, |gamma|=0.1301, time=8.5min
Epoch  320: Loss=46.2291, LR=2.9e-02, kappa=5.217, |gamma|=0.1293, time=8.8min
Epoch  330: Loss=46.0580, LR=2.6e-02, kappa=5.252, |gamma|=0.1287, time=9.1min
Epoch  340: Loss=45.9051, LR=2.4e-02, kappa=5.283, |gamma|=0.1282, time=9.3min
Epoch  350: Loss=45.7691, LR=2.1e-02, kappa=5.311, |gamma|=0.1277, time=9.6min
Epoch  360: Loss=45.6485, LR=1.9e-02, kappa=5.334, |gamma|=0.1272, time=9.9min
Epoch  370: Loss=45.5422, LR=1.6e-02, kappa=5.355, |gamma|=0.1268, time=10.1min
Epoch  380: Loss=45.4489, LR=1.4e-02, kappa=5.373, |gamma|=0.1265, time=10.4min
Epoch  390: Loss=45.3676, LR=1.2e-02, kappa=5.388, |gamma|=0.1262, time=10.7min
Epoch  400: Loss=45.2973, LR=1.0e-02, kappa=5.401, |gamma|=0.1260, time=11.0min
Epoch  410: Loss=45.2369, LR=8.5e-03, kappa=5.412, |gamma|=0.1258, time=11.2min
Epoch  420: Loss=45.1855, LR=7.0e-03, kappa=5.421, |gamma|=0.1256, time=11.5min
Epoch  430: Loss=45.1422, LR=5.6e-03, kappa=5.428, |gamma|=0.1255, time=11.8min
Epoch  440: Loss=45.1061, LR=4.4e-03, kappa=5.434, |gamma|=0.1254, time=12.0min
Epoch  450: Loss=45.0761, LR=3.3e-03, kappa=5.438, |gamma|=0.1253, time=12.3min
Epoch  460: Loss=45.0514, LR=2.5e-03, kappa=5.441, |gamma|=0.1252, time=12.6min
Epoch  470: Loss=45.0309, LR=1.8e-03, kappa=5.444, |gamma|=0.1252, time=12.9min
Epoch  480: Loss=45.0137, LR=1.4e-03, kappa=5.446, |gamma|=0.1251, time=13.2min
Epoch  490: Loss=44.9988, LR=1.1e-03, kappa=5.447, |gamma|=0.1251, time=13.5min

Training complete: 500 epochs in 13.7 min
Final loss: 44.9864

Final params: kappa=5.4482, mean|gamma|=0.1250
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2/enrollment_model_REPARAM_W0.0001_batch_350000_360000.pt

Batch 36 DONE in 13.9 min (35/39 completed)

======================================================================
BATCH 37/40: samples 360000-370000
Elapsed: 506 min | Est remaining: ~60 min (1.0 hrs)
======================================================================

============================================================
REPARAM v2 Training: samples 360000 to 370000
Epochs: 500, LR: 0.1, grad_clip: 5.0
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 30 diseases
Cluster 1: 14 diseases
Cluster 2: 15 diseases
Cluster 3: 13 diseases
Cluster 4: 8 diseases
Cluster 5: 5 diseases
Cluster 6: 9 diseases
Cluster 7: 11 diseases
Cluster 8: 30 diseases
Cluster 9: 86 diseases
Cluster 10: 8 diseases
Cluster 11: 21 diseases
Cluster 12: 11 diseases
Cluster 13: 15 diseases
Cluster 14: 25 diseases
Cluster 15: 12 diseases
Cluster 16: 5 diseases
Cluster 17: 5 diseases
Cluster 18: 16 diseases
Cluster 19: 9 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=62.5406, LR=1.0e-01, kappa=1.100, |gamma|=0.1008, time=0.0min
Epoch   10: Loss=141.3100, LR=1.0e-01, kappa=2.019, |gamma|=0.0979, time=0.3min
Epoch   20: Loss=100.2838, LR=1.0e-01, kappa=2.648, |gamma|=0.1408, time=0.6min
Epoch   30: Loss=71.3406, LR=9.9e-02, kappa=2.973, |gamma|=0.1609, time=0.8min
Epoch   40: Loss=61.6556, LR=9.8e-02, kappa=3.091, |gamma|=0.1640, time=1.1min
Epoch   50: Loss=58.8020, LR=9.7e-02, kappa=3.103, |gamma|=0.1674, time=1.4min
Epoch   60: Loss=57.5729, LR=9.6e-02, kappa=3.087, |gamma|=0.1697, time=1.7min
Epoch   70: Loss=56.8212, LR=9.5e-02, kappa=3.081, |gamma|=0.1698, time=1.9min
Epoch   80: Loss=56.3539, LR=9.4e-02, kappa=3.093, |gamma|=0.1705, time=2.2min
Epoch   90: Loss=56.0007, LR=9.2e-02, kappa=3.118, |gamma|=0.1718, time=2.5min
Epoch  100: Loss=55.6063, LR=9.0e-02, kappa=3.152, |gamma|=0.1734, time=2.8min
Epoch  110: Loss=55.1758, LR=8.8e-02, kappa=3.196, |gamma|=0.1745, time=3.0min
Epoch  120: Loss=54.7100, LR=8.6e-02, kappa=3.252, |gamma|=0.1740, time=3.3min
Epoch  130: Loss=54.1686, LR=8.4e-02, kappa=3.325, |gamma|=0.1707, time=3.6min
Epoch  140: Loss=53.5786, LR=8.2e-02, kappa=3.419, |gamma|=0.1682, time=3.8min
Epoch  150: Loss=52.9495, LR=7.9e-02, kappa=3.530, |gamma|=0.1661, time=4.1min
Epoch  160: Loss=52.2793, LR=7.7e-02, kappa=3.653, |gamma|=0.1641, time=4.4min
Epoch  170: Loss=51.5962, LR=7.4e-02, kappa=3.785, |gamma|=0.1630, time=4.7min
Epoch  180: Loss=50.9184, LR=7.1e-02, kappa=3.923, |gamma|=0.1616, time=4.9min
Epoch  190: Loss=50.2587, LR=6.8e-02, kappa=4.063, |gamma|=0.1598, time=5.2min
Epoch  200: Loss=49.6233, LR=6.6e-02, kappa=4.205, |gamma|=0.1582, time=5.5min
Epoch  210: Loss=49.0384, LR=6.3e-02, kappa=4.342, |gamma|=0.1578, time=5.7min
Epoch  220: Loss=48.5039, LR=5.9e-02, kappa=4.470, |gamma|=0.1567, time=6.0min
Epoch  230: Loss=48.0205, LR=5.6e-02, kappa=4.586, |gamma|=0.1559, time=6.3min
Epoch  240: Loss=47.5854, LR=5.3e-02, kappa=4.691, |gamma|=0.1550, time=6.6min
Epoch  250: Loss=47.1955, LR=5.0e-02, kappa=4.785, |gamma|=0.1541, time=6.8min
Epoch  260: Loss=46.8462, LR=4.7e-02, kappa=4.869, |gamma|=0.1532, time=7.1min
Epoch  270: Loss=46.5333, LR=4.4e-02, kappa=4.944, |gamma|=0.1523, time=7.4min
Epoch  280: Loss=46.2530, LR=4.1e-02, kappa=5.010, |gamma|=0.1515, time=7.6min
Epoch  290: Loss=46.0018, LR=3.8e-02, kappa=5.069, |gamma|=0.1506, time=7.9min
Epoch  300: Loss=45.7765, LR=3.5e-02, kappa=5.121, |gamma|=0.1498, time=8.2min
Epoch  310: Loss=45.5746, LR=3.2e-02, kappa=5.167, |gamma|=0.1491, time=8.4min
Epoch  320: Loss=45.3937, LR=2.9e-02, kappa=5.208, |gamma|=0.1483, time=8.7min
Epoch  330: Loss=45.2321, LR=2.6e-02, kappa=5.245, |gamma|=0.1476, time=9.0min
Epoch  340: Loss=45.0878, LR=2.4e-02, kappa=5.277, |gamma|=0.1469, time=9.3min
Epoch  350: Loss=44.9595, LR=2.1e-02, kappa=5.305, |gamma|=0.1463, time=9.5min
Epoch  360: Loss=44.8457, LR=1.9e-02, kappa=5.329, |gamma|=0.1457, time=9.8min
Epoch  370: Loss=44.7453, LR=1.6e-02, kappa=5.350, |gamma|=0.1451, time=10.1min
Epoch  380: Loss=44.6572, LR=1.4e-02, kappa=5.369, |gamma|=0.1447, time=10.4min
Epoch  390: Loss=44.5803, LR=1.2e-02, kappa=5.384, |gamma|=0.1443, time=10.6min
Epoch  400: Loss=44.5137, LR=1.0e-02, kappa=5.398, |gamma|=0.1440, time=10.9min
Epoch  410: Loss=44.4565, LR=8.5e-03, kappa=5.409, |gamma|=0.1437, time=11.2min
Epoch  420: Loss=44.4078, LR=7.0e-03, kappa=5.418, |gamma|=0.1434, time=11.4min
Epoch  430: Loss=44.3666, LR=5.6e-03, kappa=5.425, |gamma|=0.1432, time=11.7min
Epoch  440: Loss=44.3323, LR=4.4e-03, kappa=5.431, |gamma|=0.1430, time=12.0min
Epoch  450: Loss=44.3037, LR=3.3e-03, kappa=5.436, |gamma|=0.1429, time=12.2min
Epoch  460: Loss=44.2801, LR=2.5e-03, kappa=5.439, |gamma|=0.1427, time=12.5min
Epoch  470: Loss=44.2605, LR=1.8e-03, kappa=5.442, |gamma|=0.1426, time=12.8min
Epoch  480: Loss=44.2440, LR=1.4e-03, kappa=5.444, |gamma|=0.1425, time=13.1min
Epoch  490: Loss=44.2297, LR=1.1e-03, kappa=5.445, |gamma|=0.1424, time=13.3min

Training complete: 500 epochs in 13.6 min
Final loss: 44.2177

Final params: kappa=5.4464, mean|gamma|=0.1423
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2/enrollment_model_REPARAM_W0.0001_batch_360000_370000.pt

Batch 37 DONE in 13.7 min (36/39 completed)

======================================================================
BATCH 38/40: samples 370000-380000
Elapsed: 519 min | Est remaining: ~45 min (0.8 hrs)
======================================================================

============================================================
REPARAM v2 Training: samples 370000 to 380000
Epochs: 500, LR: 0.1, grad_clip: 5.0
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 19 diseases
Cluster 1: 13 diseases
Cluster 2: 6 diseases
Cluster 3: 22 diseases
Cluster 4: 15 diseases
Cluster 5: 9 diseases
Cluster 6: 13 diseases
Cluster 7: 5 diseases
Cluster 8: 5 diseases
Cluster 9: 19 diseases
Cluster 10: 7 diseases
Cluster 11: 25 diseases
Cluster 12: 19 diseases
Cluster 13: 15 diseases
Cluster 14: 14 diseases
Cluster 15: 10 diseases
Cluster 16: 10 diseases
Cluster 17: 5 diseases
Cluster 18: 14 diseases
Cluster 19: 103 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=63.7181, LR=1.0e-01, kappa=1.100, |gamma|=0.1008, time=0.0min
Epoch   10: Loss=138.3467, LR=1.0e-01, kappa=2.024, |gamma|=0.0942, time=0.3min
Epoch   20: Loss=100.2298, LR=1.0e-01, kappa=2.665, |gamma|=0.1275, time=0.6min
Epoch   30: Loss=72.6089, LR=9.9e-02, kappa=3.000, |gamma|=0.1497, time=0.8min
Epoch   40: Loss=62.8445, LR=9.8e-02, kappa=3.131, |gamma|=0.1576, time=1.1min
Epoch   50: Loss=59.8239, LR=9.7e-02, kappa=3.157, |gamma|=0.1586, time=1.4min
Epoch   60: Loss=58.4944, LR=9.6e-02, kappa=3.155, |gamma|=0.1570, time=1.6min
Epoch   70: Loss=57.7102, LR=9.5e-02, kappa=3.160, |gamma|=0.1554, time=1.9min
Epoch   80: Loss=57.2441, LR=9.4e-02, kappa=3.184, |gamma|=0.1556, time=2.2min
Epoch   90: Loss=56.8659, LR=9.2e-02, kappa=3.223, |gamma|=0.1560, time=2.4min
Epoch  100: Loss=56.4315, LR=9.0e-02, kappa=3.274, |gamma|=0.1561, time=2.7min
Epoch  110: Loss=55.9607, LR=8.8e-02, kappa=3.338, |gamma|=0.1554, time=3.0min
Epoch  120: Loss=55.4341, LR=8.6e-02, kappa=3.417, |gamma|=0.1541, time=3.3min
Epoch  130: Loss=54.8514, LR=8.4e-02, kappa=3.512, |gamma|=0.1519, time=3.5min
Epoch  140: Loss=54.2729, LR=8.2e-02, kappa=3.623, |gamma|=0.1501, time=3.8min
Epoch  150: Loss=53.5648, LR=7.9e-02, kappa=3.748, |gamma|=0.1487, time=4.1min
Epoch  160: Loss=52.8594, LR=7.7e-02, kappa=3.883, |gamma|=0.1452, time=4.3min
Epoch  170: Loss=52.1515, LR=7.4e-02, kappa=4.030, |gamma|=0.1433, time=4.6min
Epoch  180: Loss=51.4586, LR=7.1e-02, kappa=4.181, |gamma|=0.1410, time=4.9min
Epoch  190: Loss=50.7920, LR=6.8e-02, kappa=4.331, |gamma|=0.1387, time=5.1min
Epoch  200: Loss=50.1590, LR=6.6e-02, kappa=4.473, |gamma|=0.1359, time=5.4min
Epoch  210: Loss=49.5796, LR=6.3e-02, kappa=4.607, |gamma|=0.1348, time=5.7min
Epoch  220: Loss=49.0433, LR=5.9e-02, kappa=4.730, |gamma|=0.1338, time=6.0min
Epoch  230: Loss=48.5511, LR=5.6e-02, kappa=4.842, |gamma|=0.1328, time=6.2min
Epoch  240: Loss=48.1096, LR=5.3e-02, kappa=4.944, |gamma|=0.1325, time=6.5min
Epoch  250: Loss=47.7118, LR=5.0e-02, kappa=5.035, |gamma|=0.1320, time=6.8min
Epoch  260: Loss=47.3540, LR=4.7e-02, kappa=5.117, |gamma|=0.1317, time=7.0min
Epoch  270: Loss=47.0326, LR=4.4e-02, kappa=5.189, |gamma|=0.1313, time=7.3min
Epoch  280: Loss=46.7439, LR=4.1e-02, kappa=5.254, |gamma|=0.1308, time=7.6min
Epoch  290: Loss=46.4850, LR=3.8e-02, kappa=5.311, |gamma|=0.1303, time=7.8min
Epoch  300: Loss=46.2527, LR=3.5e-02, kappa=5.362, |gamma|=0.1297, time=8.1min
Epoch  310: Loss=46.0445, LR=3.2e-02, kappa=5.407, |gamma|=0.1291, time=8.4min
Epoch  320: Loss=45.8582, LR=2.9e-02, kappa=5.446, |gamma|=0.1285, time=8.7min
Epoch  330: Loss=45.6917, LR=2.6e-02, kappa=5.481, |gamma|=0.1280, time=8.9min
Epoch  340: Loss=45.5433, LR=2.4e-02, kappa=5.512, |gamma|=0.1276, time=9.2min
Epoch  350: Loss=45.4112, LR=2.1e-02, kappa=5.539, |gamma|=0.1271, time=9.5min
Epoch  360: Loss=45.2932, LR=1.9e-02, kappa=5.562, |gamma|=0.1266, time=9.7min
Epoch  370: Loss=45.1890, LR=1.6e-02, kappa=5.583, |gamma|=0.1259, time=10.0min
Epoch  380: Loss=45.0981, LR=1.4e-02, kappa=5.601, |gamma|=0.1259, time=10.3min
Epoch  390: Loss=45.0191, LR=1.2e-02, kappa=5.616, |gamma|=0.1255, time=10.5min
Epoch  400: Loss=44.9504, LR=1.0e-02, kappa=5.629, |gamma|=0.1250, time=10.8min
Epoch  410: Loss=44.8915, LR=8.5e-03, kappa=5.640, |gamma|=0.1247, time=11.1min
Epoch  420: Loss=44.8416, LR=7.0e-03, kappa=5.649, |gamma|=0.1245, time=11.4min
Epoch  430: Loss=44.7995, LR=5.6e-03, kappa=5.656, |gamma|=0.1244, time=11.6min
Epoch  440: Loss=44.7644, LR=4.4e-03, kappa=5.661, |gamma|=0.1242, time=11.9min
Epoch  450: Loss=44.7353, LR=3.3e-03, kappa=5.666, |gamma|=0.1241, time=12.2min
Epoch  460: Loss=44.7113, LR=2.5e-03, kappa=5.669, |gamma|=0.1240, time=12.4min
Epoch  470: Loss=44.6914, LR=1.8e-03, kappa=5.671, |gamma|=0.1240, time=12.7min
Epoch  480: Loss=44.6747, LR=1.4e-03, kappa=5.673, |gamma|=0.1239, time=13.0min
Epoch  490: Loss=44.6601, LR=1.1e-03, kappa=5.675, |gamma|=0.1239, time=13.2min

Training complete: 500 epochs in 13.5 min
Final loss: 44.6480

Final params: kappa=5.6757, mean|gamma|=0.1238
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2/enrollment_model_REPARAM_W0.0001_batch_370000_380000.pt

Batch 38 DONE in 13.6 min (37/39 completed)

======================================================================
BATCH 39/40: samples 380000-390000
Elapsed: 533 min | Est remaining: ~30 min (0.5 hrs)
======================================================================

============================================================
REPARAM v2 Training: samples 380000 to 390000
Epochs: 500, LR: 0.1, grad_clip: 5.0
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 15 diseases
Cluster 1: 23 diseases
Cluster 2: 10 diseases
Cluster 3: 6 diseases
Cluster 4: 14 diseases
Cluster 5: 12 diseases
Cluster 6: 5 diseases
Cluster 7: 10 diseases
Cluster 8: 12 diseases
Cluster 9: 8 diseases
Cluster 10: 41 diseases
Cluster 11: 11 diseases
Cluster 12: 9 diseases
Cluster 13: 25 diseases
Cluster 14: 8 diseases
Cluster 15: 14 diseases
Cluster 16: 25 diseases
Cluster 17: 80 diseases
Cluster 18: 11 diseases
Cluster 19: 9 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=63.5337, LR=1.0e-01, kappa=1.100, |gamma|=0.1009, time=0.0min
Epoch   10: Loss=139.1875, LR=1.0e-01, kappa=2.022, |gamma|=0.0966, time=0.3min
Epoch   20: Loss=100.3682, LR=1.0e-01, kappa=2.657, |gamma|=0.1354, time=0.6min
Epoch   30: Loss=72.4285, LR=9.9e-02, kappa=2.986, |gamma|=0.1529, time=0.8min
Epoch   40: Loss=62.6841, LR=9.8e-02, kappa=3.107, |gamma|=0.1577, time=1.1min
Epoch   50: Loss=59.7050, LR=9.7e-02, kappa=3.125, |gamma|=0.1556, time=1.4min
Epoch   60: Loss=58.4019, LR=9.6e-02, kappa=3.118, |gamma|=0.1544, time=1.6min
Epoch   70: Loss=57.6277, LR=9.5e-02, kappa=3.119, |gamma|=0.1530, time=1.9min
Epoch   80: Loss=57.1649, LR=9.4e-02, kappa=3.137, |gamma|=0.1538, time=2.2min
Epoch   90: Loss=56.7980, LR=9.2e-02, kappa=3.170, |gamma|=0.1543, time=2.4min
Epoch  100: Loss=56.3819, LR=9.0e-02, kappa=3.214, |gamma|=0.1535, time=2.7min
Epoch  110: Loss=55.9284, LR=8.8e-02, kappa=3.269, |gamma|=0.1528, time=3.0min
Epoch  120: Loss=55.4188, LR=8.6e-02, kappa=3.340, |gamma|=0.1515, time=3.2min
Epoch  130: Loss=54.8553, LR=8.4e-02, kappa=3.428, |gamma|=0.1497, time=3.5min
Epoch  140: Loss=54.2227, LR=8.2e-02, kappa=3.535, |gamma|=0.1466, time=3.7min
Epoch  150: Loss=53.5449, LR=7.9e-02, kappa=3.660, |gamma|=0.1438, time=4.0min
Epoch  160: Loss=52.8421, LR=7.7e-02, kappa=3.799, |gamma|=0.1415, time=4.3min
Epoch  170: Loss=52.1253, LR=7.4e-02, kappa=3.947, |gamma|=0.1397, time=4.5min
Epoch  180: Loss=51.4069, LR=7.1e-02, kappa=4.101, |gamma|=0.1388, time=4.8min
Epoch  190: Loss=50.7167, LR=6.8e-02, kappa=4.254, |gamma|=0.1379, time=5.1min
Epoch  200: Loss=50.0710, LR=6.6e-02, kappa=4.402, |gamma|=0.1372, time=5.3min
Epoch  210: Loss=49.4642, LR=6.3e-02, kappa=4.541, |gamma|=0.1362, time=5.6min
Epoch  220: Loss=48.9112, LR=5.9e-02, kappa=4.670, |gamma|=0.1354, time=5.9min
Epoch  230: Loss=48.4119, LR=5.6e-02, kappa=4.787, |gamma|=0.1346, time=6.2min
Epoch  240: Loss=47.9626, LR=5.3e-02, kappa=4.892, |gamma|=0.1337, time=6.4min
Epoch  250: Loss=47.5604, LR=5.0e-02, kappa=4.986, |gamma|=0.1330, time=6.7min
Epoch  260: Loss=47.2004, LR=4.7e-02, kappa=5.070, |gamma|=0.1324, time=7.0min
Epoch  270: Loss=46.8784, LR=4.4e-02, kappa=5.145, |gamma|=0.1317, time=7.2min
Epoch  280: Loss=46.5903, LR=4.1e-02, kappa=5.211, |gamma|=0.1312, time=7.5min
Epoch  290: Loss=46.3325, LR=3.8e-02, kappa=5.269, |gamma|=0.1306, time=7.7min
Epoch  300: Loss=46.1019, LR=3.5e-02, kappa=5.321, |gamma|=0.1301, time=8.0min
Epoch  310: Loss=45.8956, LR=3.2e-02, kappa=5.367, |gamma|=0.1295, time=8.3min
Epoch  320: Loss=45.7114, LR=2.9e-02, kappa=5.408, |gamma|=0.1290, time=8.5min
Epoch  330: Loss=45.5471, LR=2.6e-02, kappa=5.443, |gamma|=0.1285, time=8.8min
Epoch  340: Loss=45.4009, LR=2.4e-02, kappa=5.475, |gamma|=0.1281, time=9.1min
Epoch  350: Loss=45.2712, LR=2.1e-02, kappa=5.502, |gamma|=0.1278, time=9.3min
Epoch  360: Loss=45.1565, LR=1.9e-02, kappa=5.526, |gamma|=0.1274, time=9.6min
Epoch  370: Loss=45.0555, LR=1.6e-02, kappa=5.547, |gamma|=0.1271, time=9.9min
Epoch  380: Loss=44.9671, LR=1.4e-02, kappa=5.565, |gamma|=0.1269, time=10.1min
Epoch  390: Loss=44.8902, LR=1.2e-02, kappa=5.581, |gamma|=0.1266, time=10.4min
Epoch  400: Loss=44.8237, LR=1.0e-02, kappa=5.594, |gamma|=0.1264, time=10.7min
Epoch  410: Loss=44.7667, LR=8.5e-03, kappa=5.605, |gamma|=0.1262, time=10.9min
Epoch  420: Loss=44.7183, LR=7.0e-03, kappa=5.614, |gamma|=0.1260, time=11.2min
Epoch  430: Loss=44.6775, LR=5.6e-03, kappa=5.621, |gamma|=0.1259, time=11.5min
Epoch  440: Loss=44.6434, LR=4.4e-03, kappa=5.627, |gamma|=0.1258, time=11.7min
Epoch  450: Loss=44.6152, LR=3.3e-03, kappa=5.631, |gamma|=0.1257, time=12.0min
Epoch  460: Loss=44.5920, LR=2.5e-03, kappa=5.634, |gamma|=0.1256, time=12.3min
Epoch  470: Loss=44.5728, LR=1.8e-03, kappa=5.637, |gamma|=0.1256, time=12.5min
Epoch  480: Loss=44.5566, LR=1.4e-03, kappa=5.639, |gamma|=0.1255, time=12.8min
Epoch  490: Loss=44.5426, LR=1.1e-03, kappa=5.640, |gamma|=0.1255, time=13.0min

Training complete: 500 epochs in 13.3 min
Final loss: 44.5309

Final params: kappa=5.6411, mean|gamma|=0.1255
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2/enrollment_model_REPARAM_W0.0001_batch_380000_390000.pt

Batch 39 DONE in 13.4 min (38/39 completed)

======================================================================
BATCH 40/40: samples 390000-400000
Elapsed: 546 min | Est remaining: ~15 min (0.2 hrs)
======================================================================

============================================================
REPARAM v2 Training: samples 390000 to 400000
Epochs: 500, LR: 0.1, grad_clip: 5.0
============================================================

G_with_sex shape: (10000, 47)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.signature_refs = torch.tensor(signature_references, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.Y = torch.tensor(Y, dtype=torch.float32)
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.prevalence_t = torch.tensor(prevalence_t, dtype=torch.float32)

Cluster Sizes:
Cluster 0: 12 diseases
Cluster 1: 18 diseases
Cluster 2: 40 diseases
Cluster 3: 15 diseases
Cluster 4: 7 diseases
Cluster 5: 9 diseases
Cluster 6: 92 diseases
Cluster 7: 14 diseases
Cluster 8: 5 diseases
Cluster 9: 7 diseases
Cluster 10: 9 diseases
Cluster 11: 5 diseases
Cluster 12: 27 diseases
Cluster 13: 17 diseases
Cluster 14: 14 diseases
Cluster 15: 8 diseases
Cluster 16: 7 diseases
Cluster 17: 11 diseases
Cluster 18: 21 diseases
Cluster 19: 10 diseases
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.
Initializing with 20 disease states + 1 healthy state (REPARAM)
Reparameterized init complete: gamma, psi in NLL path; delta, epsilon have GP prior.

Training with cosine annealing + gradient clipping...
/Users/sarahurbut/aladynoulli2/pyScripts_forPublish/clust_huge_amp_vectorized_reparam.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  event_times_tensor = torch.tensor(event_times, dtype=torch.long)
Epoch    0: Loss=62.6000, LR=1.0e-01, kappa=1.100, |gamma|=0.1009, time=0.0min
Epoch   10: Loss=141.1868, LR=1.0e-01, kappa=2.019, |gamma|=0.0963, time=0.3min
Epoch   20: Loss=100.3224, LR=1.0e-01, kappa=2.647, |gamma|=0.1290, time=0.5min
Epoch   30: Loss=71.4356, LR=9.9e-02, kappa=2.972, |gamma|=0.1456, time=0.8min
Epoch   40: Loss=61.7329, LR=9.8e-02, kappa=3.092, |gamma|=0.1515, time=1.1min
Epoch   50: Loss=58.8660, LR=9.7e-02, kappa=3.108, |gamma|=0.1516, time=1.3min
Epoch   60: Loss=57.6283, LR=9.6e-02, kappa=3.097, |gamma|=0.1515, time=1.6min
Epoch   70: Loss=56.8712, LR=9.5e-02, kappa=3.096, |gamma|=0.1522, time=1.9min
Epoch   80: Loss=56.4013, LR=9.4e-02, kappa=3.115, |gamma|=0.1529, time=2.1min
Epoch   90: Loss=56.0325, LR=9.2e-02, kappa=3.148, |gamma|=0.1516, time=2.4min
Epoch  100: Loss=55.6199, LR=9.0e-02, kappa=3.196, |gamma|=0.1505, time=2.7min
Epoch  110: Loss=55.1676, LR=8.8e-02, kappa=3.257, |gamma|=0.1497, time=2.9min
Epoch  120: Loss=54.6532, LR=8.6e-02, kappa=3.334, |gamma|=0.1478, time=3.2min
Epoch  130: Loss=54.0915, LR=8.4e-02, kappa=3.431, |gamma|=0.1463, time=3.5min
Epoch  140: Loss=53.4478, LR=8.2e-02, kappa=3.547, |gamma|=0.1438, time=3.7min
Epoch  150: Loss=52.7602, LR=7.9e-02, kappa=3.684, |gamma|=0.1412, time=4.0min
Epoch  160: Loss=52.0418, LR=7.7e-02, kappa=3.835, |gamma|=0.1390, time=4.3min
Epoch  170: Loss=51.3059, LR=7.4e-02, kappa=3.995, |gamma|=0.1370, time=4.5min
Epoch  180: Loss=50.5784, LR=7.1e-02, kappa=4.158, |gamma|=0.1353, time=4.8min
Epoch  190: Loss=49.8854, LR=6.8e-02, kappa=4.317, |gamma|=0.1336, time=5.0min
Epoch  200: Loss=49.2421, LR=6.6e-02, kappa=4.468, |gamma|=0.1321, time=5.3min
Epoch  210: Loss=48.6417, LR=6.3e-02, kappa=4.607, |gamma|=0.1309, time=5.6min
Epoch  220: Loss=48.1033, LR=5.9e-02, kappa=4.734, |gamma|=0.1296, time=5.8min
Epoch  230: Loss=47.6211, LR=5.6e-02, kappa=4.847, |gamma|=0.1283, time=6.1min
Epoch  240: Loss=47.1910, LR=5.3e-02, kappa=4.947, |gamma|=0.1271, time=6.4min
Epoch  250: Loss=46.8069, LR=5.0e-02, kappa=5.036, |gamma|=0.1258, time=6.6min
Epoch  260: Loss=46.4638, LR=4.7e-02, kappa=5.114, |gamma|=0.1245, time=6.9min
Epoch  270: Loss=46.1561, LR=4.4e-02, kappa=5.184, |gamma|=0.1230, time=7.1min
Epoch  280: Loss=45.8761, LR=4.1e-02, kappa=5.247, |gamma|=0.1214, time=7.4min
Epoch  290: Loss=45.6222, LR=3.8e-02, kappa=5.304, |gamma|=0.1204, time=7.7min
Epoch  300: Loss=45.3944, LR=3.5e-02, kappa=5.356, |gamma|=0.1196, time=7.9min
Epoch  310: Loss=45.1910, LR=3.2e-02, kappa=5.403, |gamma|=0.1190, time=8.2min
Epoch  320: Loss=45.0097, LR=2.9e-02, kappa=5.444, |gamma|=0.1183, time=8.5min
Epoch  330: Loss=44.8485, LR=2.6e-02, kappa=5.480, |gamma|=0.1178, time=8.7min
Epoch  340: Loss=44.7053, LR=2.4e-02, kappa=5.512, |gamma|=0.1173, time=9.0min
Epoch  350: Loss=44.5786, LR=2.1e-02, kappa=5.540, |gamma|=0.1169, time=9.3min
Epoch  360: Loss=44.4667, LR=1.9e-02, kappa=5.564, |gamma|=0.1165, time=9.5min
Epoch  370: Loss=44.3684, LR=1.6e-02, kappa=5.585, |gamma|=0.1162, time=9.8min
Epoch  380: Loss=44.2825, LR=1.4e-02, kappa=5.603, |gamma|=0.1158, time=10.1min
Epoch  390: Loss=44.2078, LR=1.2e-02, kappa=5.619, |gamma|=0.1156, time=10.4min
Epoch  400: Loss=44.1433, LR=1.0e-02, kappa=5.632, |gamma|=0.1153, time=10.6min
Epoch  410: Loss=44.0881, LR=8.5e-03, kappa=5.643, |gamma|=0.1151, time=10.9min
Epoch  420: Loss=44.0412, LR=7.0e-03, kappa=5.652, |gamma|=0.1149, time=11.2min
Epoch  430: Loss=44.0017, LR=5.6e-03, kappa=5.659, |gamma|=0.1147, time=11.4min
Epoch  440: Loss=43.9688, LR=4.4e-03, kappa=5.665, |gamma|=0.1146, time=11.7min
Epoch  450: Loss=43.9416, LR=3.3e-03, kappa=5.670, |gamma|=0.1145, time=12.0min
Epoch  460: Loss=43.9192, LR=2.5e-03, kappa=5.673, |gamma|=0.1144, time=12.2min
Epoch  470: Loss=43.9007, LR=1.8e-03, kappa=5.676, |gamma|=0.1143, time=12.5min
Epoch  480: Loss=43.8851, LR=1.4e-03, kappa=5.677, |gamma|=0.1142, time=12.8min
Epoch  490: Loss=43.8716, LR=1.1e-03, kappa=5.679, |gamma|=0.1142, time=13.0min

Training complete: 500 epochs in 13.3 min
Final loss: 43.8604

Final params: kappa=5.6800, mean|gamma|=0.1141
Saved to: /Users/sarahurbut/Library/CloudStorage/Dropbox/censor_e_batchrun_vectorized_REPARAM_v2/enrollment_model_REPARAM_W0.0001_batch_390000_400000.pt

Batch 40 DONE in 13.4 min (39/39 completed)

======================================================================
ALL DONE: 39 completed, 0 failed in 560 min (9.3 hrs)
======================================================================
