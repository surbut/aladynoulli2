{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading components...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fl/ng5crz0x0fnb6c6x8dk7tfth0000gn/T/ipykernel_65939/1627225843.py:42: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  Y = torch.load(base_path + 'Y_tensor.pt')\n",
      "/var/folders/fl/ng5crz0x0fnb6c6x8dk7tfth0000gn/T/ipykernel_65939/1627225843.py:43: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  E = torch.load(base_path + 'E_enrollment_full.pt')\n",
      "/var/folders/fl/ng5crz0x0fnb6c6x8dk7tfth0000gn/T/ipykernel_65939/1627225843.py:44: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  G = torch.load(base_path + 'G_matrix.pt')\n",
      "/var/folders/fl/ng5crz0x0fnb6c6x8dk7tfth0000gn/T/ipykernel_65939/1627225843.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  essentials = torch.load(base_path + 'model_essentials.pt')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded all components successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fl/ng5crz0x0fnb6c6x8dk7tfth0000gn/T/ipykernel_65939/1627225843.py:60: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  total_checkpoint = torch.load(total_fit_path, map_location='cpu')\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.special import expit\n",
    "from scipy.stats import multivariate_normal\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import SpectralClustering  # Add this import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('/Users/sarahurbut/aladynoulli2/pyScripts')\n",
    "sys.path.append('/Users/sarahurbut/aladynoulli2/pyScripts/new_oct_revision')\n",
    "from utils import *\n",
    "import sys\n",
    "import os\n",
    "import gc\n",
    "\n",
    "\n",
    "def subset_data(Y, E, G, start_index, end_index):\n",
    "    \"\"\"Subset data based on indices.\"\"\"\n",
    "    indices = list(range(start_index, end_index))\n",
    "    Y_subset = Y[indices]\n",
    "    E_subset = E[indices]\n",
    "    G_subset = G[indices]\n",
    "    return Y_subset, E_subset, G_subset, indices\n",
    "\n",
    "def load_model_essentials(base_path='/Users/sarahurbut/Library/CloudStorage/Dropbox-Personal/data_for_running/'):\n",
    "    \"\"\"\n",
    "    Load all essential components\n",
    "    \"\"\"\n",
    "    print(\"Loading components...\")\n",
    "    \n",
    "    # Load large matrices\n",
    "    Y = torch.load(base_path + 'Y_tensor.pt')\n",
    "    E = torch.load(base_path + 'E_enrollment_full.pt')\n",
    "    G = torch.load(base_path + 'G_matrix.pt')\n",
    "    \n",
    "    # Load other components\n",
    "    essentials = torch.load(base_path + 'model_essentials.pt')\n",
    "    \n",
    "    print(\"Loaded all components successfully!\")\n",
    "    \n",
    "    return Y, E, G, essentials\n",
    "\n",
    "# Load and initialize model:\n",
    "Y, E, G, essentials = load_model_essentials()\n",
    "\n",
    "    \n",
    "    # Path to your total fit model\n",
    "from clust_huge_amp_fixedPhi import *\n",
    "total_fit_path = '/Users/sarahurbut/Library/CloudStorage/Dropbox-Personal/enrollment_model_W0.0001_fulldata_sexspecific.pt'\n",
    "total_checkpoint = torch.load(total_fit_path, map_location='cpu')\n",
    "phi_total = total_checkpoint['model_state_dict']['phi'].cpu().numpy()  # shape: (K, D, T)\n",
    "psi_total = total_checkpoint['model_state_dict']['psi'].cpu().numpy()  # shape: (K, D, T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the RDS file\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "fh_processed=pd.read_csv('/Users/sarahurbut/Library/Cloudstorage/Dropbox-Personal/baselinagefamh_full_withpcs.csv')\n",
    "len(fh_processed)\n",
    "disease_names=essentials['disease_names']\n",
    "prevalence_logit=essentials['prevalence_t']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fl/ng5crz0x0fnb6c6x8dk7tfth0000gn/T/ipykernel_65939/3965418255.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  total_checkpoint = torch.load(total_fit_path, map_location='cpu')\n"
     ]
    }
   ],
   "source": [
    "total_fit_path = '/Users/sarahurbut/Library/CloudStorage/Dropbox-Personal/enrollment_model_W0.0001_fulldata_sexspecific.pt'\n",
    "total_checkpoint = torch.load(total_fit_path, map_location='cpu')\n",
    "phi = total_checkpoint['model_state_dict']['phi'].cpu().numpy()  # shape: (K, D, T)\n",
    "psi_total = total_checkpoint['model_state_dict']['psi'].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>identifier</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>f.22009.0.1</th>\n",
       "      <th>f.22009.0.2</th>\n",
       "      <th>f.22009.0.3</th>\n",
       "      <th>f.22009.0.4</th>\n",
       "      <th>f.22009.0.5</th>\n",
       "      <th>f.22009.0.6</th>\n",
       "      <th>f.22009.0.7</th>\n",
       "      <th>f.22009.0.8</th>\n",
       "      <th>f.22009.0.9</th>\n",
       "      <th>f.22009.0.10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000015</td>\n",
       "      <td>69</td>\n",
       "      <td>1</td>\n",
       "      <td>-12.02340</td>\n",
       "      <td>3.25100</td>\n",
       "      <td>1.224590</td>\n",
       "      <td>4.776840</td>\n",
       "      <td>2.31802</td>\n",
       "      <td>2.445040</td>\n",
       "      <td>-3.067290</td>\n",
       "      <td>-1.08792</td>\n",
       "      <td>0.550964</td>\n",
       "      <td>1.201910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000023</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>-14.07040</td>\n",
       "      <td>2.83809</td>\n",
       "      <td>-1.709870</td>\n",
       "      <td>-0.782455</td>\n",
       "      <td>-7.93571</td>\n",
       "      <td>-2.176480</td>\n",
       "      <td>-0.688286</td>\n",
       "      <td>-2.59305</td>\n",
       "      <td>4.890120</td>\n",
       "      <td>5.056270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000037</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>-15.00630</td>\n",
       "      <td>4.03881</td>\n",
       "      <td>-1.711670</td>\n",
       "      <td>6.200160</td>\n",
       "      <td>-2.77040</td>\n",
       "      <td>0.809991</td>\n",
       "      <td>1.012190</td>\n",
       "      <td>3.03839</td>\n",
       "      <td>5.836320</td>\n",
       "      <td>-1.955720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000042</td>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>-12.08720</td>\n",
       "      <td>6.29019</td>\n",
       "      <td>-1.486040</td>\n",
       "      <td>-3.426860</td>\n",
       "      <td>-5.35913</td>\n",
       "      <td>-0.146506</td>\n",
       "      <td>2.788340</td>\n",
       "      <td>-4.25511</td>\n",
       "      <td>-6.282060</td>\n",
       "      <td>-0.862379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000059</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>-9.58886</td>\n",
       "      <td>3.77243</td>\n",
       "      <td>0.274298</td>\n",
       "      <td>-1.357730</td>\n",
       "      <td>1.91795</td>\n",
       "      <td>-1.627780</td>\n",
       "      <td>-0.191782</td>\n",
       "      <td>-1.28348</td>\n",
       "      <td>2.726250</td>\n",
       "      <td>1.320520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   identifier  age  sex  f.22009.0.1  f.22009.0.2  f.22009.0.3  f.22009.0.4  \\\n",
       "0     1000015   69    1    -12.02340      3.25100     1.224590     4.776840   \n",
       "1     1000023   44    1    -14.07040      2.83809    -1.709870    -0.782455   \n",
       "2     1000037   69    0    -15.00630      4.03881    -1.711670     6.200160   \n",
       "3     1000042   66    1    -12.08720      6.29019    -1.486040    -3.426860   \n",
       "4     1000059   54    0     -9.58886      3.77243     0.274298    -1.357730   \n",
       "\n",
       "   f.22009.0.5  f.22009.0.6  f.22009.0.7  f.22009.0.8  f.22009.0.9  \\\n",
       "0      2.31802     2.445040    -3.067290     -1.08792     0.550964   \n",
       "1     -7.93571    -2.176480    -0.688286     -2.59305     4.890120   \n",
       "2     -2.77040     0.809991     1.012190      3.03839     5.836320   \n",
       "3     -5.35913    -0.146506     2.788340     -4.25511    -6.282060   \n",
       "4      1.91795    -1.627780    -0.191782     -1.28348     2.726250   \n",
       "\n",
       "   f.22009.0.10  \n",
       "0      1.201910  \n",
       "1      5.056270  \n",
       "2     -1.955720  \n",
       "3     -0.862379  \n",
       "4      1.320520  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fh_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import cProfile\n",
    "import pstats\n",
    "from pstats import SortKey\n",
    "from clust_huge_amp_fixedPhi import *\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "# Define all batches (adjust end point to your actual dataset size)\n",
    "batches = [\n",
    "    (0, 10000)]\n",
    "    '''\n",
    "    (10000, 20000)],\n",
    "\n",
    "    (20000, 30000),\n",
    "    (30000, 40000),\n",
    "    (40000, 50000),\n",
    "    (50000, 60000),\n",
    "    (60000, 70000),\n",
    "    (70000, 80000),\n",
    "    (80000, 90000),\n",
    "    (90000, 100000),\n",
    "    (100000, 110000),\n",
    "    (110000, 120000),\n",
    "    (120000, 130000),\n",
    "    (130000, 140000),\n",
    "    (140000, 150000),\n",
    "    (150000, 160000),\n",
    "    (160000, 170000),\n",
    "    (170000, 180000),\n",
    "    (180000, 190000),\n",
    "    (190000, 200000),\n",
    "    (200000, 210000),\n",
    "    (210000, 220000),\n",
    "    (220000, 230000),\n",
    "    (230000, 240000),\n",
    "    (240000, 250000),\n",
    "    (250000, 260000),\n",
    "    (260000, 270000),\n",
    "    (270000, 280000),\n",
    "    (280000, 290000),\n",
    "    (290000, 300000),\n",
    "    (300000, 310000),\n",
    "    (310000, 320000),\n",
    "    (320000, 330000),\n",
    "    (330000, 340000),\n",
    "    (340000, 350000),\n",
    "    (350000, 360000),\n",
    "    (360000, 370000),\n",
    "    (370000, 380000),\n",
    "    (380000, 390000),\n",
    "    (390000, 400000),\n",
    "    # Adjust to your actual size\n",
    "]\n",
    "'''\n",
    "output_dir = \"/Users/sarahurbut/Library/CloudStorage/Dropbox/enrollment_predictions_fixedphi_withpcs/\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Will process {len(batches)} batches\")\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PROCESS EACH BATCH\n",
    "# ============================================================================\n",
    "for batch_idx, (start, stop) in enumerate(batches):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"BATCH {batch_idx+1}/{len(batches)}: Processing patients {start} to {stop}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    try:\n",
    "        # Set random seeds for reproducibility\n",
    "        torch.manual_seed(42)\n",
    "        np.random.seed(42)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed(42)\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "        \n",
    "        # Subset the data\n",
    "        print(f\"Subsetting data...\")\n",
    "        Y_batch, E_batch, G_batch, indices = subset_data(Y, E, G, \n",
    "                                                          start_index=start, \n",
    "                                                          end_index=stop)\n",
    "        \n",
    "        # Get demographics and add sex\n",
    "        pce_df_subset = fh_processed.iloc[start:stop].reset_index(drop=True)\n",
    "        sex = pce_df_subset['sex'].values\n",
    "        pc_columns = ['f.22009.0.1', 'f.22009.0.2', 'f.22009.0.3', 'f.22009.0.4', 'f.22009.0.5',\n",
    "              'f.22009.0.6', 'f.22009.0.7', 'f.22009.0.8', 'f.22009.0.9', 'f.22009.0.10']\n",
    "        pcs = pce_df_subset[pc_columns].values\n",
    "\n",
    "# Stack: G_batch + sex + 10 PCs\n",
    "        G_with_sex_pcs = np.column_stack([G_batch, sex, pcs])\n",
    "        \n",
    "        print(f\"Data shapes: Y={Y_batch.shape}, E={E_batch.shape}, G={G_with_sex.shape}\")\n",
    "        \n",
    "        # Initialize model with FIXED phi and psi\n",
    "        print(f\"Initializing model with fixed phi/psi...\")\n",
    "        model = AladynSurvivalFixedPhi(\n",
    "            N=Y_batch.shape[0],\n",
    "            D=Y_batch.shape[1],\n",
    "            T=Y_batch.shape[2],\n",
    "            K=20,\n",
    "            P=G_with_sex.shape[1],\n",
    "            G=G_with_sex,\n",
    "            Y=Y_batch,\n",
    "            R=0,\n",
    "            W=0.0001,\n",
    "            prevalence_t=essentials['prevalence_t'],\n",
    "            init_sd_scaler=1e-1,\n",
    "            genetic_scale=1,\n",
    "            pretrained_phi=phi_total,\n",
    "            pretrained_psi=psi_total,\n",
    "            signature_references=signature_refs,\n",
    "            healthy_reference=True,\n",
    "            disease_names=essentials['disease_names']\n",
    "        )\n",
    "        \n",
    "        # Verify phi and psi are fixed\n",
    "        if np.allclose(model.phi.cpu().numpy(), phi_total):\n",
    "            print(\"✓ phi matches phi_total!\")\n",
    "        else:\n",
    "            print(\"✗ WARNING: phi does NOT match phi_total!\")\n",
    "            \n",
    "        if np.allclose(model.psi.cpu().numpy(), psi_total):\n",
    "            print(\"✓ psi matches psi_total!\")\n",
    "        else:\n",
    "            print(\"✗ WARNING: psi does NOT match psi_total!\")\n",
    "        \n",
    "        # Train model (only lambda is being estimated)\n",
    "        print(f\"Training model...\")\n",
    "        profiler = cProfile.Profile()\n",
    "        profiler.enable()\n",
    "        \n",
    "        history = model.fit(\n",
    "            E_batch, \n",
    "            num_epochs=200, \n",
    "            learning_rate=1e-1, \n",
    "            lambda_reg=1e-2\n",
    "        )\n",
    "        \n",
    "        profiler.disable()\n",
    "        stats = pstats.Stats(profiler).sort_stats(SortKey.CUMULATIVE)\n",
    "        stats.print_stats(10)\n",
    "        \n",
    "        # Generate and save predictions\n",
    "        print(f\"Generating predictions...\")\n",
    "        with torch.no_grad():\n",
    "            pi, _, _ = model.forward()\n",
    "            \n",
    "            # Save predictions\n",
    "            pi_filename = os.path.join(output_dir, \n",
    "                                       f\"pi_enroll_fixedphi_sex_{start}_{stop}.pt\")\n",
    "            torch.save(pi, pi_filename)\n",
    "            print(f\"✓ Saved predictions to {pi_filename}\")\n",
    "            \n",
    "            # Save model state (optional, for debugging)\n",
    "            model_filename = os.path.join(output_dir, \n",
    "                                         f\"model_enroll_fixedphi_sex_{start}_{stop}.pt\")\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'E': E_batch,\n",
    "                'prevalence_t': model.prevalence_t,\n",
    "                'logit_prevalence_t': model.logit_prev_t,\n",
    "                'start_index': start,\n",
    "                'end_index': stop,\n",
    "                #'history': history\n",
    "            }, model_filename)\n",
    "            print(f\"✓ Saved model to {model_filename}\")\n",
    "        \n",
    "        # Clean up to free memory\n",
    "        print(f\"Cleaning up memory...\")\n",
    "        del pi, model, Y_batch, E_batch, G_batch, G_with_sex, pce_df_subset\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        print(f\"✓ Batch {batch_idx+1}/{len(batches)} complete!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ ERROR in batch {start}-{stop}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"ALL BATCHES COMPLETE!\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# ============================================================================\n",
    "# CONCATENATE ALL PREDICTIONS INTO ONE FILE\n",
    "# ============================================================================\n",
    "print(f\"\\nConcatenating all predictions into single file...\")\n",
    "pi_batches = []\n",
    "successful_batches = []\n",
    "\n",
    "for start, stop in batches:\n",
    "    pi_filename = os.path.join(output_dir, f\"pi_enroll_fixedphi_sex_{start}_{stop}.pt\")\n",
    "    try:\n",
    "        pi_batch = torch.load(pi_filename)\n",
    "        pi_batches.append(pi_batch)\n",
    "        successful_batches.append((start, stop))\n",
    "        print(f\"✓ Loaded {pi_filename}, shape: {pi_batch.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Could not load {pi_filename}: {e}\")\n",
    "\n",
    "if pi_batches:\n",
    "    print(f\"\\nConcatenating {len(pi_batches)} batches...\")\n",
    "    pi_full = torch.cat(pi_batches, dim=0)\n",
    "    print(f\"Final shape: {pi_full.shape}\")\n",
    "    \n",
    "    # Save combined file\n",
    "    full_filename = os.path.join(output_dir, \"pi_enroll_fixedphi_sex_FULL.pt\")\n",
    "    torch.save(pi_full, full_filename)\n",
    "    print(f\"✓ Saved combined predictions to {full_filename}\")\n",
    "    \n",
    "    # Save batch info\n",
    "    batch_info = {\n",
    "        'batches': successful_batches,\n",
    "        'total_patients': pi_full.shape[0],\n",
    "        'n_diseases': pi_full.shape[1],\n",
    "        'n_timepoints': pi_full.shape[2]\n",
    "    }\n",
    "    info_filename = os.path.join(output_dir, \"batch_info.pt\")\n",
    "    torch.save(batch_info, info_filename)\n",
    "    print(f\"✓ Saved batch info to {info_filename}\")\n",
    "else:\n",
    "    print(\"✗ No successful batches to concatenate!\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"DONE! Ready for washout analysis.\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=tl['history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "dir_path = \"/Users/sarahurbut/Library/CloudStorage/Dropbox/enrollment_predictions_fixedphi/\"\n",
    "\n",
    "# Get all model files\n",
    "model_files = [f for f in os.listdir(dir_path) \n",
    "               if f.startswith('model_enroll_fixedphi_sex_') and f.endswith('.pt')]\n",
    "\n",
    "print(f\"Found {len(model_files)} model files\")\n",
    "print(\"Removing history from all checkpoints...\\n\")\n",
    "\n",
    "for filename in model_files:\n",
    "    filepath = os.path.join(dir_path, filename)\n",
    "    \n",
    "    # Get original size\n",
    "    original_size = os.path.getsize(filepath) / 1e9\n",
    "    print(f\"Cleaning {filename} ({original_size:.2f} GB)...\")\n",
    "    \n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(filepath)\n",
    "    \n",
    "    # Remove history and clone tensors\n",
    "    clean_checkpoint = {\n",
    "        'model_state_dict': {k: v.clone() if isinstance(v, torch.Tensor) else v \n",
    "                            for k, v in checkpoint['model_state_dict'].items()},\n",
    "        'E': checkpoint['E'].clone(),\n",
    "        'prevalence_t': checkpoint['prevalence_t'].clone(),\n",
    "        'logit_prevalence_t': checkpoint['logit_prevalence_t'].clone(),\n",
    "        'start_index': checkpoint['start_index'],\n",
    "        'end_index': checkpoint['end_index']\n",
    "        # NO HISTORY!\n",
    "    }\n",
    "    \n",
    "    # Save cleaned version\n",
    "    torch.save(clean_checkpoint, filepath)\n",
    "    \n",
    "    new_size = os.path.getsize(filepath) / 1e9\n",
    "    saved = original_size - new_size\n",
    "    \n",
    "    print(f\"✓ {original_size:.2f} GB → {new_size:.3f} GB (saved {saved:.2f} GB)\\n\")\n",
    "    \n",
    "    # Clean up\n",
    "    del checkpoint, clean_checkpoint\n",
    "\n",
    "print(\"✅ All files cleaned! History removed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the RDS file\n",
    "readRDS = robjects.r['readRDS']\n",
    "pce_data = readRDS('/Users/sarahurbut/Library/Cloudstorage/Dropbox-Personal/pce_df_prevent.rds')\n",
    "pce_df = pandas2ri.rpy2py(pce_data)  # Convert to pandas DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if import works\n",
    "try:\n",
    "    from evaluatetdccode import evaluate_major_diseases_wsex_with_bootstrap_dynamic_1year_different_start_end_numeric_sex\n",
    "    print(\"✅ Import successful\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Import failed: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluatetdccode import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from evaluatetdccode import evaluate_major_diseases_wsex_with_bootstrap_dynamic_1year_different_start_end_numeric_sex\n",
    "\n",
    "# Load the full data once\n",
    "Y, E, G, essentials = load_model_essentials()\n",
    "fh_processed = pd.read_csv('/Users/sarahurbut/Library/Cloudstorage/Dropbox-Personal/baselinagefamh.csv')\n",
    "\n",
    "# Define batches (same as training)\n",
    "batches = [\n",
    "    (0, 10000)#, (10000, 20000), (20000, 30000), \n",
    "    # ... add all your batches\n",
    "]\n",
    "\n",
    "# Storage for results\n",
    "washout_results = {\n",
    "    '0yr': {},  # No washout\n",
    "    '1yr': {},  # 1-year washout  \n",
    "    '2yr': {}   # 2-year washout\n",
    "}\n",
    "\n",
    "# Run washout analysis on each batch\n",
    "for start, stop in batches:\n",
    "    print(f\"\\n=== Processing batch {start}-{stop} ===\")\n",
    "    \n",
    "    # Load batch predictions\n",
    "    pi_filename = f\"/Users/sarahurbut/Library/CloudStorage/Dropbox/enrollment_predictions_fixedphi/pi_enroll_fixedphi_sex_{start}_{stop}.pt\"\n",
    "    pi_batch = torch.load(pi_filename)\n",
    "    \n",
    "    # Subset other data to match\n",
    "    Y_batch = Y[start:stop]\n",
    "    E_batch = E[start:stop] \n",
    "    pce_df_batch = fh_processed.iloc[start:stop].reset_index(drop=True)\n",
    "    \n",
    "    # Run washout analysis for this batch\n",
    "    for washout_name, offset in [('0yr', 0), ('1yr', 1), ('2yr', 2)]:\n",
    "        print(f\"  Running {washout_name} washout...\")\n",
    "        \n",
    "        results = evaluate_major_diseases_wsex_with_bootstrap_dynamic_1year_different_start_end_numeric_sex(\n",
    "            pi=pi_batch,\n",
    "            Y_100k=Y_batch,\n",
    "            E_100k=E_batch,\n",
    "            disease_names=essentials['disease_names'],\n",
    "            pce_df=pce_df_batch,\n",
    "            n_bootstraps=50,  # Fewer bootstraps per batch\n",
    "            follow_up_duration_years=1,\n",
    "            start_offset=offset\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        for disease, metrics in results.items():\n",
    "            if disease not in washout_results[washout_name]:\n",
    "                washout_results[washout_name][disease] = {\n",
    "                    'aucs': [], 'cis': [], 'events': [], 'rates': []\n",
    "                }\n",
    "            \n",
    "            washout_results[washout_name][disease]['aucs'].append(metrics['auc'])\n",
    "            washout_results[washout_name][disease]['cis'].append((metrics['ci_lower'], metrics['ci_upper']))\n",
    "            washout_results[washout_name][disease]['events'].append(metrics['n_events'])\n",
    "            washout_results[washout_name][disease]['rates'].append(metrics['event_rate'])\n",
    "    \n",
    "    # Clean up memory\n",
    "    del pi_batch, Y_batch, E_batch, pce_df_batch\n",
    "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "# Aggregate results across batches\n",
    "print(\"\\n=== AGGREGATED WASHOUT RESULTS ===\")\n",
    "for washout_name, diseases in washout_results.items():\n",
    "    print(f\"\\n{washout_name.upper()} WASHOUT:\")\n",
    "    for disease, metrics in diseases.items():\n",
    "        aucs = [a for a in metrics['aucs'] if not pd.isna(a)]\n",
    "        if aucs:\n",
    "            mean_auc = np.mean(aucs)\n",
    "            print(f\"  {disease}: {mean_auc:.3f} (from {len(aucs)} batches)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_washout_results import *"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
