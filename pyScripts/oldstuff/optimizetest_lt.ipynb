{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'optimize_psi'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mload_ext\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mautoreload\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mautoreload\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptimize_psi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcluster_g_logit_init_acceptpsi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      5\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mload_ext\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mautoreload\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'optimize_psi'"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from optimize_psi import *\n",
    "from cluster_g_logit_init_acceptpsi import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.special import expit\n",
    "from scipy.stats import multivariate_normal\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import SpectralClustering  # Add this import\n",
    "\n",
    "#def load_model_essentials(base_path='/home/surbut/Downloads/data_for_running/'):\n",
    "def load_model_essentials(base_path='/Users/sarahurbut/Dropbox (Personal)/data_for_running/'):\n",
    "    \"\"\"\n",
    "    Load all essential components\n",
    "    \"\"\"\n",
    "    print(\"Loading components...\")\n",
    "    \n",
    "    # Load large matrices\n",
    "    Y = torch.load(base_path + 'Y_tensor.pt')\n",
    "    E = torch.load(base_path + 'E_matrix.pt')\n",
    "    G = torch.load(base_path + 'G_matrix.pt')\n",
    "    \n",
    "    # Load other components\n",
    "    essentials = torch.load(base_path + 'model_essentials.pt')\n",
    "    \n",
    "    print(\"Loaded all components successfully!\")\n",
    "    \n",
    "    return Y, E, G, essentials\n",
    "\n",
    "# Load and initialize model:\n",
    "Y, E, G, essentials = load_model_essentials()\n",
    "\n",
    "def subset_data(Y, E, G, n_samples=10000, start_idx=0, seed=42):\n",
    "    \"\"\"\n",
    "    Subset the data starting from start_idx\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # Take consecutive samples starting from start_idx\n",
    "    end_idx = start_idx + n_samples\n",
    "    \n",
    "    # Subset all matrices using the same indices\n",
    "    Y_sub = Y[start_idx:end_idx]\n",
    "    E_sub = E[start_idx:end_idx]\n",
    "    G_sub = G[start_idx:end_idx]\n",
    "    \n",
    "    print(f\"Using data from index {start_idx} to {end_idx}\")\n",
    "    print(f\"Shapes: Y={Y_sub.shape}, E={E_sub.shape}, G={G_sub.shape}\")\n",
    "    \n",
    "    return Y_sub, E_sub, G_sub, range(start_idx, end_idx)\n",
    "\n",
    "# Subset the data\n",
    "# Later in the notebook, when you subset the data:\n",
    "Y_100k, E_100k, G_100k, indices = subset_data(Y, E, G, n_samples=10000, start_idx=0)  \n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "# Initialize model with subsetted data\n",
    "\n",
    "psi_config = {'in_cluster': 1, 'out_cluster': -2, 'noise_in': 0.1, 'noise_out': 0.01}\n",
    "model = AladynSurvivalFixedKernelsAvgLoss_clust_logitInit_psitest(\n",
    "    N=Y_100k.shape[0],\n",
    "    D=Y_100k.shape[1],\n",
    "    T=Y_100k.shape[2],\n",
    "    K=essentials['K'],\n",
    "    P=essentials['P'],\n",
    "    G=G_100k,\n",
    "    Y=Y_100k,\n",
    "    prevalence_t=essentials['prevalence_t']\n",
    ")\n",
    "\n",
    "model.initialize_params(psi_config=psi_config)\n",
    "model.plot_initial_params()\n",
    "model.visualize_initialization()\n",
    "model.psi\n",
    "\n",
    "history = model.fit(E_100k, num_epochs=1000, learning_rate=1e-4, lambda_reg=1e-2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create figure with subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot loss\n",
    "ax1.plot(history['loss'])\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training Loss')\n",
    "ax1.grid(True)\n",
    "\n",
    "# Plot gradients\n",
    "ax2.plot(history['max_grad_lambda'], label='Lambda')\n",
    "ax2.plot(history['max_grad_phi'], label='Phi')\n",
    "ax2.plot(history['max_grad_gamma'], label='Gamma')\n",
    "ax2.plot(history['max_grad_psi'], label='Psi')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Max Gradient Magnitude')\n",
    "ax2.set_title('Parameter Gradients')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "disease_names=essentials['disease_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Save complete state for R\n",
    "save_path = '12model_complete_for_R_1211_12.pt'\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'clusters': model.clusters,\n",
    "    'psi': model.psi,\n",
    "    'Y': Y_100k,\n",
    "    'prevalence_t':essentials['prevalence_t'],\n",
    "    'loit_prevalence_t': model.logit_prev_t,\n",
    "    'G': G_100k,\n",
    "    'E': E,\n",
    "    'indices' : indices,\n",
    "    'disease_names': disease_names,\n",
    "    'hyperparameters': {\n",
    "        'N': Y_100k.shape[0],\n",
    "        'D': Y_100k.shape[1],\n",
    "        'T':Y_100k.shape[2],\n",
    "        'P': G_100k.shape[1],\n",
    "        'K': model.phi.shape[0]\n",
    "    }\n",
    "}, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Get predictions and actual values\n",
    "predicted = model.forward()\n",
    "pi_pred = predicted[0] if isinstance(predicted, tuple) else predicted\n",
    "pi_pred = pi_pred.cpu().detach().numpy()\n",
    "Y = model.Y.cpu().detach().numpy()\n",
    "\n",
    "# 2. Calculate marginal risks directly\n",
    "# Assuming dimensions are: [N, D, T] for both Y and pi_pred\n",
    "observed_risk = Y.mean(axis=0).flatten()  # average across individuals\n",
    "predicted_risk = pi_pred.mean(axis=0).flatten()\n",
    "\n",
    "# 3. Apply calibration\n",
    "scale_factor = np.mean(observed_risk) / np.mean(predicted_risk)\n",
    "calibrated_risk = predicted_risk * scale_factor\n",
    "\n",
    "# 4. Plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Original predictions\n",
    "plt.subplot(121)\n",
    "plt.scatter(observed_risk, predicted_risk, alpha=0.5)\n",
    "plt.plot([0, 0.02], [0, 0.02], 'r--')  # y=x line\n",
    "plt.title('Original Predictions')\n",
    "plt.xlabel('Observed Risk')\n",
    "plt.ylabel('Predicted Risk')\n",
    "\n",
    "# Calibrated predictions\n",
    "plt.subplot(122)\n",
    "plt.scatter(observed_risk, calibrated_risk, alpha=0.5)\n",
    "plt.plot([0, 0.02], [0, 0.02], 'r--')  # y=x line\n",
    "plt.title('Calibrated Predictions')\n",
    "plt.xlabel('Observed Risk')\n",
    "plt.ylabel('Calibrated Risk')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "print(f\"Mean observed risk: {np.mean(observed_risk):.6f}\")\n",
    "print(f\"Mean predicted risk (original): {np.mean(predicted_risk):.6f}\")\n",
    "print(f\"Mean predicted risk (calibrated): {np.mean(calibrated_risk):.6f}\")\n",
    "print(f\"Calibration scale factor: {scale_factor:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "ss_res = np.sum((observed_risk - calibrated_risk) ** 2)\n",
    "ss_tot = np.sum((observed_risk - np.mean(observed_risk)) ** 2)\n",
    "r2 = 1 - (ss_res / ss_tot)\n",
    "\n",
    "print(f\"R^2: {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "def plot_signature_top_diseases_centered(model, disease_names, n_top=10):\n",
    "    \"\"\"\n",
    "    Show top diseases for each signature, centered relative to prevalence\n",
    "    \"\"\"\n",
    "    # Get phi and prevalence\n",
    "    phi = model.phi.detach().numpy()  # Shape: (K, D, T)\n",
    "    prevalence_logit = model.logit_prev_t.detach().numpy()  # Shape: (D, T)\n",
    "    \n",
    "    # Center phi relative to prevalence\n",
    "    phi_centered = np.zeros_like(phi)\n",
    "    for k in range(phi.shape[0]):\n",
    "        for d in range(phi.shape[1]):\n",
    "            phi_centered[k, d, :] = phi[k, d, :] - prevalence_logit[d, :]\n",
    "    \n",
    "    # Average over time\n",
    "    phi_avg = phi_centered.mean(axis=2)  # Shape: (K, D)\n",
    "    \n",
    "    # For each signature, get top diseases\n",
    "    for k in range(phi_avg.shape[0]):\n",
    "        scores = phi_avg[k, :]\n",
    "        top_indices = np.argsort(scores)[-n_top:][::-1]\n",
    "        \n",
    "        print(f\"\\nTop {n_top} diseases in Signature {k} (relative to baseline):\")\n",
    "        for idx in top_indices:\n",
    "            avg_effect = scores[idx]\n",
    "            temporal_std = np.std(phi_centered[k, idx, :])\n",
    "            # Convert to odds ratio for interpretability\n",
    "            odds_ratio = np.exp(avg_effect)\n",
    "            print(f\"{disease_names[idx]}: effect={avg_effect:.3f} (OR={odds_ratio:.2f}), std={temporal_std:.3f}\")\n",
    "\n",
    "# Run visualization\n",
    "plot_signature_top_diseases_centered(model, disease_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "def compare_disease_rankings(model, disease_names, n_top=10):\n",
    "    \"\"\"\n",
    "    Compare initial vs final disease rankings for each signature\n",
    "    \"\"\"\n",
    "    # Get initial rankings from psi\n",
    "    psi = model.psi.detach().numpy()  # Shape: (K, D)\n",
    "    \n",
    "    # Get final rankings from centered phi\n",
    "    phi = model.phi.detach().numpy()  # Shape: (K, D, T)\n",
    "    prevalence_logit = model.logit_prev_t.detach().numpy()  # Shape: (D, T)\n",
    "    \n",
    "    # Center phi relative to prevalence\n",
    "    phi_centered = np.zeros_like(phi)\n",
    "    for k in range(phi.shape[0]):\n",
    "        for d in range(phi.shape[1]):\n",
    "            phi_centered[k, d, :] = phi[k, d, :] - prevalence_logit[d, :]\n",
    "    \n",
    "    # Average over time\n",
    "    phi_avg = phi_centered.mean(axis=2)  # Shape: (K, D)\n",
    "    \n",
    "    # Compare rankings for each signature\n",
    "    for k in range(phi_avg.shape[0]):\n",
    "        print(f\"\\nSignature {k}:\")\n",
    "        \n",
    "        # Get initial top diseases from psi\n",
    "        initial_scores = psi[k, :]\n",
    "        initial_top = np.argsort(initial_scores)[-n_top:][::-1]\n",
    "        \n",
    "        # Get final top diseases from phi\n",
    "        final_scores = phi_avg[k, :]\n",
    "        final_top = np.argsort(final_scores)[-n_top:][::-1]\n",
    "        \n",
    "        print(\"\\nInitial top diseases:\")\n",
    "        for i, idx in enumerate(initial_top):\n",
    "            print(f\"{i+1}. {disease_names[idx]}: {initial_scores[idx]:.3f}\")\n",
    "            \n",
    "        print(\"\\nFinal top diseases:\")\n",
    "        for i, idx in enumerate(final_top):\n",
    "            print(f\"{i+1}. {disease_names[idx]}: {final_scores[idx]:.3f}\")\n",
    "            \n",
    "        # Calculate rank changes\n",
    "        initial_ranks = {disease: rank for rank, disease in enumerate(initial_top)}\n",
    "        final_ranks = {disease: rank for rank, disease in enumerate(final_top)}\n",
    "        \n",
    "        # Find diseases that changed ranks significantly\n",
    "        changed_diseases = set(initial_top) | set(final_top)\n",
    "        for disease in changed_diseases:\n",
    "            initial_rank = initial_ranks.get(disease, n_top+1)\n",
    "            final_rank = final_ranks.get(disease, n_top+1)\n",
    "            if abs(final_rank - initial_rank) > 2:  # Threshold for significant change\n",
    "                print(f\"\\n{disease_names[disease]} changed from rank {initial_rank+1} to {final_rank+1}\")\n",
    "\n",
    "# Run comparison\n",
    "compare_disease_rankings(model, disease_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "model.visualize_clusters(disease_names)\n",
    "model.visualize_initialization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "def plot_signature_temporal_patterns(model, disease_names, n_top=10, selected_signatures=None):\n",
    "    \"\"\"\n",
    "    Show temporal patterns of top diseases for each signature\n",
    "    \"\"\"\n",
    "    #phi = model.phi.detach().numpy()  # Shape: (K, D, T)\n",
    "    #phi_avg = phi.mean(axis=2)  # Average over time\n",
    "\n",
    "     # Get phi and prevalence\n",
    "    phi = model.phi.detach().numpy()  # Shape: (K, D, T)\n",
    "    prevalence_logit = model.logit_prev_t.detach().numpy()  # Shape: (D, T)\n",
    "    \n",
    "    # Center phi relative to prevalence\n",
    "    phi_centered = np.zeros_like(phi)\n",
    "    for k in range(phi.shape[0]):\n",
    "        for d in range(phi.shape[1]):\n",
    "            phi_centered[k, d, :] = phi[k, d, :] - prevalence_logit[d, :]\n",
    "    \n",
    "    # Average over time\n",
    "    phi_avg = phi_centered.mean(axis=2)  # Shape: (K, D)\n",
    "    \n",
    "    # Select which signatures to plot\n",
    "    if selected_signatures is None:\n",
    "        selected_signatures = range(phi_avg.shape[0])\n",
    "    \n",
    "    # Create subplots for each selected signature\n",
    "    n_sigs = len(selected_signatures)\n",
    "    fig, axes = plt.subplots(n_sigs, 1, figsize=(15, 5*n_sigs))\n",
    "    if n_sigs == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, k in enumerate(selected_signatures):\n",
    "        # Get top diseases\n",
    "        scores = phi_avg[k, :]\n",
    "        top_indices = np.argsort(scores)[-n_top:][::-1]\n",
    "        \n",
    "        # Plot temporal patterns\n",
    "        ax = axes[i]\n",
    "        for idx in top_indices:\n",
    "            temporal_pattern = phi[k, idx, :]\n",
    "            ax.plot(temporal_pattern, label=disease_names[idx])\n",
    "        \n",
    "        ax.set_title(f'Signature {k} - Top Disease Temporal Patterns')\n",
    "        ax.set_xlabel('Time')\n",
    "        ax.set_ylabel('Phi Value')\n",
    "        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# First show the top diseases\n",
    "\n",
    "\n",
    "# Then show their temporal patterns\n",
    "# You can select specific signatures of interest:\n",
    "plot_signature_temporal_patterns(model, disease_names, selected_signatures=[0,1,14,15,16,13,17])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
