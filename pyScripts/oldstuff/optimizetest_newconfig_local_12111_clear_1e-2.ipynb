{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading components...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2f/nxrbr3pn1msgrl4ffyj33zd00000gn/T/ipykernel_60395/1069408875.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  Y = torch.load(base_path + 'Y_tensor.pt')\n",
      "/var/folders/2f/nxrbr3pn1msgrl4ffyj33zd00000gn/T/ipykernel_60395/1069408875.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  E = torch.load(base_path + 'E_matrix.pt')\n",
      "/var/folders/2f/nxrbr3pn1msgrl4ffyj33zd00000gn/T/ipykernel_60395/1069408875.py:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  G = torch.load(base_path + 'G_matrix.pt')\n",
      "/var/folders/2f/nxrbr3pn1msgrl4ffyj33zd00000gn/T/ipykernel_60395/1069408875.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  essentials = torch.load(base_path + 'model_essentials.pt')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded all components successfully!\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from cluster_g_logit_init_acceptpsi import *\n",
    "\n",
    "\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.special import expit\n",
    "from scipy.stats import multivariate_normal\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import SpectralClustering  # Add this import\n",
    "\n",
    "def load_model_essentials(base_path='/Users/sarahurbut/Dropbox (Personal)/data_for_running/'):\n",
    "    \"\"\"\n",
    "    Load all essential components\n",
    "    \"\"\"\n",
    "    print(\"Loading components...\")\n",
    "    \n",
    "    # Load large matrices\n",
    "    Y = torch.load(base_path + 'Y_tensor.pt')\n",
    "    E = torch.load(base_path + 'E_matrix.pt')\n",
    "    G = torch.load(base_path + 'G_matrix.pt')\n",
    "    \n",
    "    # Load other components\n",
    "    essentials = torch.load(base_path + 'model_essentials.pt')\n",
    "    \n",
    "    print(\"Loaded all components successfully!\")\n",
    "    \n",
    "    return Y, E, G, essentials\n",
    "\n",
    "# Load and initialize model:\n",
    "Y, E, G, essentials = load_model_essentials()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shapes: Y=torch.Size([407878, 348, 52]), E=torch.Size([407878, 348]), G=torch.Size([407878, 36])\n",
      "New shapes: Y=torch.Size([10000, 348, 52]), E=torch.Size([10000, 348]), G=torch.Size([10000, 36])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def subset_data(Y, E, G, n_samples=50000, seed=42):\n",
    "    \"\"\"\n",
    "    Subset the data to n_samples individuals while maintaining consistency\n",
    "    \n",
    "    Args:\n",
    "        Y: tensor of shape [N, D, T]\n",
    "        E: tensor of shape [N, D]\n",
    "        G: tensor of shape [N, P]\n",
    "        n_samples: number of individuals to keep\n",
    "        seed: random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        Y_sub, E_sub, G_sub: subsetted tensors\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # Get total number of individuals\n",
    "    N = Y.shape[0]\n",
    "    \n",
    "    # Randomly select n_samples indices\n",
    "    indices = torch.randperm(N)[:n_samples]\n",
    "    \n",
    "    # Subset all matrices using the same indices\n",
    "    Y_sub = Y[indices]\n",
    "    E_sub = E[indices]\n",
    "    G_sub = G[indices]\n",
    "    \n",
    "    print(f\"Original shapes: Y={Y.shape}, E={E.shape}, G={G.shape}\")\n",
    "    print(f\"New shapes: Y={Y_sub.shape}, E={E_sub.shape}, G={G_sub.shape}\")\n",
    "    \n",
    "    return Y_sub, E_sub, G_sub, indices\n",
    "\n",
    "# Subset the data\n",
    "Y_100k, E_100k, G_100k, indices = subset_data(Y, E, G, n_samples=10000,seed=1)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "# Initialize model with subsetted data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original cluster sizes:\n",
      "Cluster 0: 16 diseases\n",
      "Cluster 1: 21 diseases\n",
      "Cluster 2: 15 diseases\n",
      "Cluster 3: 82 diseases\n",
      "Cluster 4: 5 diseases\n",
      "Cluster 5: 7 diseases\n",
      "Cluster 6: 8 diseases\n",
      "Cluster 7: 22 diseases\n",
      "Cluster 8: 28 diseases\n",
      "Cluster 9: 12 diseases\n",
      "Cluster 10: 11 diseases\n",
      "Cluster 11: 8 diseases\n",
      "Cluster 12: 7 diseases\n",
      "Cluster 13: 13 diseases\n",
      "Cluster 14: 10 diseases\n",
      "Cluster 15: 5 diseases\n",
      "Cluster 16: 29 diseases\n",
      "Cluster 17: 17 diseases\n",
      "Cluster 18: 9 diseases\n",
      "Cluster 19: 23 diseases\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2f/nxrbr3pn1msgrl4ffyj33zd00000gn/T/ipykernel_60395/685643090.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  initial_psi = torch.load('initial_psi_400k.pt')\n",
      "/var/folders/2f/nxrbr3pn1msgrl4ffyj33zd00000gn/T/ipykernel_60395/685643090.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  initial_clusters = torch.load('initial_clusters_400k.pt')\n"
     ]
    }
   ],
   "source": [
    "# When initializing the model:\n",
    "original_G = G_100k.clone().detach()  # Store the original G - proper tensor copy\n",
    "\n",
    "# Now in your batch run, load and verify:\n",
    "initial_psi = torch.load('initial_psi_400k.pt')\n",
    "initial_clusters = torch.load('initial_clusters_400k.pt')\n",
    "\n",
    "original_cluster_sizes = {}\n",
    "unique, counts = np.unique(initial_clusters, return_counts=True)\n",
    "for k, count in zip(unique, counts):\n",
    "    original_cluster_sizes[k] = count\n",
    "print(\"\\nOriginal cluster sizes:\")\n",
    "for k, count in original_cluster_sizes.items():\n",
    "    print(f\"Cluster {k}: {count} diseases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarahurbut/aladynoulli/pyScripts/cluster_g_logit_init_acceptpsi.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.G = torch.tensor(G, dtype=torch.float32)\n",
      "/Users/sarahurbut/aladynoulli/pyScripts/cluster_g_logit_init_acceptpsi.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.G = torch.tensor(G_scaled, dtype=torch.float32)\n",
      "/Users/sarahurbut/aladynoulli/pyScripts/cluster_g_logit_init_acceptpsi.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.Y = torch.tensor(Y, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda kernel condition number: 56314924.00\n",
      "Phi kernel condition number: 59684776.00\n",
      "\n",
      "Cluster Sizes:\n",
      "Cluster 0: 11 diseases\n",
      "Cluster 1: 6 diseases\n",
      "Cluster 2: 13 diseases\n",
      "Cluster 3: 97 diseases\n",
      "Cluster 4: 22 diseases\n",
      "Cluster 5: 22 diseases\n",
      "Cluster 6: 15 diseases\n",
      "Cluster 7: 17 diseases\n",
      "Cluster 8: 35 diseases\n",
      "Cluster 9: 6 diseases\n",
      "Cluster 10: 13 diseases\n",
      "Cluster 11: 9 diseases\n",
      "Cluster 12: 11 diseases\n",
      "Cluster 13: 6 diseases\n",
      "Cluster 14: 17 diseases\n",
      "Cluster 15: 5 diseases\n",
      "Cluster 16: 15 diseases\n",
      "Cluster 17: 8 diseases\n",
      "Cluster 18: 17 diseases\n",
      "Cluster 19: 3 diseases\n",
      "Initialization complete!\n",
      "\n",
      "Using true psi from simulation\n",
      "Initialization complete!\n",
      "\n",
      "Clusters match exactly: True\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "model = AladynSurvivalFixedKernelsAvgLoss_clust_logitInit_psitest(\n",
    "    N=Y_100k.shape[0],\n",
    "    D=Y_100k.shape[1],\n",
    "    T=Y_100k.shape[2],\n",
    "    K=essentials['K'],\n",
    "    P=essentials['P'],\n",
    "    G=G_100k,\n",
    "    Y=Y_100k,\n",
    "    prevalence_t=essentials['prevalence_t']\n",
    ")\n",
    "\n",
    "# Initialize and train\n",
    "model.initialize_params(true_psi=initial_psi)\n",
    "model.clusters = initial_clusters\n",
    "clusters_match = np.array_equal(initial_clusters, model.clusters)\n",
    "print(f\"\\nClusters match exactly: {clusters_match}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2f/nxrbr3pn1msgrl4ffyj33zd00000gn/T/ipykernel_60395/1616550811.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  initial_psi = torch.load('initial_psi_400k.pt')\n",
      "/var/folders/2f/nxrbr3pn1msgrl4ffyj33zd00000gn/T/ipykernel_60395/1616550811.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  initial_clusters = torch.load('initial_clusters_400k.pt')\n",
      "/Users/sarahurbut/aladynoulli/pyScripts/cluster_g_logit_init_acceptpsi.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.G = torch.tensor(G, dtype=torch.float32)\n",
      "/Users/sarahurbut/aladynoulli/pyScripts/cluster_g_logit_init_acceptpsi.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.G = torch.tensor(G_scaled, dtype=torch.float32)\n",
      "/Users/sarahurbut/aladynoulli/pyScripts/cluster_g_logit_init_acceptpsi.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.Y = torch.tensor(Y, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda kernel condition number: 56314924.00\n",
      "Phi kernel condition number: 59684776.00\n",
      "\n",
      "Cluster Sizes:\n",
      "Cluster 0: 11 diseases\n",
      "Cluster 1: 6 diseases\n",
      "Cluster 2: 13 diseases\n",
      "Cluster 3: 97 diseases\n",
      "Cluster 4: 22 diseases\n",
      "Cluster 5: 22 diseases\n",
      "Cluster 6: 15 diseases\n",
      "Cluster 7: 17 diseases\n",
      "Cluster 8: 35 diseases\n",
      "Cluster 9: 6 diseases\n",
      "Cluster 10: 13 diseases\n",
      "Cluster 11: 9 diseases\n",
      "Cluster 12: 11 diseases\n",
      "Cluster 13: 6 diseases\n",
      "Cluster 14: 17 diseases\n",
      "Cluster 15: 5 diseases\n",
      "Cluster 16: 15 diseases\n",
      "Cluster 17: 8 diseases\n",
      "Cluster 18: 17 diseases\n",
      "Cluster 19: 3 diseases\n",
      "Initialization complete!\n",
      "\n",
      "Using true psi from simulation\n",
      "Initialization complete!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "initial_psi = torch.load('initial_psi_400k.pt')\n",
    "initial_clusters = torch.load('initial_clusters_400k.pt')\n",
    "\n",
    "model = AladynSurvivalFixedKernelsAvgLoss_clust_logitInit_psitest(\n",
    "    N=Y_100k.shape[0],\n",
    "    D=Y_100k.shape[1],\n",
    "    T=Y_100k.shape[2],\n",
    "    K=essentials['K'],\n",
    "    P=essentials['P'],\n",
    "    G=G_100k,\n",
    "    Y=Y_100k,\n",
    "    prevalence_t=essentials['prevalence_t']\n",
    ")\n",
    "\n",
    "# Initialize with saved parameters\n",
    "model.initialize_params(true_psi=initial_psi)\n",
    "model.clusters = initial_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "initial_gamma = model.gamma.detach().clone()\n",
    "initial_phi = model.phi.detach().clone()\n",
    "initial_lambda = model.lambda_.detach().clone()\n",
    "initial_psi = model.psi.detach().clone()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarahurbut/aladynoulli/pyScripts/cluster_g_logit_init_acceptpsi.py:209: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  event_times_tensor = torch.tensor(event_times, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask_before_event shape: torch.Size([10000, 348, 52])\n",
      "mask_at_event shape: torch.Size([10000, 348, 52])\n",
      "\n",
      "Epoch 0\n",
      "Loss: 1104.8180\n",
      "Psi gradient stats:\n",
      "Mean: -5.0435e-05\n",
      "Std:  4.7890e-03\n",
      "Max:  2.0796e-02\n",
      "Min:  -1.6971e-02\n",
      "Epoch 0, Loss: 1104.8180, Gradients - Lambda: 5.200e-01, Phi: 1.330e+01, Gamma: 5.746e-02, Psi: 2.080e-02\n",
      "mask_before_event shape: torch.Size([10000, 348, 52])\n",
      "mask_at_event shape: torch.Size([10000, 348, 52])\n",
      "\n",
      "Epoch 1\n",
      "Loss: 100282.2031\n",
      "Psi gradient stats:\n",
      "Mean: -7.3001e-06\n",
      "Std:  6.7115e-03\n",
      "Max:  3.0121e-02\n",
      "Min:  -2.3804e-02\n",
      "Epoch 1, Loss: 100282.2031, Gradients - Lambda: 4.193e+00, Phi: 9.145e+01, Gamma: 1.072e-01, Psi: 3.012e-02\n",
      "\n",
      "Estimated total training time: 51.5 minutes\n",
      "mask_before_event shape: torch.Size([10000, 348, 52])\n",
      "mask_at_event shape: torch.Size([10000, 348, 52])\n",
      "\n",
      "Epoch 2\n",
      "Loss: 9418.9199\n",
      "Psi gradient stats:\n",
      "Mean: -3.1704e-05\n",
      "Std:  4.9119e-03\n",
      "Max:  2.2282e-02\n",
      "Min:  -1.8841e-02\n",
      "Epoch 2, Loss: 9418.9199, Gradients - Lambda: 2.541e+00, Phi: 6.096e+01, Gamma: 7.469e-02, Psi: 2.228e-02\n",
      "mask_before_event shape: torch.Size([10000, 348, 52])\n",
      "mask_at_event shape: torch.Size([10000, 348, 52])\n",
      "\n",
      "Epoch 3\n",
      "Loss: 23197.8418\n",
      "Psi gradient stats:\n",
      "Mean: -5.0611e-05\n",
      "Std:  5.2220e-03\n",
      "Max:  2.1321e-02\n",
      "Min:  -1.8391e-02\n",
      "Epoch 3, Loss: 23197.8418, Gradients - Lambda: 2.718e+00, Phi: 5.787e+01, Gamma: 7.756e-02, Psi: 2.132e-02\n",
      "mask_before_event shape: torch.Size([10000, 348, 52])\n",
      "mask_at_event shape: torch.Size([10000, 348, 52])\n",
      "\n",
      "Epoch 4\n",
      "Loss: 54697.8828\n",
      "Psi gradient stats:\n",
      "Mean: -5.6976e-05\n",
      "Std:  5.7721e-03\n",
      "Max:  2.2011e-02\n",
      "Min:  -2.1042e-02\n",
      "Epoch 4, Loss: 54697.8828, Gradients - Lambda: 3.061e+00, Phi: 6.922e+01, Gamma: 8.602e-02, Psi: 2.201e-02\n",
      "mask_before_event shape: torch.Size([10000, 348, 52])\n",
      "mask_at_event shape: torch.Size([10000, 348, 52])\n",
      "\n",
      "Epoch 5\n",
      "Loss: 38997.2031\n",
      "Psi gradient stats:\n",
      "Mean: -5.6890e-05\n",
      "Std:  5.3172e-03\n",
      "Max:  2.0826e-02\n",
      "Min:  -1.9695e-02\n",
      "Epoch 5, Loss: 38997.2031, Gradients - Lambda: 2.630e+00, Phi: 5.918e+01, Gamma: 6.937e-02, Psi: 2.083e-02\n",
      "mask_before_event shape: torch.Size([10000, 348, 52])\n",
      "mask_at_event shape: torch.Size([10000, 348, 52])\n",
      "\n",
      "Epoch 6\n",
      "Loss: 11427.6074\n",
      "Psi gradient stats:\n",
      "Mean: -5.6237e-05\n",
      "Std:  4.5699e-03\n",
      "Max:  1.9887e-02\n",
      "Min:  -1.6962e-02\n",
      "Epoch 6, Loss: 11427.6074, Gradients - Lambda: 2.151e+00, Phi: 4.812e+01, Gamma: 5.949e-02, Psi: 1.989e-02\n",
      "mask_before_event shape: torch.Size([10000, 348, 52])\n",
      "mask_at_event shape: torch.Size([10000, 348, 52])\n",
      "\n",
      "Epoch 7\n",
      "Loss: 4036.6118\n",
      "Psi gradient stats:\n",
      "Mean: -5.6870e-05\n",
      "Std:  4.3222e-03\n",
      "Max:  1.8841e-02\n",
      "Min:  -1.8198e-02\n",
      "Epoch 7, Loss: 4036.6118, Gradients - Lambda: 2.485e+00, Phi: 5.965e+01, Gamma: 7.668e-02, Psi: 1.884e-02\n",
      "mask_before_event shape: torch.Size([10000, 348, 52])\n",
      "mask_at_event shape: torch.Size([10000, 348, 52])\n",
      "\n",
      "Epoch 8\n",
      "Loss: 16622.1465\n",
      "Psi gradient stats:\n",
      "Mean: -6.1987e-05\n",
      "Std:  4.6265e-03\n",
      "Max:  2.0962e-02\n",
      "Min:  -1.8354e-02\n",
      "Epoch 8, Loss: 16622.1465, Gradients - Lambda: 2.724e+00, Phi: 6.115e+01, Gamma: 8.528e-02, Psi: 2.096e-02\n",
      "mask_before_event shape: torch.Size([10000, 348, 52])\n",
      "mask_at_event shape: torch.Size([10000, 348, 52])\n",
      "\n",
      "Epoch 9\n",
      "Loss: 26743.1934\n",
      "Psi gradient stats:\n",
      "Mean: -7.1578e-05\n",
      "Std:  4.8517e-03\n",
      "Max:  2.2148e-02\n",
      "Min:  -1.8703e-02\n",
      "Epoch 9, Loss: 26743.1934, Gradients - Lambda: 2.479e+00, Phi: 5.017e+01, Gamma: 7.664e-02, Psi: 2.215e-02\n",
      "mask_before_event shape: torch.Size([10000, 348, 52])\n",
      "mask_at_event shape: torch.Size([10000, 348, 52])\n",
      "\n",
      "Epoch 10\n",
      "Loss: 21936.9102\n",
      "Psi gradient stats:\n",
      "Mean: -8.1413e-05\n",
      "Std:  4.6959e-03\n",
      "Max:  2.1572e-02\n",
      "Min:  -1.8394e-02\n",
      "Epoch 10, Loss: 21936.9102, Gradients - Lambda: 2.020e+00, Phi: 4.512e+01, Gamma: 6.297e-02, Psi: 2.157e-02\n",
      "\n",
      "Early stopping triggered at epoch 10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history_new = model.fit(E_100k, num_epochs=100, learning_rate=1e-2, lambda_reg=1e-2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Pretrained model stats:\")\n",
    "print(f\"G stats: mean={self.G.mean():.3f}, std={self.G.std():.3f}\")\n",
    "print(f\"gamma stats: mean={self.gamma.mean():.3f}, std={self.gamma.std():.3f}\")\n",
    "print(f\"Initial lambda stats: mean={self.lambda_.mean():.3f}, std={self.lambda_.std():.3f}\")\n",
    "print(f\"phi stats: mean={self.phi.mean():.3f}, std={self.phi.std():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# First get the trained parameters from your existing model\n",
    "pretrained_phi = model.phi.detach().clone()\n",
    "pretrained_gamma = model.gamma.detach().clone()\n",
    "pretrained_psi = model.psi.detach().clone()\n",
    "\n",
    "# Get a new batch of 10k individuals\n",
    "Y_new, E_new, G_new, new_indices = subset_data(Y, E, G, n_samples=10000, seed=43)  # Using different seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from gp_clust_pretrained import *\n",
    "del pretrained_model\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "# Create pretrained model for new data\n",
    "pretrained_model = AladynSurvivalPretrainedModel(\n",
    "    N=Y_new.shape[0],  # 10000\n",
    "    D=Y_new.shape[1],  # same number of diseases\n",
    "    T=Y_new.shape[2],  # same time points\n",
    "    K=essentials['K'],  # same number of clusters\n",
    "    P=essentials['P'],  # same number of genetic components\n",
    "    G=G_new,           # genetic data for new individuals\n",
    "    Y=Y_new,           # outcome data for new individuals\n",
    "    prevalence_t=essentials['prevalence_t'],  # same prevalence\n",
    "    pretrained_phi=pretrained_phi,\n",
    "    pretrained_gamma=pretrained_gamma,\n",
    "    pretrained_psi=pretrained_psi\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Fit only lambda parameters for new individuals\n",
    "history_new = pretrained_model.fit(E_new, num_epochs=100, learning_rate=1e-3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create figure with subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot loss\n",
    "ax1.plot(history_new['loss'])\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training Loss')\n",
    "ax1.grid(True)\n",
    "\n",
    "# Plot gradients\n",
    "ax2.plot(history_new['max_grad_lambda'], label='Lambda')\n",
    "\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Max Gradient Magnitude')\n",
    "ax2.set_title('Parameter Gradients')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "disease_names=essentials['disease_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert gamma tensors to numpy arrays\n",
    "initial_gamma_np = initial_gamma.detach().numpy()\n",
    "final_gamma_np = model.gamma.detach().numpy()\n",
    "\n",
    "# Create a figure with two subplots side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Plot initial gamma\n",
    "sns.heatmap(initial_gamma_np, ax=ax1, cmap='RdBu_r', center=0)\n",
    "ax1.set_title('Initial Gamma')\n",
    "\n",
    "# Plot final gamma\n",
    "sns.heatmap(final_gamma_np, ax=ax2, cmap='RdBu_r', center=0)\n",
    "ax2.set_title('Final Gamma')\n",
    "\n",
    "# Add a title to the figure\n",
    "plt.suptitle('Comparison of Initial vs Final Gamma Values', fontsize=16)\n",
    "\n",
    "# You can also add a colorbar\n",
    "plt.tight_layout()\n",
    "\n",
    "# To see the actual difference, you can also create a difference heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "diff = final_gamma_np - initial_gamma_np\n",
    "sns.heatmap(diff, cmap='RdBu_r', center=0)\n",
    "plt.title('Difference (Final - Initial Gamma)')\n",
    "plt.show()\n",
    "\n",
    "# Print some summary statistics\n",
    "print(f\"Mean absolute difference: {np.abs(diff).mean():.4f}\")\n",
    "print(f\"Max absolute difference: {np.abs(diff).max():.4f}\")\n",
    "print(f\"Standard deviation of differences: {np.std(diff):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Convert phi tensors to numpy arrays\n",
    "initial_phi_np = initial_phi.detach().numpy()  # Shape: (K, D, T)\n",
    "final_phi_np = model.phi.detach().numpy()\n",
    "\n",
    "# Take mean over time dimension\n",
    "initial_phi_mean = initial_phi_np.mean(axis=2)  # Shape: (K, D)\n",
    "final_phi_mean = final_phi_np.mean(axis=2)\n",
    "\n",
    "# Create figure with two subplots side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Plot initial phi (averaged over time)\n",
    "sns.heatmap(initial_phi_mean, ax=ax1, cmap='RdBu_r', center=0)\n",
    "ax1.set_title('Initial Phi (Mean over Time)')\n",
    "ax1.set_xlabel('Disease')\n",
    "ax1.set_ylabel('Signature')\n",
    "\n",
    "# Plot final phi (averaged over time)\n",
    "sns.heatmap(final_phi_mean, ax=ax2, cmap='RdBu_r', center=0)\n",
    "ax2.set_title('Final Phi (Mean over Time)')\n",
    "ax2.set_xlabel('Disease')\n",
    "ax2.set_ylabel('Signature')\n",
    "\n",
    "plt.suptitle('Comparison of Initial vs Final Phi Values (Averaged over Time)', fontsize=16)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Plot difference\n",
    "plt.figure(figsize=(10, 8))\n",
    "diff_mean = final_phi_mean - initial_phi_mean\n",
    "sns.heatmap(diff_mean, cmap='RdBu_r', center=0)\n",
    "plt.title('Difference in Phi (Final - Initial), Mean over Time')\n",
    "plt.xlabel('Disease')\n",
    "plt.ylabel('Signature')\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics for both full tensor and time-averaged differences\n",
    "print(\"Full tensor statistics:\")\n",
    "diff_full = final_phi_np - initial_phi_np\n",
    "print(f\"Mean absolute difference: {np.abs(diff_full).mean():.4f}\")\n",
    "print(f\"Max absolute difference: {np.abs(diff_full).max():.4f}\")\n",
    "print(f\"Standard deviation of differences: {np.std(diff_full):.4f}\")\n",
    "\n",
    "print(\"\\nTime-averaged statistics:\")\n",
    "print(f\"Mean absolute difference: {np.abs(diff_mean).mean():.4f}\")\n",
    "print(f\"Max absolute difference: {np.abs(diff_mean).max():.4f}\")\n",
    "print(f\"Standard deviation of differences: {np.std(diff_mean):.4f}\")\n",
    "\n",
    "# Optionally, look at temporal variation\n",
    "temporal_std = np.std(diff_full, axis=2)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(temporal_std, cmap='viridis')\n",
    "plt.title('Standard Deviation of Changes Across Time')\n",
    "plt.xlabel('Disease')\n",
    "plt.ylabel('Signature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "Y_global=Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "Y_global.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Get predictions and actual values\n",
    "predicted = model.forward()\n",
    "pi_pred = predicted[0] if isinstance(predicted, tuple) else predicted\n",
    "pi_pred = pi_pred.cpu().detach().numpy()\n",
    "Y = model.Y.cpu().detach().numpy()\n",
    "\n",
    "# 2. Calculate marginal risks directly\n",
    "# Assuming dimensions are: [N, D, T] for both Y and pi_pred\n",
    "observed_risk = Y.mean(axis=0).flatten()  # average across individuals\n",
    "predicted_risk = pi_pred.mean(axis=0).flatten()\n",
    "\n",
    "# 3. Apply calibration\n",
    "scale_factor = np.mean(observed_risk) / np.mean(predicted_risk)\n",
    "calibrated_risk = predicted_risk * scale_factor\n",
    "\n",
    "# 4. Plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Original predictions\n",
    "plt.subplot(121)\n",
    "plt.scatter(observed_risk, predicted_risk, alpha=0.5)\n",
    "plt.plot([0, 0.02], [0, 0.02], 'r--')  # y=x line\n",
    "plt.title('Original Predictions')\n",
    "plt.xlabel('Observed Risk')\n",
    "plt.ylabel('Predicted Risk')\n",
    "\n",
    "# Calibrated predictions\n",
    "plt.subplot(122)\n",
    "plt.scatter(observed_risk, calibrated_risk, alpha=0.5)\n",
    "plt.plot([0, 0.02], [0, 0.02], 'r--')  # y=x line\n",
    "plt.title('Calibrated Predictions')\n",
    "plt.xlabel('Observed Risk')\n",
    "plt.ylabel('Calibrated Risk')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "print(f\"Mean observed risk: {np.mean(observed_risk):.6f}\")\n",
    "print(f\"Mean predicted risk (original): {np.mean(predicted_risk):.6f}\")\n",
    "print(f\"Mean predicted risk (calibrated): {np.mean(calibrated_risk):.6f}\")\n",
    "print(f\"Calibration scale factor: {scale_factor:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "ss_res = np.sum((observed_risk - calibrated_risk) ** 2)\n",
    "ss_tot = np.sum((observed_risk - np.mean(observed_risk)) ** 2)\n",
    "r2 = 1 - (ss_res / ss_tot)\n",
    "\n",
    "print(f\"R^2: {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Get predictions and actual values\n",
    "predicted = pretrained_model.forward()\n",
    "pi_pred = predicted[0] if isinstance(predicted, tuple) else predicted\n",
    "pi_pred = pi_pred.cpu().detach().numpy()\n",
    "Y = pretrained_model.Y.cpu().detach().numpy()\n",
    "\n",
    "# 2. Calculate marginal risks directly\n",
    "# Assuming dimensions are: [N, D, T] for both Y and pi_pred\n",
    "observed_risk = Y.mean(axis=0).flatten()  # average across individuals\n",
    "predicted_risk = pi_pred.mean(axis=0).flatten()\n",
    "\n",
    "# 3. Apply calibration\n",
    "scale_factor = np.mean(observed_risk) / np.mean(predicted_risk)\n",
    "calibrated_risk = predicted_risk * scale_factor\n",
    "\n",
    "# 4. Plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Original predictions\n",
    "plt.subplot(121)\n",
    "plt.scatter(observed_risk, predicted_risk, alpha=0.5)\n",
    "plt.plot([0, 0.02], [0, 0.02], 'r--')  # y=x line\n",
    "plt.title('Original Predictions')\n",
    "plt.xlabel('Observed Risk')\n",
    "plt.ylabel('Predicted Risk')\n",
    "\n",
    "# Calibrated predictions\n",
    "plt.subplot(122)\n",
    "plt.scatter(observed_risk, calibrated_risk, alpha=0.5)\n",
    "plt.plot([0, 0.02], [0, 0.02], 'r--')  # y=x line\n",
    "plt.title('Calibrated Predictions')\n",
    "plt.xlabel('Observed Risk')\n",
    "plt.ylabel('Calibrated Risk')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "print(f\"Mean observed risk: {np.mean(observed_risk):.6f}\")\n",
    "print(f\"Mean predicted risk (original): {np.mean(predicted_risk):.6f}\")\n",
    "print(f\"Mean predicted risk (calibrated): {np.mean(calibrated_risk):.6f}\")\n",
    "print(f\"Calibration scale factor: {scale_factor:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "ss_res = np.sum((observed_risk - calibrated_risk) ** 2)\n",
    "ss_tot = np.sum((observed_risk - np.mean(observed_risk)) ** 2)\n",
    "r2 = 1 - (ss_res / ss_tot)\n",
    "\n",
    "print(f\"R^2: {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "def plot_signature_top_diseases_centered(model, disease_names, n_top=10):\n",
    "    \"\"\"\n",
    "    Show top diseases for each signature, centered relative to prevalence\n",
    "    \"\"\"\n",
    "    # Get phi and prevalence\n",
    "    phi = model.phi.detach().numpy()  # Shape: (K, D, T)\n",
    "    prevalence_logit = model.logit_prev_t.detach().numpy()  # Shape: (D, T)\n",
    "    \n",
    "    # Center phi relative to prevalence\n",
    "    phi_centered = np.zeros_like(phi)\n",
    "    for k in range(phi.shape[0]):\n",
    "        for d in range(phi.shape[1]):\n",
    "            phi_centered[k, d, :] = phi[k, d, :] - prevalence_logit[d, :]\n",
    "    \n",
    "    # Average over time\n",
    "    phi_avg = phi_centered.mean(axis=2)  # Shape: (K, D)\n",
    "    \n",
    "    # For each signature, get top diseases\n",
    "    for k in range(phi_avg.shape[0]):\n",
    "        scores = phi_avg[k, :]\n",
    "        top_indices = np.argsort(scores)[-n_top:][::-1]\n",
    "        \n",
    "        print(f\"\\nTop {n_top} diseases in Signature {k} (relative to baseline):\")\n",
    "        for idx in top_indices:\n",
    "            avg_effect = scores[idx]\n",
    "            temporal_std = np.std(phi_centered[k, idx, :])\n",
    "            # Convert to odds ratio for interpretability\n",
    "            odds_ratio = np.exp(avg_effect)\n",
    "            print(f\"{disease_names[idx]}: effect={avg_effect:.3f} (OR={odds_ratio:.2f}), std={temporal_std:.3f}\")\n",
    "\n",
    "# Run visualization\n",
    "plot_signature_top_diseases_centered(model, essentials['disease_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "def compare_disease_rankings(model, disease_names, n_top=10):\n",
    "    \"\"\"\n",
    "    Compare initial vs final disease rankings for each signature\n",
    "    \"\"\"\n",
    "    # Get initial rankings from psi\n",
    "    psi = model.psi.detach().numpy()  # Shape: (K, D)\n",
    "    \n",
    "    # Get final rankings from centered phi\n",
    "    phi = model.phi.detach().numpy()  # Shape: (K, D, T)\n",
    "    prevalence_logit = model.logit_prev_t.detach().numpy()  # Shape: (D, T)\n",
    "    \n",
    "    # Center phi relative to prevalence\n",
    "    phi_centered = np.zeros_like(phi)\n",
    "    for k in range(phi.shape[0]):\n",
    "        for d in range(phi.shape[1]):\n",
    "            phi_centered[k, d, :] = phi[k, d, :] - prevalence_logit[d, :]\n",
    "    \n",
    "    # Average over time\n",
    "    phi_avg = phi_centered.mean(axis=2)  # Shape: (K, D)\n",
    "    \n",
    "    # Compare rankings for each signature\n",
    "    for k in range(phi_avg.shape[0]):\n",
    "        print(f\"\\nSignature {k}:\")\n",
    "        \n",
    "        # Get initial top diseases from psi\n",
    "        initial_scores = psi[k, :]\n",
    "        initial_top = np.argsort(initial_scores)[-n_top:][::-1]\n",
    "        \n",
    "        # Get final top diseases from phi\n",
    "        final_scores = phi_avg[k, :]\n",
    "        final_top = np.argsort(final_scores)[-n_top:][::-1]\n",
    "        \n",
    "        print(\"\\nInitial top diseases:\")\n",
    "        for i, idx in enumerate(initial_top):\n",
    "            print(f\"{i+1}. {disease_names[idx]}: {initial_scores[idx]:.3f}\")\n",
    "            \n",
    "        print(\"\\nFinal top diseases:\")\n",
    "        for i, idx in enumerate(final_top):\n",
    "            print(f\"{i+1}. {disease_names[idx]}: {final_scores[idx]:.3f}\")\n",
    "            \n",
    "        # Calculate rank changes\n",
    "        initial_ranks = {disease: rank for rank, disease in enumerate(initial_top)}\n",
    "        final_ranks = {disease: rank for rank, disease in enumerate(final_top)}\n",
    "        \n",
    "        # Find diseases that changed ranks significantly\n",
    "        changed_diseases = set(initial_top) | set(final_top)\n",
    "        for disease in changed_diseases:\n",
    "            initial_rank = initial_ranks.get(disease, n_top+1)\n",
    "            final_rank = final_ranks.get(disease, n_top+1)\n",
    "            if abs(final_rank - initial_rank) > 2:  # Threshold for significant change\n",
    "                print(f\"\\n{disease_names[disease]} changed from rank {initial_rank+1} to {final_rank+1}\")\n",
    "\n",
    "# Run comparison\n",
    "compare_disease_rankings(model, essentials['disease_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "def plot_signature_temporal_patterns(model, disease_names, n_top=10, selected_signatures=None):\n",
    "    \"\"\"\n",
    "    Show temporal patterns of top diseases for each signature\n",
    "    \"\"\"\n",
    "    #phi = model.phi.detach().numpy()  # Shape: (K, D, T)\n",
    "    #phi_avg = phi.mean(axis=2)  # Average over time\n",
    "\n",
    "     # Get phi and prevalence\n",
    "    phi = model.phi.detach().numpy()  # Shape: (K, D, T)\n",
    "    prevalence_logit = model.logit_prev_t.detach().numpy()  # Shape: (D, T)\n",
    "    \n",
    "    # Center phi relative to prevalence\n",
    "    phi_centered = np.zeros_like(phi)\n",
    "    for k in range(phi.shape[0]):\n",
    "        for d in range(phi.shape[1]):\n",
    "            phi_centered[k, d, :] = phi[k, d, :] - prevalence_logit[d, :]\n",
    "    \n",
    "    # Average over time\n",
    "    phi_avg = phi_centered.mean(axis=2)  # Shape: (K, D)\n",
    "    \n",
    "    # Select which signatures to plot\n",
    "    if selected_signatures is None:\n",
    "        selected_signatures = range(phi_avg.shape[0])\n",
    "    \n",
    "    # Create subplots for each selected signature\n",
    "    n_sigs = len(selected_signatures)\n",
    "    fig, axes = plt.subplots(n_sigs, 1, figsize=(15, 5*n_sigs))\n",
    "    if n_sigs == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, k in enumerate(selected_signatures):\n",
    "        # Get top diseases\n",
    "        scores = phi_avg[k, :]\n",
    "        top_indices = np.argsort(scores)[-n_top:][::-1]\n",
    "        \n",
    "        # Plot temporal patterns\n",
    "        ax = axes[i]\n",
    "        for idx in top_indices:\n",
    "            temporal_pattern = phi[k, idx, :]\n",
    "            ax.plot(temporal_pattern, label=disease_names[idx])\n",
    "        \n",
    "        ax.set_title(f'Signature {k} - Top Disease Temporal Patterns')\n",
    "        ax.set_xlabel('Time')\n",
    "        ax.set_ylabel('Phi Value')\n",
    "        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# First show the top diseases\n",
    "\n",
    "\n",
    "# Then show their temporal patterns\n",
    "# You can select specific signatures of interest:\n",
    "disease_names=essentials['disease_names']\n",
    "plot_signature_temporal_patterns(model, disease_names, selected_signatures=[0,1,14,15,16,13,17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "Y_avg_global = torch.mean(torch.tensor(Y_global) if isinstance(Y_global, np.ndarray) else Y_global, dim=2)\n",
    "Y_avg_batch = torch.mean(torch.tensor(Y_100k) if isinstance(Y_100k, np.ndarray) else Y_100k, dim=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "def compare_disease_patterns(k, true_psi, Y_avg_global, Y_avg_batch):\n",
    "    \"\"\"Compare disease patterns between global and batch data\"\"\"\n",
    "    strong_diseases = (true_psi[k] > 0)\n",
    "    \n",
    "    # Get mean pattern for each dataset (average across patients)\n",
    "    pattern_large = Y_avg_global[:, strong_diseases].mean(dim=0)  # Average across all patients\n",
    "    pattern_small = Y_avg_batch[:, strong_diseases].mean(dim=0)  # Average across batch patients\n",
    "    \n",
    "    # Now both patterns are just length of strong_diseases\n",
    "    correlation = torch.corrcoef(\n",
    "        torch.stack([pattern_large, pattern_small])\n",
    "    )[0,1]\n",
    "    \n",
    "    print(f\"Signature {k} pattern correlation: {correlation:.3f}\")\n",
    "    print(f\"Mean pattern difference: {(pattern_large - pattern_small).abs().mean():.3f}\")\n",
    "\n",
    "# Now use it\n",
    "print(\"Comparing disease sharing patterns between global and batch data:\")\n",
    "for k in range(model.K):\n",
    "    print(f\"\\nSignature {k}:\")\n",
    "    compare_disease_patterns(k, model.psi, Y_avg_global, Y_avg_batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env_pyro2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
